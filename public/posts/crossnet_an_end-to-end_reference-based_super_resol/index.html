<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/posts/crossnet_an_end-to-end_reference-based_super_resol/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/posts/crossnet_an_end-to-end_reference-based_super_resol/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/posts/crossnet_an_end-to-end_reference-based_super_resol/"><meta property="og:site_name" content="Home"><meta property="og:title" content="CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping"><meta property="og:description" content="Paper-reading notes: CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-21T09:40:06+00:00"><meta property="article:modified_time" content="2025-10-21T09:40:06+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/posts/crossnet_an_end-to-end_reference-based_super_resol/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/posts/crossnet_an_end-to-end_reference-based_super_resol/image_1.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/posts/crossnet_an_end-to-end_reference-based_super_resol/image_2.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/posts/crossnet_an_end-to-end_reference-based_super_resol/image_3.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/posts/crossnet_an_end-to-end_reference-based_super_resol/image_4.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/posts/crossnet_an_end-to-end_reference-based_super_resol/image.png"><meta name=twitter:title content="CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping"><meta name=twitter:description content="Paper-reading notes: CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://my-blog-alpha-vert.vercel.app/posts/"},{"@type":"ListItem","position":2,"name":"CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping","item":"https://my-blog-alpha-vert.vercel.app/posts/crossnet_an_end-to-end_reference-based_super_resol/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping","name":"CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping","description":"Paper-reading notes: CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping","keywords":[],"articleBody":"Abstract What‚Äôs the Reference-based Super-resolution (RefSR) Network:\nSuper-resolves a low-resolution (LR) image given an external high-resolution (HR) reference image The reference image and LR image share similar viewpoint but with significant resolution gap (8√ó). Solve the problem Existing RefSR methods work in a cascaded way such as patch matching followed by synthesis pipeline with two independently defined objective functions Divide the image into many small patches (like tiny squares), each patch is compared with a reference image to find its most similar region. But every patch makes its decision independently. ‚Üí Inter-patch misalignment Because of the small misalignments, the grid of patch boundaries in the final image shows. ‚Üí Grid artifacts Old methods trained two steps separately ‚Üí Inefficient training The challenge large-scale (8√ó) super-resolution problem the spatial resolution is increased by 8 times in each dimension (width and height). So the total number of pixels increases from 8√ó8 to 64√ó64. patch matching ‚Üí warping Structure:\nimage encoders extract multi-scale features from both the LR and the reference images cross-scale warping layers spatially aligns the reference feature map with the LR feature map warping module originated from spatial transformer network (STN) fusion decoder aggregates feature maps from both domains to synthesize the HR output Scale Resolution (relative) Example size What it focuses on Scale 0 √ó1 (full resolution) 512√ó512 Fine details (small shifts) Scale 1 √ó2 smaller 256√ó256 Medium motions Scale 2 √ó4 smaller 128√ó128 Larger motions Scale 3 √ó8 smaller 64√ó64 Very large motions Scale 4‚Äì5 √ó16, √ó32 smaller 32√ó32, 16√ó16 Extremely coarse view (too little detail) Result Using cross-scale warping, our network is able to perform spatial alignment at pixel-level in an end-to-end fashion, which improves the existing schemes both in precision (around 2dB-4dB) and efficiency (more than 100 times faster).\nspatial alignment at pixel-level ‚Üí precision and efficiency precision efficiency 1. Introduction The two critical issues in RefSR:\nImage correspondence between the two input images High resolution synthesis of the LR image. The ‚Äòpatch maching + synthesis‚Äô pipeline ‚Üí the end-to-end CrossNet ‚Üí results comparisons.\nüí° Flow estimator Input: feature maps from LR and Ref encoders.\nComputation:\nThe module compares these features (using convolutions) and learns to predict displacement vectors (optical flow).\nOutput: a flow map $F(x, y) = (\\Delta x, \\Delta y)$.\nUse: the warping layer applies this flow map to the reference feature map:\n$$ \\tilde{R}(x, y) = R(x + \\Delta x, y + \\Delta y) $$\nso the warped reference aligns with the LR image.\nNon-rigid deformation: when an object changes its shape or structure in the image (for example, bending, twisting, or changing due to different camera angles).\nRigid = only simple shifts, rotation, or scaling. Non-rigid = more complex distortions ‚Äî like bending, stretching, or perspective change. Grid artifacts: visible blocky or checker-like patterns that appear because the image was reconstructed from many small, rigid square patches that don‚Äôt align smoothly.\nGrid artifacts occur when an image is reconstructed from many small square patches that don‚Äôt align perfectly at their borders. The Laplacian is a mathematical operator that measures how much a pixel value differs from its surroundings.\nIn other words, it tells you where the image changes quickly ‚Äî that‚Äôs usually at edges or texture details.\n2. Related Work Multi-scale deep super resolution we employ MDSR as a sub-module for LR images feature extraction and RefSR synthesis.\nMDSR stands for Multi-scale Deep Super-Resolution Network Used for Feature extraction ‚Üí understanding what‚Äôs in the LR image RefSR synthesis ‚Üí combining LR and reference features to output the high-resolution result Warping and synthesis We follow such ‚Äúwarping and synthesis‚Äù pipeline. However, our approach is different from existing works in the following ways:\nOur approach performs multi-scale warping on feature domain at pixel-scale which accelerates the model convergence by allowing flow to be globally updated at higher scales. a novel fusion scheme is proposed for image synthesis. concatenation, linearly combining images 3. Approach 3.1 Fully Conv Cross-scale Alignment Module It is necessary to perform spatial alignment for reference image, since it is captured at different view points from LR image.\nCross-scale warping We propose cross-scale warping to perform non-rigid image transformation.\nOur proposed cross-scale warping operation considers a pixel-wise shift vector ( V ):\n$$ I_o = warp(y_{Ref}, V) $$\nwhich assigns a specific shift vector for each pixel location, so that it avoids the blocky and blurry artifacts.\nüí° Pixel-wise shift vector (V) ‚Üí patch matching\n( V ) represents a flow field, where each pixel gets its own small movement vector (Œîx, Œîy). A flow field is a map (like a vector field) that assigns a motion vector to every pixel in the image. So instead of moving the entire image or patch, CrossNet can move each pixel individually ‚Äî very flexible. The equation:\n$$ I_o = warp(y_{Ref}, V) $$\nmeans:\nThe output image ( $I_o$ ) is generated by warping the reference image ( $y_{Ref}$ ) according to the flow field ( $V$ ).\nEach pixel in ( $y_{Ref}$ ) is shifted by its corresponding vector in ( $V$ ).\nCross-scale flow estimator Purpose: Predict pixel-wise flow fields to align the upsampled LR image with the HR reference.\nModel: Based on FlowNetS, adapted for multi-scale correspondence.\nInputs:\n$I_{LR‚Üë}$ : LR image upsampled by MDSR (SISR) $I_{REF}$ : reference image Outputs: Multi-scale flow fields\n${V^{(3)}, V^{(2)}, V^{(1)}, V^{(0)}}$ (coarse ‚Üí fine).\nModification: √ó4 bilinear upsampling with ‚Üí two √ó2 upsampling modules + skip connections + deconvolution ‚Üí finer, smoother flow prediction.\nAdvantage:\nCaptures both large and small displacements; Enables accurate, non-rigid alignment Reduces warping artifacts. üí° How it works (coarse-to-fine refinement)\nThe coarse flow field (V¬≥) roughly aligns big structures. The next flow (V¬≤) refines alignment for medium details. The fine flows (V¬π, V‚Å∞) correct small local misalignments and textures. These flow fields are combined hierarchically ‚Äî like zooming in step-by-step to improve precision. 3.2 End-to-end Network Structure Network structure of CrossNet\nüí° Network:\na LR image encoder a reference image encoder a decoder ‚Üí U-Net LR image Encoder Goal: Extract multi-scale feature maps from the low-resolution (LR) image for alignment and fusion.\nStructure:\nUses a Single-Image SR (SISR) upsampling to enlarge the LR image first. Then applies 4 convolutional layers (5√ó5 filters, 64 channels). Each layer creates a feature map at a different scale (0 ‚Üí 3). Stride = 1 for the first layer, stride = 2 for deeper ones (downsampling by 2). Output:\nA set of multi-scale LR feature maps. $$ F_{LR}^{(0)}, F_{LR}^{(1)}, F_{LR}^{(2)}, F_{LR}^{(3)} $$\nActivation: ReLU (œÉ).\nReference image encoder Goal: Extract and align multi-scale reference features from the HR reference image.\nStructure:\nUses the same 4-scale encoder design as the LR encoder. Produces feature maps . $$ {F_{REF}^{(0)}, F_{REF}^{(1)}, F_{REF}^{(2)}, F_{REF}^{(3)}}¬†$$\nLR and reference encoders have different weights, allowing complementary feature learning. Alignment:\nEach reference feature map $F_{REF}^{(i)}$ is warped using the cross-scale flow $V^{(i)}$. This generates spatially aligned reference features $$ \\hat{F}{REF}^{(i)} = warp(F_{REF}^{(i)}, V^{(i)}) $$\nDecoder Goal: Fuse the low-resolution (LR) features and warped reference (Ref) features across multiple scales to reconstruct the super-resolved (SR) image. Structure Overview The decoder follows a U-Net‚Äìlike architecture. It performs multi-scale fusion and up-sampling using deconvolution layers. Each scale combines: The LR feature at that scale $F_{LR}^{(i)}$, The warped reference feature $\\hat{F}_{REF}^{(i)}$, The decoder feature from the next coarser scale $F_{D}^{(i+1)}$ (if available). üí° Equations (Eq. 6)\nFor the coarsest scale (i = 3):\n$$ F_{D}^{(3)} = \\sigma \\big( W_{D}^{(3)} \\star (F_{LR}^{(3)}, \\hat{F}{REF}^{(3)}) + b{D}^{(3)} \\big) $$\nFor finer scales (i = 2, 1, 0):\n$$ F_{D}^{(i)} = \\sigma \\big( W_{D}^{(i)} \\star (F_{LR}^{(i+1)}, \\hat{F}{REF}^{(i+1)}, F{D}^{(i+1)}) + b_{D}^{(i)} \\big) $$\nwhere:\n$\\star$ denotes the deconvolution operation (transposed convolution). $W_{D}^{(i)}$: deconvolution filters (size 4√ó4, 64 filters, stride 2). $\\sigma$: activation function (ReLU). $b_{D}^{(i)}$ : bias term. Thus, features are progressively upsampled and refined from coarse ‚Üí fine.\nüí° Post-Fusion (Eq. 7)\nAfter obtaining the final decoder feature map $F_{D}^{(0)}$,\nthree convolutional layers (filter size 5√ó5) are applied to refine and generate the SR image:\n$$ \\begin{aligned} F_1 \u0026= \\sigma(W_1 * F_{D}^{(0)} + b_1), \\ F_2 \u0026= \\sigma(W_2 * F_1 + b_2), \\ I_p \u0026= \\sigma(W_p * F_2 + b_p), \\end{aligned} $$\nwhere:\n$W_1, W_2, W_p$: convolution filters with channel numbers {64, 64, 3}, $I_p$: final super-resolved output image. 3.3 Loss Function Goal: Train CrossNet to generate super-resolved (SR) outputs $I_p$ close to the ground-truth HR images $I_{HR}$.\nFormula:\n$$ L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{s} \\rho(I_{HR}^{(i)}(s) - I_{p}^{(i)}(s)) $$\nPenalty: Uses the Charbonnier loss\n$$ \\rho(x) = \\sqrt{x^2 + 0.001^2} $$\nA smooth, robust version of L1 loss that reduces the effect of outliers.\nVariables:\n$N$: number of training samples $s$: pixel (spatial location) $i$: training sample index 4. Experiment 4.1 Dataset Dataset: The representative Flower dataset and Light Field Video (LFVideo) dataset. Each light field image has: 376 √ó 541 spatial samples 14 √ó 14 angular samples Model training: Each light field image has: 320 √ó 512 spatial samples 8 √ó 8 angular samples Test generalization: Datasets: Stanford Light Field dataset Scene Light Field dataset During testing, they apply the big input images using a sliding window approach: Window size: 512√ó512 Stride: 256 4.2 Evaluation Training setup: Trained for 200K iterations on Flower and LFVideo datasets. Scale factors: √ó4 and √ó8 super-resolution. Learning rate: 1e-4 / 7e-5 ‚Üí decayed to 1e-5 / 7e-6 after 150K iterations. Optimizer: Adam (Œ≤‚ÇÅ = 0.9, Œ≤‚ÇÇ = 0.999). Comparisons: Competes with RefSR methods (SS-Net, PatchMatch) and SISR methods (SRCNN, VDSR, MDSR). Evaluation metrics: PSNR, SSIM, and IFC on √ó4 and √ó8 scales. Reference images from position (0,0); LR images from (1,1) and (7,7). Results: CrossNet achieves 2‚Äì4 dB PSNR gain over previous methods. CrossNet consistently outperforms the resting approaches under different disparities, datasets and scales. Quantitative evaluation of the sota SISR and RefSR algorithms, in terms of PSNR/SSIM/IFC for scale factors √ó4 and √ó8 respectively.\nMetric Meaning PSNR (Peak Signal-to-Noise Ratio) Measures reconstruction accuracy (higher = clearer, less error). SSIM (Structural Similarity Index) Measures structural similarity to the ground truth (higher = more visually similar). IFC (Information Fidelity Criterion) Evaluates how much visual information is preserved (higher = better detail). Generalization During training, apply a parallax augmentation procedure this means they randomly shift the reference image by ‚Äì15 to +15 pixels both horizontally and vertically. The purpose is to simulate different viewpoint disparities (parallax changes) make the model more robust to viewpoint variations. They initialize the model using parameters pre-trained on the LFVideo dataset, Then re-train on the Flower dataset for 200 K iterations to improve generalization. The initial learning rate is 7 √ó 10‚Åª‚Åµ, which decays by factors 0.5, 0.2, 0.1 at 50 K, 100 K, 150 K iterations. Table 2 and 3 show PSNR comparison results: Their re-trained model (CrossNet) outperforms PatchMatch [11] and SS-Net [2] on both Stanford and Scene Light Field datasets. The improvement is roughly +1.79 ‚Äì 2.50 dB (Stanford) and +2.84 dB (Scene LF dataset). Efficiency within 1 seconds machine: 8 Intel Xeon CPU (3.4 GHz) a GeForce GTX 1080 GPU 4.3 Discussion Flows at scale 0‚Äì3 were coherent (good). Flows at scale 4‚Äì5 were too noisy ‚Äî because those very small maps (like 32√ó32) lost too much information. Training setup:\nTrain both CrossNet and CrossNet-iw with the same procedure: Pre-train on LFVideo dataset Fine-tune on Flower dataset 200 K iterations total Additionally, CrossNet-iw pretraining: Pre-train only the flow estimator WS-SRNet using an image warping task for 100 K iterations. Train the whole network jointly for another 100 K iterations. FlowNetS+ adds extra upsampling layers\nIn the original FlowNetS: The final flow map is smaller than the input (maybe ¬º or ¬Ω resolution). This is fine for rough alignment but loses small motion details. In FlowNetS+: They add extra upsampling layers so the final flow map is finer (closer to full size). That‚Äôs why it aligns better ‚Äî it can describe tiny pixel movements more accurately. Downsampling = make smaller. Upsampling = make bigger.\n","wordCount":"1976","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/posts/crossnet_an_end-to-end_reference-based_super_resol/image.png","datePublished":"2025-10-21T09:40:06Z","dateModified":"2025-10-21T09:40:06Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/posts/crossnet_an_end-to-end_reference-based_super_resol/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;¬ª&nbsp;<a href=/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping</h1><div class=post-description>Paper-reading notes: CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping</div><div class=post-meta><span title='2025-10-21 09:40:06 +0000 +0000'>October 21, 2025</span>&nbsp;¬∑&nbsp;<span>1976 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#abstract aria-label=Abstract>Abstract</a><ul><li><a href=#solve-the-problem aria-label="Solve the problem">Solve the problem</a></li><li><a href=#result aria-label=Result>Result</a></li></ul></li><li><a href=#1-introduction aria-label="1. Introduction">1. Introduction</a><ul><ul><li><a href=#flow-estimator aria-label="Flow estimator">Flow estimator</a></li></ul></ul></li><li><a href=#2-related-work aria-label="2. Related Work">2. Related Work</a><ul><li><a href=#multi-scale-deep-super-resolution aria-label="Multi-scale deep super resolution">Multi-scale deep super resolution</a></li><li><a href=#warping-and-synthesis aria-label="Warping and synthesis">Warping and synthesis</a></li></ul></li><li><a href=#3-approach aria-label="3. Approach">3. Approach</a><ul><li><a href=#31-fully-conv-cross-scale-alignment-module aria-label="3.1 Fully Conv Cross-scale Alignment Module">3.1 Fully Conv Cross-scale Alignment Module</a><ul><li><a href=#cross-scale-warping aria-label="Cross-scale warping">Cross-scale warping</a></li><li><a href=#cross-scale-flow-estimator aria-label="Cross-scale flow estimator">Cross-scale flow estimator</a></li></ul></li><li><a href=#32-end-to-end-network-structure aria-label="3.2 End-to-end Network Structure">3.2 End-to-end Network Structure</a><ul><li><a href=#lr-image-encoder aria-label="LR image Encoder">LR image Encoder</a></li><li><a href=#reference-image-encoder aria-label="Reference image encoder">Reference image encoder</a></li><li><a href=#decoder aria-label=Decoder>Decoder</a></li></ul></li><li><a href=#33-loss-function aria-label="3.3 Loss Function">3.3 Loss Function</a></li></ul></li><li><a href=#4-experiment aria-label="4. Experiment">4. Experiment</a><ul><li><a href=#41-dataset aria-label="4.1 Dataset">4.1 Dataset</a></li><li><a href=#42-evaluation aria-label="4.2 Evaluation">4.2 Evaluation</a><ul><li><a href=#generalization aria-label=Generalization>Generalization</a></li><li><a href=#efficiency aria-label=Efficiency>Efficiency</a></li></ul></li><li><a href=#43-discussion aria-label="4.3 Discussion">4.3 Discussion</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h1><p>What‚Äôs the <strong>Reference-based Super-resolution (RefSR)</strong> Network:</p><ul><li>Super-resolves a <strong>low-resolution (LR)</strong> image given an external <strong>high-resolution (HR) reference image</strong></li><li>The reference image and LR image share similar viewpoint but with significant resolution gap (8√ó).</li></ul><h2 id=solve-the-problem>Solve the problem<a hidden class=anchor aria-hidden=true href=#solve-the-problem>#</a></h2><ul><li>Existing RefSR methods work in a cascaded way such as <strong>patch matching</strong> followed by <strong>synthesis pipeline</strong> with two independently defined objective functions<ul><li>Divide the image into many small <strong>patches</strong> (like tiny squares), each patch is compared with a <strong>reference image</strong> to find its most similar region.</li><li>But every patch makes its decision independently. ‚Üí <strong>Inter-patch misalignment</strong></li><li>Because of the small misalignments, the <strong>grid</strong> of patch boundaries in the final image shows. ‚Üí <strong>Grid artifacts</strong></li><li>Old methods trained <strong>two steps separately ‚Üí Inefficient training</strong></li></ul></li><li>The challenge large-scale (8√ó) super-resolution problem<ul><li>the <strong>spatial resolution is increased by 8 times</strong> in each dimension (width and height).</li><li>So the total number of pixels increases from 8√ó8 to 64√ó64.</li></ul></li><li><strong>patch matching ‚Üí warping</strong></li></ul><p>Structure:</p><ol><li><strong>image encoders</strong><ol><li>extract multi-scale features from both the LR and the reference images</li></ol></li><li><strong>cross-scale warping layers</strong><ol><li>spatially aligns the reference feature map with the LR feature map</li><li>warping module originated from <strong>spatial transformer network (STN)</strong></li></ol></li><li><strong>fusion decoder</strong><ol><li>aggregates feature maps from both domains to synthesize the HR output</li></ol></li></ol><table><thead><tr><th>Scale</th><th>Resolution (relative)</th><th>Example size</th><th>What it focuses on</th></tr></thead><tbody><tr><td><strong>Scale 0</strong></td><td>√ó1 (full resolution)</td><td>512√ó512</td><td>Fine details (small shifts)</td></tr><tr><td><strong>Scale 1</strong></td><td>√ó2 smaller</td><td>256√ó256</td><td>Medium motions</td></tr><tr><td><strong>Scale 2</strong></td><td>√ó4 smaller</td><td>128√ó128</td><td>Larger motions</td></tr><tr><td><strong>Scale 3</strong></td><td>√ó8 smaller</td><td>64√ó64</td><td>Very large motions</td></tr><tr><td><strong>Scale 4‚Äì5</strong></td><td>√ó16, √ó32 smaller</td><td>32√ó32, 16√ó16</td><td>Extremely coarse view (too little detail)</td></tr></tbody></table><h2 id=result>Result<a hidden class=anchor aria-hidden=true href=#result>#</a></h2><p>Using cross-scale warping, our network is able to perform <strong>spatial alignmen</strong>t at pixel-level in an end-to-end fashion, which improves the existing schemes both in <strong>precision</strong> (around 2dB-4dB) and <strong>efficiency</strong> (more than 100 times faster).</p><ul><li><strong>spatial alignment at pixel-level ‚Üí precision</strong> and <strong>efficiency</strong><ul><li><strong>precision</strong></li><li><strong>efficiency</strong></li></ul></li></ul><h1 id=1-introduction>1. Introduction<a hidden class=anchor aria-hidden=true href=#1-introduction>#</a></h1><p>The two critical issues in RefSR:</p><ol><li>Image correspondence between the two input images</li><li>High resolution synthesis of the LR image.</li></ol><p><img alt="The ‚Äòpatch maching + synthesis‚Äô pipeline ‚Üí the end-to-end CrossNet ‚Üí results comparisons." loading=lazy src=/posts/crossnet_an_end-to-end_reference-based_super_resol/image.png></p><p>The ‚Äòpatch maching + synthesis‚Äô pipeline ‚Üí the end-to-end CrossNet ‚Üí results comparisons.</p><aside>üí°<h3 id=flow-estimator>Flow estimator<a hidden class=anchor aria-hidden=true href=#flow-estimator>#</a></h3><ol><li><p><strong>Input:</strong> feature maps from LR and Ref encoders.</p></li><li><p><strong>Computation:</strong></p><p>The module compares these features (using convolutions) and learns to predict displacement vectors (optical flow).</p></li><li><p><strong>Output:</strong> a flow map $F(x, y) = (\Delta x, \Delta y)$.</p></li><li><p><strong>Use:</strong> the warping layer applies this flow map to the reference feature map:</p><p>$$
\tilde{R}(x, y) = R(x + \Delta x, y + \Delta y)
$$</p><p>so the warped reference aligns with the LR image.</p></li></ol></aside><blockquote><p><strong>Non-rigid deformation:</strong> when an object changes its <strong>shape or structure</strong> in the image (for example, bending, twisting, or changing due to different camera angles).</p></blockquote><ul><li><strong>Rigid</strong> = only simple shifts, rotation, or scaling.</li><li><strong>Non-rigid</strong> = more complex distortions ‚Äî like bending, stretching, or perspective change.</li></ul><blockquote><p><strong>Grid artifacts:</strong> visible <strong>blocky or checker-like patterns</strong> that appear because the image was reconstructed from many small, rigid square patches that don‚Äôt align smoothly.</p></blockquote><ul><li><strong>Grid artifacts</strong> occur when an image is reconstructed from many small <strong>square patches</strong> that don‚Äôt align perfectly at their borders.</li></ul><blockquote><p>The <strong>Laplacian</strong> is a <strong>mathematical operator</strong> that measures <strong>how much a pixel value differs from its surroundings</strong>.</p><p>In other words, it tells you <strong>where the image changes quickly</strong> ‚Äî that‚Äôs usually at <strong>edges</strong> or <strong>texture details</strong>.</p></blockquote><h1 id=2-related-work>2. Related Work<a hidden class=anchor aria-hidden=true href=#2-related-work>#</a></h1><h2 id=multi-scale-deep-super-resolution>Multi-scale deep super resolution<a hidden class=anchor aria-hidden=true href=#multi-scale-deep-super-resolution>#</a></h2><p>we employ <strong>MDSR</strong> as a sub-module for LR images feature extraction and RefSR synthesis.</p><ul><li>MDSR stands for <strong>Multi-scale Deep Super-Resolution Network</strong></li><li>Used for<ul><li><strong>Feature extraction</strong> ‚Üí understanding what‚Äôs in the LR image</li><li><strong>RefSR synthesis</strong> ‚Üí combining LR and reference features to output the high-resolution result</li></ul></li></ul><h2 id=warping-and-synthesis><strong>Warping and synthesis</strong><a hidden class=anchor aria-hidden=true href=#warping-and-synthesis>#</a></h2><p>We follow such ‚Äú<strong>warping and synthesis</strong>‚Äù pipeline. However, our approach is different from existing works in the following ways:</p><ol><li>Our approach performs multi-scale warping on feature domain <del>at pixel-scale</del><ul><li>which accelerates the model convergence by allowing flow to be globally updated at higher scales.</li></ul></li><li>a novel fusion scheme is proposed for image synthesis. <del>concatenation</del>, <del>linearly combining images</del></li></ol><h1 id=3-approach>3. Approach<a hidden class=anchor aria-hidden=true href=#3-approach>#</a></h1><h2 id=31-fully-conv-cross-scale-alignment-module>3.1 Fully Conv Cross-scale Alignment Module<a hidden class=anchor aria-hidden=true href=#31-fully-conv-cross-scale-alignment-module>#</a></h2><p>It is necessary to perform spatial alignment for reference image, since it is captured at different view points from LR image.</p><h3 id=cross-scale-warping>Cross-scale warping<a hidden class=anchor aria-hidden=true href=#cross-scale-warping>#</a></h3><p>We propose cross-scale warping to <strong>perform non-rigid image transformation</strong>.</p><p>Our proposed cross-scale warping operation considers a <strong>pixel-wise shift vector</strong> ( V ):</p><p>$$
I_o = warp(y_{Ref}, V)
$$</p><p>which assigns a specific shift vector for each pixel location, so that it avoids the blocky and blurry artifacts.</p><aside>üí°<p><strong>Pixel-wise shift vector (V) ‚Üí <del>patch matching</del></strong></p><ul><li>( V ) represents a <strong>flow field</strong>, where each pixel gets its own small movement vector (Œîx, Œîy).<ul><li>A <strong>flow field</strong> is a map (like a vector field) that assigns a <strong>motion vector</strong> to <strong>every pixel</strong> in the image.</li></ul></li><li>So instead of moving the entire image or patch, CrossNet can move each pixel individually ‚Äî very flexible.</li></ul><p>The equation:</p><p>$$
I_o = warp(y_{Ref}, V)
$$</p><p>means:</p><blockquote><p>The output image ( $I_o$ ) is generated by warping the reference image ( $y_{Ref}$ ) according to the flow field ( $V$ ).</p><p>Each pixel in ( $y_{Ref}$ ) is shifted by its corresponding vector in ( $V$ ).</p></blockquote></aside><h3 id=cross-scale-flow-estimator>Cross-scale flow estimator<a hidden class=anchor aria-hidden=true href=#cross-scale-flow-estimator>#</a></h3><ul><li><p><strong>Purpose:</strong> Predict pixel-wise flow fields to align the upsampled LR image with the HR reference.</p></li><li><p><strong>Model:</strong> Based on <strong>FlowNetS</strong>, adapted for multi-scale correspondence.</p></li><li><p><strong>Inputs:</strong></p><ul><li>$I_{LR‚Üë}$ : LR image upsampled by <strong>MDSR (SISR)</strong></li><li>$I_{REF}$ : reference image</li></ul></li><li><p><strong>Outputs:</strong> Multi-scale flow fields</p><p>${V^{(3)}, V^{(2)}, V^{(1)}, V^{(0)}}$ (coarse ‚Üí fine).</p></li><li><p><strong>Modification:</strong> √ó4 bilinear upsampling with ‚Üí two √ó2 upsampling modules + skip connections + deconvolution ‚Üí finer, smoother flow prediction.</p></li><li><p><strong>Advantage:</strong></p><ul><li>Captures both large and small displacements;</li><li>Enables accurate, non-rigid alignment</li><li>Reduces warping artifacts.</li></ul></li></ul><aside>üí°<p>How it works (<strong>coarse-to-fine refinement</strong>)</p><ol><li>The <strong>coarse flow field (V¬≥)</strong> roughly aligns big structures.</li><li>The <strong>next flow (V¬≤)</strong> refines alignment for medium details.</li><li>The <strong>fine flows (V¬π, V‚Å∞)</strong> correct small local misalignments and textures.</li><li>These flow fields are combined hierarchically ‚Äî like zooming in step-by-step to improve precision.</li></ol></aside><h2 id=32-end-to-end-network-structure>3.2 End-to-end Network Structure<a hidden class=anchor aria-hidden=true href=#32-end-to-end-network-structure>#</a></h2><p><img alt="Network structure of CrossNet" loading=lazy src=/posts/crossnet_an_end-to-end_reference-based_super_resol/image_1.png></p><p>Network structure of CrossNet</p><aside>üí°<p><strong>Network:</strong></p><ol><li><strong>a LR image encoder</strong></li><li><strong>a reference image encoder</strong></li><li><strong>a decoder ‚Üí U-Net</strong></li></ol></aside><h3 id=lr-image-encoder><strong>LR image Encoder</strong><a hidden class=anchor aria-hidden=true href=#lr-image-encoder>#</a></h3><ul><li><p><strong>Goal:</strong> Extract <strong>multi-scale feature maps</strong> from the <strong>low-resolution (LR)</strong> image for alignment and fusion.</p></li><li><p><strong>Structure:</strong></p><ul><li>Uses a <strong>Single-Image SR (SISR)</strong> upsampling to enlarge the LR image first.</li><li>Then applies <strong>4 convolutional layers</strong> (5√ó5 filters, 64 channels).<ul><li>Each layer creates a feature map at a different <strong>scale</strong> (0 ‚Üí 3).</li></ul></li><li><strong>Stride = 1</strong> for the first layer, <strong>stride = 2</strong> for deeper ones (downsampling by 2).</li></ul></li><li><p><strong>Output:</strong></p><ul><li>A set of <strong>multi-scale LR feature maps</strong>.</li></ul><p>$$
F_{LR}^{(0)}, F_{LR}^{(1)}, F_{LR}^{(2)}, F_{LR}^{(3)}
$$</p></li><li><p><strong>Activation:</strong> ReLU (œÉ).</p></li></ul><h3 id=reference-image-encoder>Reference image encoder<a hidden class=anchor aria-hidden=true href=#reference-image-encoder>#</a></h3><ul><li><p><strong>Goal:</strong> Extract and align <strong>multi-scale reference features</strong> from the HR reference image.</p></li><li><p><strong>Structure:</strong></p><ul><li>Uses the <strong>same 4-scale encoder</strong> design as the LR encoder.</li><li>Produces feature maps .</li></ul><p>$$
{F_{REF}^{(0)}, F_{REF}^{(1)}, F_{REF}^{(2)}, F_{REF}^{(3)}}¬†
$$</p><ul><li>LR and reference encoders have <strong>different weights</strong>, allowing complementary feature learning.</li></ul></li><li><p><strong>Alignment:</strong></p><ul><li>Each reference feature map $F_{REF}^{(i)}$ is <strong>warped</strong> using the <strong>cross-scale flow</strong> $V^{(i)}$.</li><li>This generates <strong>spatially aligned reference features</strong></li></ul><p>$$
\hat{F}{REF}^{(i)} = warp(F_{REF}^{(i)}, V^{(i)})
$$</p></li></ul><h3 id=decoder>Decoder<a hidden class=anchor aria-hidden=true href=#decoder>#</a></h3><ul><li>Goal: Fuse the <strong>low-resolution (LR)</strong> features and <strong>warped reference (Ref)</strong> features across multiple scales to reconstruct the <strong>super-resolved (SR)</strong> image.</li><li><strong>Structure Overview</strong><ul><li>The decoder follows a <strong>U-Net‚Äìlike architecture</strong>.</li><li>It performs <strong>multi-scale fusion</strong> and <strong>up-sampling</strong> using <strong>deconvolution</strong> layers.</li><li>Each scale combines:<ul><li>The <strong>LR feature</strong> at that scale $F_{LR}^{(i)}$,</li><li>The <strong>warped reference feature</strong> $\hat{F}_{REF}^{(i)}$,</li><li>The <strong>decoder feature</strong> from the next coarser scale $F_{D}^{(i+1)}$ (if available).</li></ul></li></ul></li></ul><aside>üí°<p><strong>Equations (Eq. 6)</strong></p><p>For the <strong>coarsest scale</strong> (i = 3):</p><p>$$
F_{D}^{(3)} = \sigma \big( W_{D}^{(3)} \star (F_{LR}^{(3)}, \hat{F}<em>{REF}^{(3)}) + b</em>{D}^{(3)} \big)
$$</p><p>For <strong>finer scales</strong> (i = 2, 1, 0):</p><p>$$
F_{D}^{(i)} = \sigma \big( W_{D}^{(i)} \star (F_{LR}^{(i+1)}, \hat{F}<em>{REF}^{(i+1)}, F</em>{D}^{(i+1)}) + b_{D}^{(i)} \big)
$$</p><p>where:</p><ul><li>$\star$ denotes the <strong>deconvolution operation</strong> (transposed convolution).</li><li>$W_{D}^{(i)}$: deconvolution filters (<strong>size 4√ó4, 64 filters, stride 2</strong>).</li><li>$\sigma$: activation function (ReLU).</li><li>$b_{D}^{(i)}$
: bias term.</li></ul><p>Thus, features are progressively upsampled and refined from <strong>coarse ‚Üí fine</strong>.</p></aside><hr><aside>üí°<p><strong>Post-Fusion (Eq. 7)</strong></p><p>After obtaining the final decoder feature map $F_{D}^{(0)}$,</p><p>three <strong>convolutional layers</strong> (<strong>filter size 5√ó5</strong>) are applied to refine and generate the SR image:</p><p>$$
\begin{aligned}
F_1 &= \sigma(W_1 * F_{D}^{(0)} + b_1), \
F_2 &= \sigma(W_2 * F_1 + b_2), \
I_p &= \sigma(W_p * F_2 + b_p),
\end{aligned}
$$</p><p>where:</p><ul><li>$W_1, W_2, W_p$: convolution filters with channel numbers {64, 64, 3},</li><li>$I_p$: final <strong>super-resolved output image</strong>.</li></ul></aside><h2 id=33-loss-function>3.3 Loss Function<a hidden class=anchor aria-hidden=true href=#33-loss-function>#</a></h2><ul><li><p><strong>Goal:</strong> Train CrossNet to generate super-resolved (SR) outputs $I_p$ close to the ground-truth HR images $I_{HR}$.</p></li><li><p><strong>Formula:</strong></p><p>$$
L = \frac{1}{N} \sum_{i=1}^{N} \sum_{s} \rho(I_{HR}^{(i)}(s) - I_{p}^{(i)}(s))
$$</p></li><li><p><strong>Penalty:</strong> Uses the <strong>Charbonnier loss</strong></p><p>$$
\rho(x) = \sqrt{x^2 + 0.001^2}
$$</p><blockquote><p>A smooth, robust version of L1 loss that reduces the effect of outliers.</p></blockquote></li><li><p><strong>Variables:</strong></p><ul><li>$N$: number of training samples</li><li>$s$: pixel (spatial location)</li><li>$i$: training sample index</li></ul></li></ul><h1 id=4-experiment>4. Experiment<a hidden class=anchor aria-hidden=true href=#4-experiment>#</a></h1><h2 id=41-dataset>4.1 Dataset<a hidden class=anchor aria-hidden=true href=#41-dataset>#</a></h2><ul><li>Dataset:<ul><li>The representative <strong>Flower dataset</strong> and <strong>Light Field Video (LFVideo) dataset.</strong></li><li>Each light field image has:<ul><li><strong>376 √ó 541 spatial samples</strong></li><li><strong>14 √ó 14 angular samples</strong></li></ul></li></ul></li><li>Model training:<ul><li>Each light field image has:<ul><li><strong>320 √ó 512 spatial samples</strong></li><li><strong>8 √ó 8 angular samples</strong></li></ul></li></ul></li><li>Test generalization:<ul><li>Datasets:<ul><li><strong>Stanford Light Field dataset</strong></li><li><strong>Scene Light Field dataset</strong></li></ul></li></ul></li><li>During testing, they apply the <strong>big input images</strong> using a <strong>sliding window approach</strong>:<ul><li>Window size: <strong>512√ó512</strong></li><li>Stride: <strong>256</strong></li></ul></li></ul><h2 id=42-evaluation>4.2 Evaluation<a hidden class=anchor aria-hidden=true href=#42-evaluation>#</a></h2><ul><li><strong>Training setup:</strong><ul><li>Trained for <strong>200K iterations</strong> on <strong>Flower</strong> and <strong>LFVideo</strong> datasets.</li><li>Scale factors: <strong>√ó4</strong> and <strong>√ó8</strong> super-resolution.</li><li>Learning rate: <strong>1e-4</strong> / <strong>7e-5</strong> ‚Üí decayed to 1e-5 / 7e-6 after 150K iterations.</li><li>Optimizer: <strong>Adam</strong> (Œ≤‚ÇÅ = 0.9, Œ≤‚ÇÇ = 0.999).</li></ul></li><li><strong>Comparisons:</strong><ul><li>Competes with RefSR methods (<strong>SS-Net</strong>, <strong>PatchMatch</strong>) and SISR methods (<strong>SRCNN</strong>, <strong>VDSR</strong>, <strong>MDSR</strong>).</li></ul></li><li><strong>Evaluation metrics:</strong><ul><li><strong>PSNR</strong>, <strong>SSIM</strong>, and <strong>IFC</strong> on √ó4 and √ó8 scales.</li><li>Reference images from position (0,0); LR images from (1,1) and (7,7).</li></ul></li><li><strong>Results:</strong><ul><li><strong>CrossNet</strong> achieves <strong>2‚Äì4 dB PSNR gain</strong> over previous methods.</li><li><strong>CrossNet</strong> consistently outperforms the resting approaches under different <strong>disparities</strong>, <strong>datasets</strong> and <strong>scales</strong>.</li></ul></li></ul><p><img alt="Quantitative evaluation of the sota SISR and RefSR algorithms, in terms of PSNR/SSIM/IFC for scale factors √ó4 and √ó8 respectively." loading=lazy src=/posts/crossnet_an_end-to-end_reference-based_super_resol/image_2.png></p><p>Quantitative evaluation of the sota SISR and RefSR algorithms, in terms of PSNR/SSIM/IFC for scale factors √ó4 and √ó8 respectively.</p><table><thead><tr><th>Metric</th><th>Meaning</th></tr></thead><tbody><tr><td><strong>PSNR (Peak Signal-to-Noise Ratio)</strong></td><td>Measures reconstruction accuracy (higher = clearer, less error).</td></tr><tr><td><strong>SSIM (Structural Similarity Index)</strong></td><td>Measures structural similarity to the ground truth (higher = more visually similar).</td></tr><tr><td><strong>IFC (Information Fidelity Criterion)</strong></td><td>Evaluates how much visual information is preserved (higher = better detail).</td></tr></tbody></table><h3 id=generalization><strong>Generalization</strong><a hidden class=anchor aria-hidden=true href=#generalization>#</a></h3><ul><li>During <strong>training</strong>, apply a <strong>parallax augmentation procedure</strong><ul><li>this means they randomly shift the reference image by <strong>‚Äì15 to +15 pixels</strong> both horizontally and vertically.</li><li>The purpose is to<ul><li>simulate different viewpoint disparities (parallax changes)</li><li>make the model more robust to viewpoint variations.</li></ul></li></ul></li><li>They <strong>initialize</strong> the model using parameters pre-trained on the <strong>LFVideo dataset</strong>,<ul><li>Then <strong>re-train</strong> on the <strong>Flower dataset</strong> for <strong>200 K iterations</strong> to improve generalization.</li><li>The <strong>initial learning rate</strong> is <strong>7 √ó 10‚Åª‚Åµ</strong>,<ul><li>which decays by factors <strong>0.5, 0.2, 0.1</strong> at <strong>50 K, 100 K, 150 K</strong> iterations.</li></ul></li></ul></li><li><strong>Table 2 and 3</strong> show PSNR comparison results:<ul><li>Their re-trained model (CrossNet) <strong>outperforms PatchMatch [11]</strong> and <strong>SS-Net [2]</strong> on both <strong>Stanford</strong> and <strong>Scene Light Field datasets</strong>.</li><li>The improvement is roughly <strong>+1.79 ‚Äì 2.50 dB</strong> (Stanford) and <strong>+2.84 dB</strong> (Scene LF dataset).</li></ul></li></ul><h3 id=efficiency>Efficiency<a hidden class=anchor aria-hidden=true href=#efficiency>#</a></h3><ul><li>within 1 seconds</li><li>machine:<ul><li><strong>8 Intel Xeon CPU (3.4 GHz)</strong></li><li><strong>a GeForce GTX 1080 GPU</strong></li></ul></li></ul><h2 id=43-discussion>4.3 Discussion<a hidden class=anchor aria-hidden=true href=#43-discussion>#</a></h2><p><img alt=image.png loading=lazy src=/posts/crossnet_an_end-to-end_reference-based_super_resol/image_3.png></p><ul><li>Flows at <strong>scale 0‚Äì3</strong> were coherent (good).</li><li>Flows at <strong>scale 4‚Äì5</strong> were <em>too noisy</em> ‚Äî because those very small maps (like 32√ó32) lost too much information.</li></ul><p><strong>Training setup:</strong></p><ul><li>Train both <strong>CrossNet</strong> and <strong>CrossNet-iw</strong> with the <strong>same procedure</strong>:<ul><li>Pre-train on LFVideo dataset</li><li>Fine-tune on Flower dataset</li><li>200 K iterations total</li></ul></li><li>Additionally, <strong>CrossNet-iw pretraining</strong>:<ul><li>Pre-train only the <strong>flow estimator WS-SRNet</strong> using an <strong>image warping task</strong> for 100 K iterations.</li><li>Train the <strong>whole network jointly</strong> for another 100 K iterations.</li></ul></li></ul><p><img alt=image.png loading=lazy src=/posts/crossnet_an_end-to-end_reference-based_super_resol/image_4.png></p><p>FlowNetS+ adds <em>extra upsampling layers</em></p><ul><li>In the original FlowNetS:<ul><li>The final <strong>flow map</strong> is smaller than the input (maybe ¬º or ¬Ω resolution).</li><li>This is fine for rough alignment but loses small motion details.</li></ul></li><li>In FlowNetS+:<ul><li>They add <strong>extra upsampling layers</strong> so the final flow map is <strong>finer</strong> (closer to full size).</li><li>That‚Äôs why it aligns better ‚Äî it can describe <strong>tiny pixel movements</strong> more accurately.</li></ul></li></ul><blockquote><p><strong>Downsampling = make smaller.
Upsampling = make bigger.</strong></p></blockquote></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>