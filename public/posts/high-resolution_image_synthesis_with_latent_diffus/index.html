<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Latent Diffusion Models | Home</title>
<meta name="keywords" content="">
<meta name="description" content="Paper-reading notes: High-Resolution Image Synthesis with Latent Diffusion Models">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/high-resolution_image_synthesis_with_latent_diffus/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css" integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx&#43;pA=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/selfile.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/selfile.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/selfile.png">
<link rel="apple-touch-icon" href="http://localhost:1313/selfile.png">
<link rel="mask-icon" href="http://localhost:1313/selfile.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/high-resolution_image_synthesis_with_latent_diffus/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="" crossorigin="anonymous"></script>
<script defer>
  document.addEventListener("DOMContentLoaded", function() {
    if (typeof renderMathInElement === 'function') {
      renderMathInElement(document.body, {
        
        delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "\\[", right: "\\]", display: true},
          {left: "$", right: "$", display: false},
          {left: "\\(", right: "\\)", display: false}
        ],
        
        throwOnError: false,
        
        ignoredTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      });
    }
  });
</script>

  
  <style>
     
    ul#menu .tag-button {
      background: none;
      border: none;
      padding: 0;
      margin: 0;
      font-weight: 500;
      color: inherit;
      cursor: pointer;
      text-decoration: none;
      opacity: 0.95;
      display: inline-block;
    }
    ul#menu .tag-button:hover { text-decoration: underline; }
    ul#menu .tag-button.active { text-decoration: underline; font-weight: 600; }

     
    body.is-home .post-entry { display: none; }
    body.is-home .page-footer .pagination { display: none; }
  </style>

  
  <script>
    (function(){
      function isRootPath() {
        const p = location.pathname.replace(/\/+/g, '/');
        return p === '/' || p === '' || p === '/index.html';
      }

      if (isRootPath()) document.body.classList.add('is-home');

      
      document.addEventListener('DOMContentLoaded', function(){
        const menu = document.getElementById('menu');
        if (!menu) return;
        
        if (menu.querySelector('.tag-button')) return;
        const tags = ['Notes','Thoughts','Projects'];
        tags.forEach(t => {
          const li = document.createElement('li');
          const a = document.createElement('a');
          a.className = 'tag-button';
          a.href = `/tags/${t.toLowerCase()}/`;
          a.textContent = t;
          
          if (location.pathname.toLowerCase().startsWith(`/tags/${t.toLowerCase()}`)) a.classList.add('active');
          li.appendChild(a);
          menu.appendChild(li);
        });
      });
    })();
  </script>
<meta property="og:url" content="http://localhost:1313/posts/high-resolution_image_synthesis_with_latent_diffus/">
  <meta property="og:site_name" content="Home">
  <meta property="og:title" content="Latent Diffusion Models">
  <meta property="og:description" content="Paper-reading notes: High-Resolution Image Synthesis with Latent Diffusion Models">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-06T21:19:54+00:00">
    <meta property="article:modified_time" content="2025-11-06T21:19:54+00:00">
      <meta property="og:image" content="http://localhost:1313/posts/high-resolution_image_synthesis_with_latent_diffus/image.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:1313/posts/high-resolution_image_synthesis_with_latent_diffus/image.png">
<meta name="twitter:title" content="Latent Diffusion Models">
<meta name="twitter:description" content="Paper-reading notes: High-Resolution Image Synthesis with Latent Diffusion Models">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Latent Diffusion Models",
      "item": "http://localhost:1313/posts/high-resolution_image_synthesis_with_latent_diffus/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Latent Diffusion Models",
  "name": "Latent Diffusion Models",
  "description": "Paper-reading notes: High-Resolution Image Synthesis with Latent Diffusion Models",
  "keywords": [
    
  ],
  "articleBody": "Introduction Image synthesis has advanced rapidly, with diffusion models now leading in generating complex, high-resolution images. Unlike GANs, which struggle with stability and mode collapse, and autoregressive transformers, which require billions of parameters, diffusion models achieve strong results in class-conditional generation, super-resolution, and inpainting using fewer parameters and more stable training.\nDespite their success, diffusion models are still computationally expensive. Because they model every pixel, including imperceptible details, both training and inference require enormous GPU time and memory. This makes them less accessible and environmentally costly.\nPerceptual compression removes small, imperceptible pixel details but keeps the overall visual appearance. It focuses on what humans can see clearly, not on exact pixel accuracy. Autoencoders or GANs often perform this kind of compression. Semantic compression goes further. It removes even visible low-level details but preserves the meaning or concept of the image. For example, the exact texture of a face may change, but the idea of “a person wearing glasses” remains. Latent diffusion models operate in this stage, learning high-level structure and meaning instead of pixel noise. To solve this, the authors propose training diffusion models in a latent space instead of pixel space. The idea is to first use an autoencoder to compress images into a smaller, perceptually equivalent representation, and then train the diffusion model on this compact latent data. This reduces computation while keeping visual quality.\nThe resulting method, called Latent Diffusion Model (LDM), combines an autoencoder and a diffusion U-Net, and can include transformer-based conditioning for tasks like text-to-image generation. This design makes high-resolution synthesis more efficient and scalable.\nIn summary, LDMs (1) scale better to large data, (2) significantly reduce training and inference cost, (3) produce faithful, detailed reconstructions, and (4) support versatile conditioning for multi-modal tasks such as text- or layout-based image generation.\nMethod The authors propose separating compression and generation into two stages. Instead of training directly in pixel space, they use an autoencoder to learn a latent space that keeps perceptual quality but greatly lowers computational complexity. The U-Net architecture further helps capture spatial structure, so there is no need for strong compression that harms image quality. In addition, the learned latent space can serve as a general-purpose representation for training other generative models or for tasks such as CLIP-guided image synthesis.\nPerceptual Image Compression The perceptual image compression model is an autoencoder trained with both a perceptual loss and a local patch-based adversarial loss to produce realistic, non-blurry reconstructions. The encoder compresses an image $x$ into a latent representation $z = ε(x)$, and the decoder reconstructs it as $\\tilde{x} = D(z)$. The image is downsampled by a factor $f$.\nTo keep the latent space stable, two types of regularization are used: a KL penalty (as in VAEs) or vector quantization (as in VQGANs). Unlike earlier methods that flattened the latent space into one dimension, this model keeps the 2D spatial structure of the latent representation, allowing for milder compression and higher reconstruction quality while preserving fine image details.\nLatent Diffusion Models Latent Diffusion Models (LDMs) are based on diffusion models, which learn to generate data by gradually denoising random noise through a reverse Markov process. In this framework, each step predicts a cleaner version of a noisy input, trained with a simple mean-squared error objective. By applying diffusion in the latent space learned from the perceptual autoencoder, the model focuses on meaningful, semantic image features instead of pixel-level noise. Unlike previous transformer-based methods that used discrete tokens, LDMs use a U-Net built from 2D convolutions to better capture spatial image structure. During generation, latent samples are denoised step by step and then decoded into final images with a single pass through the decoder.\nA reverse Markov process is the step-by-step denoising process that diffusion models learn — it reverses the fixed forward noising process to transform pure noise back into a clean, generated image.\nConditioning Mechanisms Conditioning mechanisms allow diffusion models to generate images based on extra input information $y$, such as text, semantic maps, or other images. To achieve this, the model learns a conditional distribution $p(z|y)$ by adding the condition $y$ to the denoising network. The authors enhance the U-Net backbone of the LDM with a cross-attention mechanism, which lets the network focus on relevant parts of $y$ while generating the image. A separate encoder $τ_θ$ converts the condition into an embedding that interacts with the U-Net through attention layers. This setup allows flexible conditioning from different modalities, meaning the same diffusion model can handle tasks like text-to-image, layout-to-image, or other image-to-image synthesis efficiently.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) \\cdot V $$\n$$ Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\quad K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\quad V = W_V^{(i)} \\cdot \\tau_\\theta(y) $$\n$K$ and $V$ come from the conditioning input (e.g., text embeddings). $Q$ comes from the U-Net feature map at the current diffusion step. Use in super-resolution Latent Diffusion Models (LDMs) can perform super-resolution efficiently by conditioning on low-resolution images. The method simply concatenates the LR input with the U-Net input, allowing the model to learn how to reconstruct high-frequency details.\nUsing a pretrained autoencoder with downsampling factor (f=4), the model (LDM-SR) is trained on ImageNet with bicubic 4× downsampling, following SR3’s setup. A user study confirms that LDM-SR produces more visually pleasing results. To further enhance detail, a perceptual loss is added as a guiding mechanism. Finally, since bicubic degradation limits generalization, a more robust version called LDM-BSR is trained using diverse degradations to handle real-world low-quality inputs.\nConclusion Latent diffusion models, a simple and efficient way to significantly improve both the training and sampling efficiency of denoising diffusion models without degrading their quality. Based on this and cross-attention conditioning mechanism, the experiments could demonstrate favorable results compared to SOTA methods across a wide range of conditional image synthesis tasks without task-specific architectures.\n",
  "wordCount" : "964",
  "inLanguage": "en",
  "image": "http://localhost:1313/posts/high-resolution_image_synthesis_with_latent_diffus/image.png","datePublished": "2025-11-06T21:19:54Z",
  "dateModified": "2025-11-06T21:19:54Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/high-resolution_image_synthesis_with_latent_diffus/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Home",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/selfile.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Home (Alt + H)">Home</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Latent Diffusion Models
    </h1>
    <div class="post-description">
      Paper-reading notes: High-Resolution Image Synthesis with Latent Diffusion Models
    </div>
    <div class="post-meta"><span title='2025-11-06 21:19:54 +0000 +0000'>November 6, 2025</span>&nbsp;·&nbsp;<span>964 words</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#method" aria-label="Method">Method</a><ul>
                        
                <li>
                    <a href="#perceptual-image-compression" aria-label="Perceptual Image Compression">Perceptual Image Compression</a></li>
                <li>
                    <a href="#latent-diffusion-models" aria-label="Latent Diffusion Models">Latent Diffusion Models</a></li>
                <li>
                    <a href="#conditioning-mechanisms" aria-label="Conditioning Mechanisms">Conditioning Mechanisms</a></li></ul>
                </li>
                <li>
                    <a href="#use-in-super-resolution" aria-label="Use in super-resolution">Use in super-resolution</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<p>Image synthesis has advanced rapidly, with diffusion models now leading in generating complex, high-resolution images. Unlike GANs, which struggle with stability and mode collapse, and autoregressive transformers, which require billions of parameters, diffusion models achieve strong results in class-conditional generation, super-resolution, and inpainting using f<strong>ewer parameters and more stable training</strong>.</p>
<p>Despite their success, diffusion models are still <strong>computationally expensive</strong>. Because they model every pixel, including <strong>imperceptible details</strong>, both training and inference require enormous GPU time and memory. This makes them less accessible and environmentally costly.</p>
<aside>
<ul>
<li><strong>Perceptual compression</strong> removes <strong>small, imperceptible pixel details</strong> but keeps the overall visual appearance. It focuses on what humans can see clearly, not on exact pixel accuracy. Autoencoders or GANs often perform this kind of compression.</li>
<li><strong>Semantic compression</strong> goes further. It removes even <strong>visible low-level details</strong> but preserves the meaning or concept of the image. For example, the exact texture of a face may change, but the idea of “a person wearing glasses” remains. Latent diffusion models operate in this stage, learning high-level structure and meaning instead of pixel noise.</li>
</ul>
</aside>
<p>To solve this, the authors propose training diffusion models in a <strong>latent space</strong> instead of pixel space. The idea is to first use an <strong>autoencoder</strong> to compress images into a smaller, perceptually equivalent representation, and then <strong>train the diffusion model on this compact latent data</strong>. This reduces computation while keeping visual quality.</p>
<p>The resulting method, called <strong>Latent Diffusion Model (LDM)</strong>, combines an autoencoder and a diffusion U-Net, and can include transformer-based <strong>conditioning</strong> for tasks like text-to-image generation. This design makes high-resolution synthesis more efficient and scalable.</p>
<p>In summary, LDMs (1) scale better to large data, (2) significantly reduce training and inference cost, (3) produce faithful, detailed reconstructions, and (4) support versatile conditioning for multi-modal tasks such as text- or layout-based image generation.</p>
<h1 id="method">Method<a hidden class="anchor" aria-hidden="true" href="#method">#</a></h1>
<p>The authors propose <strong>separating compression and generation into two stages</strong>. Instead of training directly in pixel space, they use an <strong>autoencoder</strong> to learn a <strong>latent space</strong> that keeps perceptual quality but greatly lowers computational complexity. The <strong>U-Net</strong> architecture further helps capture spatial structure, so there is no need for strong compression that harms image quality. In addition, the learned latent space can serve as a general-purpose representation for training other generative models or for tasks such as CLIP-guided image synthesis.</p>
<h2 id="perceptual-image-compression">Perceptual Image Compression<a hidden class="anchor" aria-hidden="true" href="#perceptual-image-compression">#</a></h2>
<p>The perceptual image compression model is an <strong>autoencoder</strong> trained with both a perceptual loss and a local patch-based adversarial loss to produce realistic, non-blurry reconstructions. The encoder compresses an image $x$ into a latent representation $z = ε(x)$, and the decoder reconstructs it as $\tilde{x} = D(z)$. The image is downsampled by a factor $f$.</p>
<p>To keep the latent space stable, two types of regularization are used: a <strong>KL penalty</strong> (as in VAEs) or <strong>vector quantization</strong> (as in VQGANs). Unlike earlier methods that flattened the latent space into one dimension, this model keeps the <strong>2D spatial structure</strong> of the latent representation, allowing for <strong>milder compression and higher reconstruction quality</strong> while preserving fine image details.</p>
<h2 id="latent-diffusion-models">Latent Diffusion Models<a hidden class="anchor" aria-hidden="true" href="#latent-diffusion-models">#</a></h2>
<p>Latent Diffusion Models (LDMs) are based on <strong>diffusion models</strong>, which learn to generate data by gradually denoising random noise through a reverse <strong>Markov process</strong>. In this framework, each step predicts a cleaner version of a noisy input, trained with a simple <strong>mean-squared error objective</strong>. By applying diffusion in the <strong>latent space</strong> learned from the perceptual autoencoder, the model focuses on meaningful, semantic image features instead of pixel-level noise. Unlike previous transformer-based methods that used discrete tokens, LDMs use a <strong>U-Net</strong> built from 2D convolutions to better capture spatial image structure. During generation, latent samples are denoised step by step and then decoded into final images with a single pass through the decoder.</p>
<aside>
<p>A <strong>reverse Markov process</strong> is the step-by-step <strong>denoising process</strong> that diffusion models learn — it reverses the fixed forward noising process to transform pure noise back into a clean, generated image.</p>
</aside>
<p><img alt="image.png" loading="lazy" src="/posts/high-resolution_image_synthesis_with_latent_diffus/image.png"></p>
<h2 id="conditioning-mechanisms">Conditioning Mechanisms<a hidden class="anchor" aria-hidden="true" href="#conditioning-mechanisms">#</a></h2>
<p>Conditioning mechanisms allow diffusion models to generate images based on extra input information $y$, such as text, semantic maps, or other images. To achieve this, the model learns a conditional distribution $p(z|y)$ by adding the condition $y$ to the denoising network. The authors enhance the U-Net backbone of the LDM with a <strong>cross-attention mechanism</strong>, which lets the network focus on relevant parts of $y$ while generating the image. A separate <strong>encoder</strong> $τ_θ$ converts the condition into an <strong>embedding</strong> that interacts with the U-Net through attention layers. This setup allows flexible conditioning from different modalities, meaning the same diffusion model can handle tasks like text-to-image, layout-to-image, or other image-to-image synthesis efficiently.</p>
<p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right) \cdot V
$$</p>
<p>$$
Q = W_Q^{(i)} \cdot \varphi_i(z_t), \quad
K = W_K^{(i)} \cdot \tau_\theta(y), \quad
V = W_V^{(i)} \cdot \tau_\theta(y)
$$</p>
<ul>
<li>$K$ and $V$ come from the <strong>conditioning input</strong> (e.g., text embeddings).</li>
<li>$Q$ comes from the <strong>U-Net feature map</strong> at the current diffusion step.</li>
</ul>
<h1 id="use-in-super-resolution">Use in super-resolution<a hidden class="anchor" aria-hidden="true" href="#use-in-super-resolution">#</a></h1>
<p>Latent Diffusion Models (LDMs) can perform <strong>super-resolution</strong> efficiently by conditioning on <strong>low-resolution images</strong>. The method simply concatenates the LR input with the U-Net input, allowing the model to learn how to reconstruct high-frequency details.</p>
<p>Using a pretrained autoencoder with downsampling factor (f=4), the model (<strong>LDM-SR</strong>) is trained on ImageNet with bicubic 4× downsampling, following SR3’s setup. A user study confirms that LDM-SR produces more visually pleasing results. To further enhance detail, a <strong>perceptual loss</strong> is added as a guiding mechanism. Finally, since bicubic degradation limits generalization, a more robust version called <strong>LDM-BSR</strong> is trained using diverse degradations to handle real-world low-quality inputs.</p>
<h1 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<p><strong>Latent diffusion models</strong>, a simple and efficient way to significantly improve both the training and
sampling efficiency of denoising diffusion models without degrading their quality. Based on this and cross-attention conditioning mechanism, the experiments could demonstrate favorable results compared to SOTA methods across a wide range of conditional image synthesis tasks without task-specific architectures.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
