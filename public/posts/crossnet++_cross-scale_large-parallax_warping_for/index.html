<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/posts/crossnet++_cross-scale_large-parallax_warping_for/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/posts/crossnet++_cross-scale_large-parallax_warping_for/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/posts/crossnet++_cross-scale_large-parallax_warping_for/"><meta property="og:site_name" content="Home"><meta property="og:title" content="CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution"><meta property="og:description" content="Paper-reading notes: CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-29T08:23:31+00:00"><meta property="article:modified_time" content="2025-10-29T08:23:31+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/posts/crossnet++_cross-scale_large-parallax_warping_for/660a95ba-789a-4e1a-81ea-223112f91ac2.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/posts/crossnet++_cross-scale_large-parallax_warping_for/image.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/posts/crossnet++_cross-scale_large-parallax_warping_for/660a95ba-789a-4e1a-81ea-223112f91ac2.png"><meta name=twitter:title content="CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution"><meta name=twitter:description content="Paper-reading notes: CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://my-blog-alpha-vert.vercel.app/posts/"},{"@type":"ListItem","position":2,"name":"CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution","item":"https://my-blog-alpha-vert.vercel.app/posts/crossnet++_cross-scale_large-parallax_warping_for/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution","name":"CrossNet\u002b\u002b: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution","description":"Paper-reading notes: CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution","keywords":[],"articleBody":" CrossNet and CrossNet++ Both are for Reference-Based Super-Resolution (RefSR), using a low-resolution (LR) image and a high-resolution (HR reference) image to make a sharper, high-quality output.\nThe performance of CrossNet drops with the increasing of perspective parallax, the improvement of CrossNet++:\nTwo-stage warping → improves alignment Self-supervised flow estimation → uses FlowNet to estimate motion between LR and Ref images Cross-scale alignment → Aligns features at multiple resolutions Hybrid loss functions → warping + landmark + super-resolution loss Real-world performance → produces smoother, sharper, and more realistic results, suitable for a variety of scenarios Abstraction CrossNet++ focuses on reference-based super-resolution (RefSR), improving a low-resolution image using a high-resolution reference from another camera. This task is hard because of large scale differences (8×) and big parallax (~10%) between the two views.\nTo solve this, CrossNet++ introduces an end-to-end two-stage network with:\nCross-scale warping modules, align images at multiple zoom levels to narrow down parallax, handle scale and parallax differences. Image encoder and fusion decoder, extract multi-scale features and combine them to reconstruct a high-quality super-resolved image. It uses new hybrid loss functions comprising warping loss, landmark loss and super-resolution loss to improve accuracy and stability by stabilizing the training of alignment module and helps to improve super-resolution performance.\ntwo-stage wrapping, hybrid loss\n1 Introduction The development of method:\npatch-matching + patch-synthesis + iteratively applying non-uniform warping Causes grid artifacts, incapable of handling the non-rigid image deformation Directly warping between the low and high-resolution images is inaccurate. Such iterative combination of patch matching and warping introduces heavy computational burden. The difference between rigid deformation and non-rigid deformation:\nRigid deformation = viewpoint change, like camera movement. Non-rigid deformation = object itself changes shape (face expression, fabric fold, petal bending). Grid artifacts = tiny square patterns caused by wrong image enlargement or alignment.\nwarping + synthesis It cannot effectively handle large-parallax cases that widely existed in real-world data. pre-warping + re-warping + synthesis CrossNet++ is a unified framework enabling fully end-to-end training which does not require pretraining the flow estimator. Two-stage pipeline: Two-stage cross-scale warping module. stage 1: Uses FlowNet to estimate motion (optical flow) between the low-resolution (LR) and reference (Ref) images without needing ground-truth flow (self-supervised). This produces a roughly aligned “warped-Ref” image. stage 2: Further refines alignment between the warped-Ref and LR image for more accurate warping. Hybrid loss: warping loss, landmark loss and super-resolution loss. warping loss: supervise the flow estimation implicitly. landmark loss: supervise the flow estimation explicitly. Without ground-truth flow = the model learns to estimate motion on its own, using only the images, not any pre-labeled motion data.\nInterpolation = predict inside known area Extrapolation = predict outside known area 2 Related Work 3 Preliminary of CrossNet 3.3 Network Structure 3.3.1 Alignment Module The alignment module aims to align the reference image $I_{REF}$ with the low-resolution image $I_{LR}$. CrossNet++ adopts a warping-based alignment using two-stage optical flow estimation.\nIn the first stage, a modified FlowNet (denoted as $Flow_1$) predicts the flow field between an upsampled LR image $I_{LR↑}$ and the reference image $I_{REF}$:\n$$ V_1^0 = Flow_1(I_{LR↑}, I_{REF}) $$ where $V_1^0$ represents the flow at scale 0 (the original image scale). The upsampled LR image $I_{LR↑}$ is obtained via a single-image SR method:\n$$ I_{LR↑} = SISR(I_{LR}) $$ Then, the reference image is spatially warped using this flow to produce the pre-aligned reference:\n$$ \\hat{I}_{REF} = Warp(I_{REF}, V_1^0) $$ In the second stage, the pre-aligned reference \\( \\hat{I}_{\\mathrm{REF}} \\) and the upsampled LR image \\( I_{LR}\\uparrow \\) are again input to another flow estimator \\( Flow_2 \\) to compute multi-scale flow fields:\n$$ {V_2^3, V_2^2, V_2^1, V_2^0} = Flow_2(I_{LR↑}, \\hat{I}_{REF}) $$ These multi-scale flows are used later in the synthesis network to refine alignment and reconstruct the final super-resolved image.\nthis two-stage alignment, coarse warping followed by multi-scale refinement—allows CrossNet++ to handle large parallax and depth variations, achieving more accurate correspondence and better alignment quality than the original CrossNet.\n3.3.2 Encoder Through the alignment module, we obtain four flow fields at different scales. The encoder receives the pre-aligned reference image $\\hat I_{REF}$ and the upsampled LR image $I_{LR↑}$, then extracts their feature maps at four different scales.\nThe encoder has five convolutional layers with 64 filters of size ( 5 $\\times$ 5 ).\nThe first two layers (stride = 1) extract the feature map at scale 0. The next three layers (stride = 2) produce lower-resolution feature maps for scales 1 to 3. These operations are defined as: where $\\sigma$ is the ReLU activation, $*_1$ and $*_2$ denote convolutions with strides 1 and 2 respectively, and $F^i$ is the feature map at scale $i$.\n$$ F^0 = \\sigma(W^0 *_{1} I) $$ $$ F^i = \\sigma(W^i *_{2} F^{i-1}), \\\\ \\quad i = 1, 2, 3, $$ Unlike the original CrossNet, CrossNet++ uses a shared encoder for both $\\hat I_{REF}$ and $I_{LR↑}$ instead of two separate encoders, which reduces about 0.41 M parameters while maintaining accuracy.\nThe resulting feature sets are:\n$$ {F^0_{LR}, F^1_{LR}, F^2_{LR}, F^3_{LR}} \\quad \\text{and} \\quad {F^0_{REF}, F^1_{REF}, F^2_{REF}, F^3_{REF}}. $$ Finally, each reference feature map $F^i_{REF}$ is warped using the multi-scale flow fields $V^i_2$ from to produce the aligned feature maps:\n$$ \\hat{F}^i_{REF} = Warp(F^i_{REF}, V^i_2), \\\\ \\quad i = 0, 1, 2, 3. $$ In short, the encoder extracts multi-scale feature maps for both LR and reference images using shared convolutional layers, then aligns the reference features to the LR features through warping with multi-scale flow fields, which provides precise, scale-consistent alignment for the next fusion step.\n3.3.3 Decoder After feature extraction and alignment, the decoder fuses the LR and reference feature maps and generates the final super-resolved image.\nIt follows a U-Net-like structure, which progressively upsamples the feature maps from coarse to fine scales.\nTo create the decoder features at scale $i$, the model concatenates:\nthe warped reference features $\\hat{F}^i_{REF}$, the LR image features $F^i_{LR}$ , and the decoder feature from the next coarser scale $F^{i+1}_{D}$ (if available). Then a deconvolution layer (stride 2, filter size 4 $\\times$ 4) is applied:\n$$ F^3_{D} = \\sigma(W^3_{D} *_{2} (F^3_{LR}, \\hat{F}^3_{REF})) $$ where $*_2$ is deconvolution with stride 2 and $\\sigma$ is the activation (ReLU).\n$$ F^i_{D} = \\sigma(W^i_{D} *_{2} (F^i_{LR}, \\hat{F}^i_{REF}, F^{i+1}_{D})), \\\\quad i = 2, 1, $$ After that, three more convolutional layers (filter sizes (5 $\\times$ 5), channels {64, 64, 3}) perform post-fusion to synthesize the final image $I_p$:\n$$ F^0_{D} / F_1 = \\sigma(W_1 *_{1} (F^0_{LR}, \\hat{F}^0_{REF}, F^1_{D})) $$ $$ F_2 = \\sigma(W_2 *_{1} F_1) $$ $$ I_p = \\sigma(W_p *_{1} F_2), $$ where $*_{1}$ means convolution with stride 1.\nThe decoder takes aligned multi-scale features from LR and reference images, fuses them step by step through deconvolutions and convolutions, and finally reconstructs the high-resolution output image $I_p$, the sharp, super-resolved result.\n3.4 Loss Function warping loss, landmark loss → encourage flow estimator to generate precise flow.\nsuper-resolution loss → is responsible for the final synthesized image.\n3.4.1 Warping Loss Used in the first-stage Flow Estimator to regularize the generated optical flow.\nIt ensures that the warped reference image $\\hat I_{REF}$ is close to the ground-truth HR image $I_{HR}$, assuming both share a similar viewpoint.\nThe loss minimizes pixel-wise intensity differences:\n$$ L_{warp} = \\frac{1}{2N} \\sum_{i,s,c} (\\hat{I}_{REF}(s, c) - I_{HR}(s, c))^2 $$ where $N$ is the total number of samples, $i$, $s$, and $c$ iterate over training samples, spatial locations and color channels respectively.\n3.4.2 Landmark Loss This loss provides directional geometric guidance for large-parallax cases.\nIt uses SIFT feature matching to find corresponding landmark pairs $(p, q)$ between the HR and reference images, and applies the flow field $V^0_1$ to warp these landmarks.\nThe warped landmark $\\hat{p}^j$ is computed as:\n$$ \\hat{p}^j = p^j + V^0_1[p^j] $$ and the landmark loss penalizes the distance between warped and target landmarks:\n$$ L_{lm} = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{m_i} | \\hat{p}^j - q^j |_2^2 $$ where $m_i$ is the number of landmark pairs in image $i$.\nThis term helps the flow estimator predict more accurate motion fields, especially when viewpoints differ greatly.\n3.4.3 Super-Resolution Loss This loss directly trains the model to synthesize the final super-resolved image $I_p$, comparing it with the ground-truth high-resolution image $I_{HR}$ using the Charbonnier penalty (a smooth $L_1$ loss):\n$$ L_{sr} = \\frac{1}{N} \\sum_{i,s,c} \\rho(I_{HR}(s, c) - I_p(s, c)) $$ $$ \\rho(x) = \\sqrt{x^2 + 0.001^2}. $$ 4 Experiment Flower dataset and LFVideo dataset\n14 $\\times$ 14 angular samples of size 376 $\\times$ 541. training and testing: central 8 $\\times$ 8 grid of angular samples top-left 320 $\\times$ 512 for training and testing training: 3243 images from Flower and 1080 images from LFVideo testing: 100 images from Flower and 270 images from LFVideo ","wordCount":"1433","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/posts/crossnet++_cross-scale_large-parallax_warping_for/660a95ba-789a-4e1a-81ea-223112f91ac2.png","datePublished":"2025-10-29T08:23:31Z","dateModified":"2025-10-29T08:23:31Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/posts/crossnet++_cross-scale_large-parallax_warping_for/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution</h1><div class=post-description>Paper-reading notes: CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution</div><div class=post-meta><span title='2025-10-29 08:23:31 +0000 +0000'>October 29, 2025</span>&nbsp;·&nbsp;<span>1433 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#crossnet-and-crossnet aria-label="CrossNet and CrossNet++">CrossNet and CrossNet++</a></li></ul><li><a href=#abstraction aria-label=Abstraction>Abstraction</a></li><li><a href=#1-introduction aria-label="1 Introduction">1 Introduction</a></li><li><a href=#2-related-work aria-label="2 Related Work">2 Related Work</a></li><li><a href=#3-preliminary-of-crossnet aria-label="3 Preliminary of CrossNet">3 Preliminary of CrossNet</a><ul><li><a href=#33-network-structure aria-label="3.3 Network Structure">3.3 Network Structure</a><ul><li><a href=#331-alignment-module aria-label="3.3.1 Alignment Module">3.3.1 Alignment Module</a></li><li><a href=#332-encoder aria-label="3.3.2 Encoder">3.3.2 Encoder</a></li><li><a href=#333-decoder aria-label="3.3.3 Decoder">3.3.3 Decoder</a></li></ul></li><li><a href=#34-loss-function aria-label="3.4 Loss Function">3.4 Loss Function</a><ul><li><a href=#341-warping-loss aria-label="3.4.1 Warping Loss">3.4.1 Warping Loss</a></li><li><a href=#342-landmark-loss aria-label="3.4.2 Landmark Loss">3.4.2 Landmark Loss</a></li><li><a href=#343-super-resolution-loss aria-label="3.4.3 Super-Resolution Loss">3.4.3 Super-Resolution Loss</a></li></ul></li></ul></li><li><a href=#4-experiment aria-label="4 Experiment">4 Experiment</a></li></ul></div></details></div><div class=post-content><aside><h2 id=crossnet-and-crossnet><strong>CrossNet</strong> and <strong>CrossNet++</strong><a hidden class=anchor aria-hidden=true href=#crossnet-and-crossnet>#</a></h2><p>Both are for <strong>Reference-Based Super-Resolution (RefSR),</strong> using a <strong>low-resolution (LR)</strong> image and a <strong>high-resolution (HR reference)</strong> image to make a sharper, high-quality output.</p><p>The performance of <strong>CrossNet</strong> drops with the increasing of perspective parallax, the improvement of <strong>CrossNet++:</strong></p><ul><li><strong>Two-stage warping</strong> → improves alignment</li><li><strong>Self-supervised flow estimation</strong> → uses <strong>FlowNet</strong> to estimate motion between LR and Ref images</li><li><strong>Cross-scale alignment</strong> → Aligns features at <strong>multiple resolutions</strong></li><li><strong>Hybrid loss functions</strong> → warping + landmark + super-resolution loss</li><li><strong>Real-world performance</strong> → produces smoother, <strong>sharper</strong>, and more realistic results, suitable for a variety of scenarios</li></ul></aside><h1 id=abstraction>Abstraction<a hidden class=anchor aria-hidden=true href=#abstraction>#</a></h1><p>CrossNet++ focuses on <strong>reference-based super-resolution (RefSR),</strong> improving a low-resolution image using a high-resolution reference from another camera. This task is hard because of <strong>large scale differences (8×)</strong> and <strong>big parallax (~10%)</strong> between the two views.</p><p>To solve this, CrossNet++ introduces an <strong>end-to-end two-stage network</strong> with:</p><ol><li><strong>Cross-scale warping modules,</strong> align images at multiple zoom levels to <strong>narrow down parallax</strong>, handle scale and <strong>parallax differences</strong>.</li><li><strong>Image encoder and fusion decoder,</strong> extract <strong>multi-scale features</strong> and combine them to reconstruct a high-quality super-resolved image.</li></ol><p>It uses new <strong>hybrid loss functions</strong> comprising warping loss, landmark loss and super-resolution loss to improve accuracy and stability by stabilizing the training of alignment module and helps to improve super-resolution performance.</p><aside><p><strong>two-stage wrapping, hybrid loss</strong></p></aside><h1 id=1-introduction>1 Introduction<a hidden class=anchor aria-hidden=true href=#1-introduction>#</a></h1><p>The development of method:</p><ol><li><strong>patch-matching</strong> + <strong>patch-synthesis +</strong> iteratively applying <strong>non-uniform warping</strong><ul><li><del>Causes <strong>grid artifacts,</strong> incapable of handling the <strong>non-rigid image deformation</strong></del></li><li>Directly warping between the low and high-resolution images is inaccurate.</li><li>Such iterative combination of patch matching and warping introduces heavy computational burden.</li></ul></li></ol><aside><p>The difference between <strong>rigid deformation</strong> and <strong>non-rigid deformation:</strong></p><ul><li><strong>Rigid deformation</strong> = viewpoint change, like camera movement.</li><li><strong>Non-rigid deformation</strong> = object itself changes shape (face expression, fabric fold, petal bending).</li></ul><p><strong>Grid artifact</strong>s = tiny square patterns caused by wrong image enlargement or alignment.</p></aside><ol><li><strong>warping</strong> + <strong>synthesis</strong><ul><li>It cannot effectively handle <strong>large-parallax</strong> cases that widely existed in real-world data.</li></ul></li><li><strong>pre-warping</strong> + <strong>re-warping</strong> + <strong>synthesis</strong><ul><li>CrossNet++ is a unified framework enabling fully <strong>end-to-end training</strong> which does not require pretraining the flow estimator.</li><li>Two-stage pipeline: Two-stage cross-scale warping module.<ul><li><strong>stage 1:</strong> Uses <strong>FlowNet</strong> to estimate motion (optical flow) between the low-resolution (LR) and reference (Ref) images <strong>without needing ground-truth flow</strong> (self-supervised). This produces a roughly aligned <strong>“warped-Ref”</strong> image.</li><li><strong>stage 2:</strong> Further refines alignment between the warped-Ref and LR image for <strong>more accurate warping</strong>.</li></ul></li><li>Hybrid loss: <strong>warping loss</strong>, <strong>landmark loss</strong> and super-resolution loss.<ul><li><strong>warping loss</strong>: supervise the flow estimation implicitly.</li><li><strong>landmark loss</strong>: supervise the flow estimation explicitly.</li></ul></li></ul></li></ol><aside><p><strong>Without ground-truth flow</strong> = the model learns to estimate motion <strong>on its own</strong>, using only the images, not any pre-labeled motion data.</p><ul><li><strong>Interpolation</strong> = predict inside known area</li><li><strong>Extrapolation</strong> = predict outside known area</li></ul></aside><h1 id=2-related-work>2 Related Work<a hidden class=anchor aria-hidden=true href=#2-related-work>#</a></h1><h1 id=3-preliminary-of-crossnet>3 Preliminary of CrossNet<a hidden class=anchor aria-hidden=true href=#3-preliminary-of-crossnet>#</a></h1><h2 id=33-network-structure>3.3 Network Structure<a hidden class=anchor aria-hidden=true href=#33-network-structure>#</a></h2><p><img alt=image.png loading=lazy src=/posts/crossnet++_cross-scale_large-parallax_warping_for/660a95ba-789a-4e1a-81ea-223112f91ac2.png></p><h3 id=331-alignment-module>3.3.1 Alignment Module<a hidden class=anchor aria-hidden=true href=#331-alignment-module>#</a></h3><p>The <strong>alignment module</strong> aims to align the reference image $I_{REF}$ with the low-resolution image $I_{LR}$. CrossNet++ adopts a <strong>warping-based alignment</strong> using <strong>two-stage optical flow estimation</strong>.</p><p>In the <strong>first stage</strong>, a modified <strong>FlowNet</strong> (denoted as $Flow_1$) predicts the flow field between an upsampled LR image $I_{LR↑}$ and the reference image $I_{REF}$:</p><div class=math>$$
V_1^0 = Flow_1(I_{LR↑}, I_{REF})
$$</div><p>where $V_1^0$ represents the flow at scale 0 (the original image scale). The upsampled LR image $I_{LR↑}$ is obtained via a single-image SR method:</p><div class=math>$$
I_{LR↑} = SISR(I_{LR})
$$</div><p>Then, the reference image is spatially warped using this flow to produce the pre-aligned reference:</p><div class=math>$$
\hat{I}_{REF} = Warp(I_{REF}, V_1^0)
$$</div><p>In the <strong>second stage</strong>, the pre-aligned reference \( \hat{I}_{\mathrm{REF}} \) and the upsampled LR image \( I_{LR}\uparrow \) are again input to another flow estimator \( Flow_2 \) to compute <strong>multi-scale flow fields</strong>:</p><div class=math>$$
{V_2^3, V_2^2, V_2^1, V_2^0} = Flow_2(I_{LR↑}, \hat{I}_{REF})
$$</div><p>These multi-scale flows are used later in the synthesis network to refine alignment and reconstruct the final super-resolved image.</p><p>this two-stage alignment, <strong>coarse warping</strong> followed by <strong>multi-scale refinement</strong>—allows CrossNet++ to handle <strong>large parallax</strong> and <strong>depth variations</strong>, achieving more accurate correspondence and better alignment quality than the original CrossNet.</p><h3 id=332-encoder>3.3.2 Encoder<a hidden class=anchor aria-hidden=true href=#332-encoder>#</a></h3><p>Through the alignment module, we obtain four flow fields at different scales. The <strong>encoder</strong> receives the pre-aligned reference image $\hat I_{REF}$ and the upsampled LR image $I_{LR↑}$, then extracts their feature maps at <strong>four different scales</strong>.</p><p>The encoder has <strong>five convolutional layers</strong> with 64 filters of size ( 5 $\times$ 5 ).</p><ul><li>The <strong>first two layers</strong> (stride = 1) extract the feature map at scale 0.</li><li>The <strong>next three layers</strong> (stride = 2) produce lower-resolution feature maps for scales 1 to 3.</li></ul><p>These operations are defined as:
where $\sigma$ is the ReLU activation, $*_1$ and $*_2$ denote convolutions with strides 1 and 2 respectively, and $F^i$ is the feature map at scale $i$.</p><div class=math>$$
F^0 = \sigma(W^0 *_{1} I)
$$</div><div class=math>$$
F^i = \sigma(W^i *_{2} F^{i-1}), \\ \quad i = 1, 2, 3,
$$</div><p>Unlike the original CrossNet, <strong>CrossNet++ uses a shared encoder</strong> for both $\hat I_{REF}$ and <em>$I_{LR↑}$</em> instead of two separate encoders, which reduces about <strong>0.41 M parameters</strong> while maintaining accuracy.</p><p>The resulting feature sets are:</p><div class=math>$$
{F^0_{LR}, F^1_{LR}, F^2_{LR}, F^3_{LR}} \quad \text{and} \quad {F^0_{REF}, F^1_{REF}, F^2_{REF}, F^3_{REF}}.
$$</div><p>Finally, each reference feature map $F^i_{REF}$ is <strong>warped</strong> using the multi-scale flow fields $V^i_2$ from to produce the <strong>aligned feature maps</strong>:</p><div class=math>$$
\hat{F}^i_{REF} = Warp(F^i_{REF}, V^i_2), \\ \quad i = 0, 1, 2, 3.
$$</div><p>In short, the <strong>encoder</strong> extracts <strong>multi-scale feature maps</strong> for both LR and reference images using shared convolutional layers, then aligns the reference features to the LR features through warping with multi-scale flow fields, which provides precise, scale-consistent alignment for the next fusion step.</p><h3 id=333-decoder>3.3.3 Decoder<a hidden class=anchor aria-hidden=true href=#333-decoder>#</a></h3><p>After feature extraction and alignment, the <strong>decoder</strong> fuses the LR and reference feature maps and generates the final <strong>super-resolved image</strong>.</p><p>It follows a <strong>U-Net-like structure</strong>, which progressively upsamples the feature maps from coarse to fine scales.</p><p>To create the decoder features at scale $i$, the model <strong>concatenates</strong>:</p><ul><li>the warped reference features  $\hat{F}^i_{REF}$,</li><li>the LR image features $F^i_{LR}$ ,</li><li>and the decoder feature from the next coarser scale $F^{i+1}_{D}$ (if available).</li></ul><p>Then a <strong>deconvolution layer</strong> (stride 2, filter size 4 $\times$ 4) is applied:</p><div class=math>$$
F^3_{D} = \sigma(W^3_{D} *_{2} (F^3_{LR}, \hat{F}^3_{REF}))
$$</div><p>where $*_2$ is deconvolution with stride 2 and $\sigma$ is the activation (ReLU).</p><div class=math>$$
F^i_{D} = \sigma(W^i_{D} *_{2} (F^i_{LR}, \hat{F}^i_{REF}, F^{i+1}_{D})), \\quad i = 2, 1,
$$</div><p>After that, <strong>three more convolutional layers</strong> (filter sizes (5 $\times$ 5), channels {64, 64, 3}) perform <strong>post-fusion</strong> to synthesize the final image $I_p$:</p><div class=math>$$
F^0_{D} / F_1 = \sigma(W_1 *_{1} (F^0_{LR}, \hat{F}^0_{REF}, F^1_{D}))
$$</div><div class=math>$$
F_2 = \sigma(W_2 *_{1} F_1)
$$</div><div class=math>$$
I_p = \sigma(W_p *_{1} F_2),
$$</div><p>where $*_{1}$ means convolution with stride 1.</p><p>The decoder takes aligned multi-scale features from LR and reference images, fuses them step by step through deconvolutions and convolutions, and finally reconstructs the <strong>high-resolution output image</strong> $I_p$, the sharp, super-resolved result.</p><h2 id=34-loss-function>3.4 Loss Function<a hidden class=anchor aria-hidden=true href=#34-loss-function>#</a></h2><p>warping loss, landmark loss → encourage flow estimator to generate precise flow.</p><p>super-resolution loss → is responsible for the final synthesized image.</p><p><img alt=image.png loading=lazy src=/posts/crossnet++_cross-scale_large-parallax_warping_for/image.png></p><h3 id=341-warping-loss>3.4.1 Warping Loss<a hidden class=anchor aria-hidden=true href=#341-warping-loss>#</a></h3><p>Used in the <strong>first-stage Flow Estimator</strong> to regularize the generated optical flow.</p><p>It ensures that the warped reference image  $\hat I_{REF}$ is close to the ground-truth HR image $I_{HR}$, assuming both share a similar viewpoint.</p><p>The loss minimizes pixel-wise intensity differences:</p><div class=math>$$
L_{warp} = \frac{1}{2N} \sum_{i,s,c} (\hat{I}_{REF}(s, c) - I_{HR}(s, c))^2
$$</div><p>where $N$ is the total number of samples, $i$, $s$, and $c$ iterate over training samples, spatial locations and color channels respectively.</p><h3 id=342-landmark-loss>3.4.2 Landmark Loss<a hidden class=anchor aria-hidden=true href=#342-landmark-loss>#</a></h3><p>This loss provides <strong>directional geometric guidance</strong> for large-parallax cases.</p><p>It uses <strong>SIFT feature matching</strong> to find corresponding <strong>landmark pairs</strong> $(p, q)$ between the HR and reference images, and applies the flow field $V^0_1$ to warp these landmarks.</p><p>The warped landmark $\hat{p}^j$ is computed as:</p><div class=math>$$
\hat{p}^j = p^j + V^0_1[p^j]
$$</div><p>and the <strong>landmark loss</strong> penalizes the distance between warped and target landmarks:</p><div class=math>$$
L_{lm} = \frac{1}{2N} \sum_{i=1}^{N} \sum_{j=1}^{m_i} | \hat{p}^j - q^j |_2^2
$$</div><p>where $m_i$ is the number of landmark pairs in image $i$.</p><p>This term helps the flow estimator predict more accurate motion fields, especially when viewpoints differ greatly.</p><h3 id=343-super-resolution-loss>3.4.3 Super-Resolution Loss<a hidden class=anchor aria-hidden=true href=#343-super-resolution-loss>#</a></h3><p>This loss directly trains the model to synthesize the final <strong>super-resolved image</strong> $I_p$, comparing it with the ground-truth high-resolution image $I_{HR}$ using the <strong>Charbonnier penalty</strong> (a smooth $L_1$ loss):</p><div class=math>$$
L_{sr} = \frac{1}{N} \sum_{i,s,c} \rho(I_{HR}(s, c) - I_p(s, c))
$$</div><div class=math>$$
\rho(x) = \sqrt{x^2 + 0.001^2}.
$$</div><h1 id=4-experiment>4 Experiment<a hidden class=anchor aria-hidden=true href=#4-experiment>#</a></h1><p>Flower dataset and LFVideo dataset</p><ul><li>14 $\times$ 14 angular samples of size 376 $\times$ 541.</li><li>training and testing:<ul><li>central 8 $\times$ 8 grid of angular samples</li><li>top-left 320 $\times$ 512 for training and testing</li></ul></li><li><strong>training</strong>: 3243 images from Flower and 1080 images from LFVideo</li><li><strong>testing</strong>: 100 images from Flower and 270 images from LFVideo</li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>