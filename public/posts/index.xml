<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Re:Log</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Re:Log</description>
    <generator>Hugo -- 0.152.1</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 17 Jan 2026 15:06:44 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>UrbanLF: A Comprehensive Light Field Dataset for Semantic Segmentation of Urban Scenes</title>
      <link>http://localhost:1313/posts/2026-01-17-urbanlf-a-comprehensive-light-field-dataset-for-se/</link>
      <pubDate>Sat, 17 Jan 2026 15:06:44 +0000</pubDate>
      <guid>http://localhost:1313/posts/2026-01-17-urbanlf-a-comprehensive-light-field-dataset-for-se/</guid>
      <description>Paper-reading notes: UrbanLF</description>
    </item>
    <item>
      <title>Large Concept Models: Language Modeling in a Sentence Representation Space</title>
      <link>http://localhost:1313/posts/2026-01-15-large-concept-models-language-modeling-in-a-senten/</link>
      <pubDate>Thu, 15 Jan 2026 14:54:13 +0000</pubDate>
      <guid>http://localhost:1313/posts/2026-01-15-large-concept-models-language-modeling-in-a-senten/</guid>
      <description>Paper-reading notes: Large Concept Models: Language Modeling in a Sentence Representation Space</description>
    </item>
    <item>
      <title>From Tokens To Thoughts: How LLMs And Humans Trade Compression For Meaning</title>
      <link>http://localhost:1313/posts/2026-01-12-from-tokens-to-thoughts-how-llms-and-humans-trade/</link>
      <pubDate>Mon, 12 Jan 2026 14:49:45 +0000</pubDate>
      <guid>http://localhost:1313/posts/2026-01-12-from-tokens-to-thoughts-how-llms-and-humans-trade/</guid>
      <description>Paper-reading notes: From Tokens To Thoughts: How LLMs And Humans Trade Compression For Meaning</description>
    </item>
    <item>
      <title>RT Series</title>
      <link>http://localhost:1313/posts/2026-01-09-rt-series/</link>
      <pubDate>Fri, 09 Jan 2026 09:23:14 +0000</pubDate>
      <guid>http://localhost:1313/posts/2026-01-09-rt-series/</guid>
      <description>Paper-reading notes: RT-1 and RT-2</description>
    </item>
    <item>
      <title>Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</title>
      <link>http://localhost:1313/posts/2025-12-28-diffusion-policy-visuomotor-policy-learning-via-action-diffusion/</link>
      <pubDate>Sun, 28 Dec 2025 15:32:18 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-28-diffusion-policy-visuomotor-policy-learning-via-action-diffusion/</guid>
      <description>Paper-reading notes: Diffusion Policy</description>
    </item>
    <item>
      <title>Synthesizer: Rethinking Self-Attention for Transformer Models</title>
      <link>http://localhost:1313/posts/2025-12-16-synthesizer-rethinking-self-attention-for-transformer-models/</link>
      <pubDate>Tue, 16 Dec 2025 08:40:53 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-16-synthesizer-rethinking-self-attention-for-transformer-models/</guid>
      <description>Paper-reading notes: Synthesizer</description>
    </item>
    <item>
      <title>Learning Transformer Programs</title>
      <link>http://localhost:1313/posts/2025-12-15-learning-transformer-programs/</link>
      <pubDate>Mon, 15 Dec 2025 08:38:28 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-15-learning-transformer-programs/</guid>
      <description>Paper-reading notes: Learning Transformer Programs</description>
    </item>
    <item>
      <title>Reformer: The Efficient Transformer</title>
      <link>http://localhost:1313/posts/2025-12-14-reformer-the-efficient-transformer/</link>
      <pubDate>Sun, 14 Dec 2025 08:39:11 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-14-reformer-the-efficient-transformer/</guid>
      <description>Paper-reading notes: Reformer</description>
    </item>
    <item>
      <title>OpenVLA: An Open-Source Vision-Language-Action Model</title>
      <link>http://localhost:1313/posts/2025-12-12-openvla-an-open-source-vision-language-action-model/</link>
      <pubDate>Fri, 12 Dec 2025 08:37:15 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-12-openvla-an-open-source-vision-language-action-model/</guid>
      <description>Paper-reading notes: OpenVLA</description>
    </item>
    <item>
      <title>Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning</title>
      <link>http://localhost:1313/posts/2025-12-10-bayesian-optimization-is-superior-to-random-search-for-machine-learning-hyperparameter-tuning/</link>
      <pubDate>Wed, 10 Dec 2025 08:36:10 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-10-bayesian-optimization-is-superior-to-random-search-for-machine-learning-hyperparameter-tuning/</guid>
      <description>Paper-reading notes: Bayesian Optimization</description>
    </item>
    <item>
      <title>Random Search for Hyper-Parameter Optimization</title>
      <link>http://localhost:1313/posts/2025-12-10-random-search-for-hyper-parameter-optimization/</link>
      <pubDate>Wed, 10 Dec 2025 08:35:15 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-10-random-search-for-hyper-parameter-optimization/</guid>
      <description>Paper-reading notes: Random Search for Hyper-Parameter Optimization</description>
    </item>
    <item>
      <title>ALTA: Compiler-Based Analysis of Transformers</title>
      <link>http://localhost:1313/posts/2025-12-09-alta-compiler-based-analysis-of-transformers/</link>
      <pubDate>Tue, 09 Dec 2025 08:34:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-09-alta-compiler-based-analysis-of-transformers/</guid>
      <description>Paper-reading notes: ALTA</description>
    </item>
    <item>
      <title>Tracr: Compiled Transformers as a Laboratory for Interpretability</title>
      <link>http://localhost:1313/posts/2025-12-08-tracr-compiled-transformers-as-a-laboratory-for-interpretability/</link>
      <pubDate>Mon, 08 Dec 2025 08:32:26 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-08-tracr-compiled-transformers-as-a-laboratory-for-interpretability/</guid>
      <description>Paper-reading notes: Tracr</description>
    </item>
    <item>
      <title>Thinking Like Transformers</title>
      <link>http://localhost:1313/posts/2025-12-07-thinking-like-transformers/</link>
      <pubDate>Sun, 07 Dec 2025 15:14:48 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-07-thinking-like-transformers/</guid>
      <description>Paper-reading notes: RASP</description>
    </item>
    <item>
      <title>It’s All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization</title>
      <link>http://localhost:1313/posts/2025-12-06-its-all-connected-a-journey-through-test-time-memorization-attentional-bias-retention-and-online-optimization/</link>
      <pubDate>Sat, 06 Dec 2025 15:13:02 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-06-its-all-connected-a-journey-through-test-time-memorization-attentional-bias-retention-and-online-optimization/</guid>
      <description>Paper-reading notes: MIRAS</description>
    </item>
    <item>
      <title>FNet: Mixing Tokens with Fourier Transforms</title>
      <link>http://localhost:1313/posts/2025-12-05-fnet-mixing-tokens-with-fourier-transforms/</link>
      <pubDate>Fri, 05 Dec 2025 15:11:32 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-05-fnet-mixing-tokens-with-fourier-transforms/</guid>
      <description>Paper-reading notes: FNet</description>
    </item>
    <item>
      <title>Linformer: Self-Attention with Linear Complexity</title>
      <link>http://localhost:1313/posts/2025-12-04-linformer-self-attention-with-linear-complexity/</link>
      <pubDate>Thu, 04 Dec 2025 15:10:45 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-04-linformer-self-attention-with-linear-complexity/</guid>
      <description>Paper-reading notes: Linformer</description>
    </item>
    <item>
      <title>Rethinking Attention with Performers</title>
      <link>http://localhost:1313/posts/2025-12-03-rethinking-attention-with-performers/</link>
      <pubDate>Wed, 03 Dec 2025 15:09:23 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-03-rethinking-attention-with-performers/</guid>
      <description>Paper-reading notes: Performers</description>
    </item>
    <item>
      <title>On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning</title>
      <link>http://localhost:1313/posts/2025-12-01-on-the-representational-capacity-of-neural-language-models-with-chain-of-thought-reasoning/</link>
      <pubDate>Mon, 01 Dec 2025 08:49:03 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-12-01-on-the-representational-capacity-of-neural-language-models-with-chain-of-thought-reasoning/</guid>
      <description>Paper-reading notes: On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning</description>
    </item>
    <item>
      <title>What Formal Languages Can Transformers Express? A Survey</title>
      <link>http://localhost:1313/posts/2025-11-30-what-formal-languages-can-transformers-express-a-survey/</link>
      <pubDate>Sun, 30 Nov 2025 08:47:55 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-30-what-formal-languages-can-transformers-express-a-survey/</guid>
      <description>Paper-reading notes: What Formal Languages Can Transformers Express? A Survey</description>
    </item>
    <item>
      <title>ATLAS: Learning to Optimally Memorize the Context at Test Time</title>
      <link>http://localhost:1313/posts/2025-11-29-atlas-learning-to-optimally-memorize-the-context-at-test-time/</link>
      <pubDate>Sat, 29 Nov 2025 08:46:44 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-29-atlas-learning-to-optimally-memorize-the-context-at-test-time/</guid>
      <description>Paper-reading notes: ATLAS</description>
    </item>
    <item>
      <title>Solving olympiad geometry without human demonstrations</title>
      <link>http://localhost:1313/posts/2025-11-28-solving-olympiad-geometry-without-human-demonstrations/</link>
      <pubDate>Fri, 28 Nov 2025 08:42:56 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-28-solving-olympiad-geometry-without-human-demonstrations/</guid>
      <description>Paper-reading notes: AlphaGeometry</description>
    </item>
    <item>
      <title>Formal Mathematical Reasoning A New Frontier in AI</title>
      <link>http://localhost:1313/posts/2025-11-27-formal-mathematical-reasoning-a-new-frontier-in-ai/</link>
      <pubDate>Thu, 27 Nov 2025 08:42:29 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-27-formal-mathematical-reasoning-a-new-frontier-in-ai/</guid>
      <description>Paper-reading notes: Formal Mathematical Reasoning A New Frontier in AI</description>
    </item>
    <item>
      <title>Titans: Learning to Memorize at Test Time</title>
      <link>http://localhost:1313/posts/2025-11-26-titans-learning-to-memorize-at-test-time/</link>
      <pubDate>Wed, 26 Nov 2025 08:42:17 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-26-titans-learning-to-memorize-at-test-time/</guid>
      <description>Paper-reading notes: Titans</description>
    </item>
    <item>
      <title>Roformer: Enhanced Transformer With Rotary Position Embedding</title>
      <link>http://localhost:1313/posts/2025-11-25-roformer-enhanced-transformer-with-rotary-position-embedding/</link>
      <pubDate>Tue, 25 Nov 2025 08:40:15 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-25-roformer-enhanced-transformer-with-rotary-position-embedding/</guid>
      <description>Paper-reading notes: Roformer</description>
    </item>
    <item>
      <title>Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</title>
      <link>http://localhost:1313/posts/2025-11-24-mastering-chess-and-shogi-by-self-play-with-a-general-reinforcement-learning-algorithm/</link>
      <pubDate>Mon, 24 Nov 2025 08:39:45 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-24-mastering-chess-and-shogi-by-self-play-with-a-general-reinforcement-learning-algorithm/</guid>
      <description>Paper-reading notes: AlphaZero</description>
    </item>
    <item>
      <title>Mastering the game of Go without human knowledge</title>
      <link>http://localhost:1313/posts/2025-11-24-mastering-the-game-of-go-without-human-knowledge/</link>
      <pubDate>Mon, 24 Nov 2025 08:38:30 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-24-mastering-the-game-of-go-without-human-knowledge/</guid>
      <description>Paper-reading notes: AlphaGo Zero</description>
    </item>
    <item>
      <title>Disentangling Light Fields for Super-Resolution and Disparity Estimation</title>
      <link>http://localhost:1313/posts/2025-11-19-disentangling-light-fields-for-super-resolution-and-disparity-estimation/</link>
      <pubDate>Wed, 19 Nov 2025 07:42:14 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-19-disentangling-light-fields-for-super-resolution-and-disparity-estimation/</guid>
      <description>Paper-reading notes: Distangling mechanism, DistgSSR, DistgASR, DistgDisp</description>
    </item>
    <item>
      <title>Hyena Hierarchy: Towards Larger Convolutional Language Models</title>
      <link>http://localhost:1313/posts/2025-11-18-hyena-hierarchy-towards-larger-convolutional-language-models/</link>
      <pubDate>Tue, 18 Nov 2025 07:39:29 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-18-hyena-hierarchy-towards-larger-convolutional-language-models/</guid>
      <description>Paper-reading notes: Hyena Hierarchy</description>
    </item>
    <item>
      <title>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</title>
      <link>http://localhost:1313/posts/2025-11-17-mamba-linear-time-sequence-modeling-with-selective-state-spaces/</link>
      <pubDate>Mon, 17 Nov 2025 07:38:28 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-17-mamba-linear-time-sequence-modeling-with-selective-state-spaces/</guid>
      <description>Paper-reading notes: Mamba</description>
    </item>
    <item>
      <title>A survey for light field super-resolution</title>
      <link>http://localhost:1313/posts/2025-11-14-a-survey-for-light-field-super-resolution/</link>
      <pubDate>Fri, 14 Nov 2025 07:41:11 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-14-a-survey-for-light-field-super-resolution/</guid>
      <description>Paper-reading notes: A survey for light field super-resolution</description>
    </item>
    <item>
      <title>Efficiently Modeling Long Sequences with Structured State Spaces</title>
      <link>http://localhost:1313/posts/2025-11-11-efficiently-modeling-long-sequences-with-structured-state-spaces/</link>
      <pubDate>Tue, 11 Nov 2025 13:19:33 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-11-efficiently-modeling-long-sequences-with-structured-state-spaces/</guid>
      <description>Paper-reading notes: S4</description>
    </item>
    <item>
      <title>Retentive Network: A Successor to Transformer for Large Language Models</title>
      <link>http://localhost:1313/posts/2025-11-11-retentive-network-a-successor-to-transformer-for-large-language-models/</link>
      <pubDate>Tue, 11 Nov 2025 10:20:33 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-11-retentive-network-a-successor-to-transformer-for-large-language-models/</guid>
      <description>Paper-reading notes: RetNet</description>
    </item>
    <item>
      <title>Exploiting Spatial and Angular Correlations With Deep Efficient Transformers for Light Field Image Super-Resolution</title>
      <link>http://localhost:1313/posts/2025-11-10-exploiting-spatial-and-angular-correlations-with-deep-efficient-transformers-for-light-field-image-super-resolution/</link>
      <pubDate>Mon, 10 Nov 2025 13:22:13 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-10-exploiting-spatial-and-angular-correlations-with-deep-efficient-transformers-for-light-field-image-super-resolution/</guid>
      <description>Paper-reading notes: LF-DET</description>
    </item>
    <item>
      <title>Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</title>
      <link>http://localhost:1313/posts/2025-11-09-mixture-of-recursions-learning-dynamic-recursive-depths-for-adaptive-token-level-computation/</link>
      <pubDate>Sun, 09 Nov 2025 15:01:10 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-09-mixture-of-recursions-learning-dynamic-recursive-depths-for-adaptive-token-level-computation/</guid>
      <description>Paper-reading notes: Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</description>
    </item>
    <item>
      <title>Reference-Based Face Super-Resolution Using the Spatial Transformer</title>
      <link>http://localhost:1313/posts/2025-11-07-reference-based-face-super-resolution-using-the-spatial-transformer/</link>
      <pubDate>Fri, 07 Nov 2025 09:32:10 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-07-reference-based-face-super-resolution-using-the-spatial-transformer/</guid>
      <description>Paper-reading notes: Reference-Based Face Super-Resolution Using the Spatial Transformer</description>
    </item>
    <item>
      <title>LMR: A Large-Scale Multi-Reference Dataset for Reference-based Super-Resolution</title>
      <link>http://localhost:1313/posts/2025-11-07-lmr-a-large-scale-multi-reference-dataset-for-reference-based-super-resolution/</link>
      <pubDate>Fri, 07 Nov 2025 08:38:54 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-07-lmr-a-large-scale-multi-reference-dataset-for-reference-based-super-resolution/</guid>
      <description>Paper-reading notes: LMR - A Large-Scale Multi-Reference Dataset for Reference-based Super-Resolution</description>
    </item>
    <item>
      <title>Latent Diffusion Models</title>
      <link>http://localhost:1313/posts/2025-11-06-latent-diffusion-models/</link>
      <pubDate>Thu, 06 Nov 2025 21:19:54 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-06-latent-diffusion-models/</guid>
      <description>Paper-reading notes: High-Resolution Image Synthesis with Latent Diffusion Models</description>
    </item>
    <item>
      <title>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</title>
      <link>http://localhost:1313/posts/2025-11-04-deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning/</link>
      <pubDate>Tue, 04 Nov 2025 12:06:46 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-04-deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning/</guid>
      <description>Paper-reading notes: DeepSeek-R1 - Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</description>
    </item>
    <item>
      <title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
      <link>http://localhost:1313/posts/2025-11-03-an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/</link>
      <pubDate>Mon, 03 Nov 2025 12:36:53 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-03-an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/</guid>
      <description>Paper-reading notes: ViT</description>
    </item>
    <item>
      <title>A Tutorial on Bayesian Optimization</title>
      <link>http://localhost:1313/posts/2025-11-01-a-tutorial-on-bayesian-optimization/</link>
      <pubDate>Sat, 01 Nov 2025 23:05:46 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-11-01-a-tutorial-on-bayesian-optimization/</guid>
      <description>Paper-reading notes: A Tutorial on Bayesian Optimization</description>
    </item>
    <item>
      <title>CrossNet&#43;&#43;: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution</title>
      <link>http://localhost:1313/posts/2025-10-29-crossnet-cross-scale-large-parallax-warping-for-reference-based-super-resolution/</link>
      <pubDate>Wed, 29 Oct 2025 08:23:31 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-10-29-crossnet-cross-scale-large-parallax-warping-for-reference-based-super-resolution/</guid>
      <description>Paper-reading notes: CrossNet&#43;&#43;: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution</description>
    </item>
    <item>
      <title>xLSTM: Extended Long Short-Term Memory</title>
      <link>http://localhost:1313/posts/2025-10-28-xlstm-extended-long-short-term-memory/</link>
      <pubDate>Tue, 28 Oct 2025 13:18:30 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-10-28-xlstm-extended-long-short-term-memory/</guid>
      <description>Paper-reading notes: xLSTM Extended Long Short-Term Memory</description>
    </item>
    <item>
      <title>RWKV: Reinventing RNNs for the Transformer Era</title>
      <link>http://localhost:1313/posts/2025-10-27-rwkv-reinventing-rnns-for-the-transformer-era/</link>
      <pubDate>Mon, 27 Oct 2025 22:50:43 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-10-27-rwkv-reinventing-rnns-for-the-transformer-era/</guid>
      <description>Paper-reading notes: RWKV: Reinventing RNNs for the Transformer Era</description>
    </item>
    <item>
      <title>Mastering the game of Go with MCTS and Deep Neural Networks</title>
      <link>http://localhost:1313/posts/2025-10-24-mastering-the-game-of-go-with-mcts-and-deep-neural-networks/</link>
      <pubDate>Fri, 24 Oct 2025 10:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-10-24-mastering-the-game-of-go-with-mcts-and-deep-neural-networks/</guid>
      <description>Paper-reading notes: Mastering the game of Go with MCTS and Deep Neural Networks</description>
    </item>
    <item>
      <title>CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping</title>
      <link>http://localhost:1313/posts/2025-10-21-crossnet-an-end-to-end-reference-based-super-resolution-network-using-cross-scale-warping/</link>
      <pubDate>Tue, 21 Oct 2025 09:40:06 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-10-21-crossnet-an-end-to-end-reference-based-super-resolution-network-using-cross-scale-warping/</guid>
      <description>Paper-reading notes: CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping</description>
    </item>
    <item>
      <title>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</title>
      <link>http://localhost:1313/posts/2025-10-20-chain-of-thought-prompting-elicits-reasoning-in-large-language-models/</link>
      <pubDate>Mon, 20 Oct 2025 13:58:55 +0200</pubDate>
      <guid>http://localhost:1313/posts/2025-10-20-chain-of-thought-prompting-elicits-reasoning-in-large-language-models/</guid>
      <description>Paper-reading notes: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</description>
    </item>
    <item>
      <title>Learning‑based light field imaging</title>
      <link>http://localhost:1313/posts/2025-10-20-learningbased-light-field-imaging/</link>
      <pubDate>Mon, 20 Oct 2025 09:50:58 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-10-20-learningbased-light-field-imaging/</guid>
      <description>Paper-reading notes: Learning‑based light field imaging</description>
    </item>
    <item>
      <title>From Local to Global: A GraphRAG Approach to Query-Focused Summarization</title>
      <link>http://localhost:1313/posts/2025-10-16-from-local-to-global-a-graphrag-approach-to-query-focused-summarization/</link>
      <pubDate>Thu, 16 Oct 2025 19:42:01 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-10-16-from-local-to-global-a-graphrag-approach-to-query-focused-summarization/</guid>
      <description>Paper-reading notes: GraphRAG</description>
    </item>
    <item>
      <title>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</title>
      <link>http://localhost:1313/posts/2025-10-15-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks/</link>
      <pubDate>Wed, 15 Oct 2025 09:43:53 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-10-15-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks/</guid>
      <description>Paper-reading notes: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</description>
    </item>
    <item>
      <title>A Bridging Model for Parallel Computation</title>
      <link>http://localhost:1313/posts/2025-10-10-a-bridging-model-for-parallel-computation/</link>
      <pubDate>Fri, 10 Oct 2025 12:30:04 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-10-10-a-bridging-model-for-parallel-computation/</guid>
      <description>Paper-reading notes: A Bridging Model for Parallel Computation</description>
    </item>
    <item>
      <title>Attention is All You Need</title>
      <link>http://localhost:1313/posts/2025-10-01-attention-is-all-you-need/</link>
      <pubDate>Wed, 01 Oct 2025 00:36:58 +0000</pubDate>
      <guid>http://localhost:1313/posts/2025-10-01-attention-is-all-you-need/</guid>
      <description>Paper-reading notes: Attention is All You Need</description>
    </item>
  </channel>
</rss>
