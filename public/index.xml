<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Home</title>
    <link>https://my-blog-alpha-vert.vercel.app/</link>
    <description>Recent content on Home</description>
    <generator>Hugo -- 0.152.1</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Dec 2025 15:32:18 +0000</lastBuildDate>
    <atom:link href="https://my-blog-alpha-vert.vercel.app/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/diffusion_policy_visuomotor_policy_learning_via_ac/</link>
      <pubDate>Sun, 28 Dec 2025 15:32:18 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/diffusion_policy_visuomotor_policy_learning_via_ac/</guid>
      <description>Paper-reading notes: Diffusion Policy</description>
    </item>
    <item>
      <title>Synthesizer: Rethinking Self-Attention for Transformer Models</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/synthesizer_rethinking_self-attention_for_transfor/</link>
      <pubDate>Tue, 16 Dec 2025 08:40:53 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/synthesizer_rethinking_self-attention_for_transfor/</guid>
      <description>Paper-reading notes: Synthesizer</description>
    </item>
    <item>
      <title>Learning Transformer Programs</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/learning_transformer_programs/</link>
      <pubDate>Mon, 15 Dec 2025 08:38:28 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/learning_transformer_programs/</guid>
      <description>Paper-reading notes: Learning Transformer Programs</description>
    </item>
    <item>
      <title>Reformer: The Efficient Transformer</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/reformer_the_efficient_transformer/</link>
      <pubDate>Sun, 14 Dec 2025 08:39:11 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/reformer_the_efficient_transformer/</guid>
      <description>Paper-reading notes: Reformer</description>
    </item>
    <item>
      <title>OpenVLA: An Open-Source Vision-Language-Action Model</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/openvla_an_open-source_vision-language-action_mode/</link>
      <pubDate>Fri, 12 Dec 2025 08:37:15 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/openvla_an_open-source_vision-language-action_mode/</guid>
      <description>Paper-reading notes: OpenVLA</description>
    </item>
    <item>
      <title>Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/bayesian_optimization_is_superior_to_random_search/</link>
      <pubDate>Wed, 10 Dec 2025 08:36:10 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/bayesian_optimization_is_superior_to_random_search/</guid>
      <description>Paper-reading notes: Bayesian Optimization</description>
    </item>
    <item>
      <title>Random Search for Hyper-Parameter Optimization</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/random_search_for_hyper-parameter_optimization/</link>
      <pubDate>Wed, 10 Dec 2025 08:35:15 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/random_search_for_hyper-parameter_optimization/</guid>
      <description>Paper-reading notes: Random Search for Hyper-Parameter Optimization</description>
    </item>
    <item>
      <title>ALTA: Compiler-Based Analysis of Transformers</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/</link>
      <pubDate>Tue, 09 Dec 2025 08:34:00 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/</guid>
      <description>Paper-reading notes: ALTA</description>
    </item>
    <item>
      <title>Tracr: Compiled Transformers as a Laboratory for Interpretability</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/tracr_compiled_transformers_as_a_laboratory_for_in/</link>
      <pubDate>Mon, 08 Dec 2025 08:32:26 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/tracr_compiled_transformers_as_a_laboratory_for_in/</guid>
      <description>Paper-reading notes: Tracr</description>
    </item>
    <item>
      <title>Thinking Like Transformers</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/thinking_like_transformers/</link>
      <pubDate>Sun, 07 Dec 2025 15:14:48 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/thinking_like_transformers/</guid>
      <description>Paper-reading notes: RASP</description>
    </item>
    <item>
      <title>It’s All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/</link>
      <pubDate>Sat, 06 Dec 2025 15:13:02 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/</guid>
      <description>Paper-reading notes: MIRAS</description>
    </item>
    <item>
      <title>FNet: Mixing Tokens with Fourier Transforms</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/fnet_mixing_tokens_with_fourier_transforms/</link>
      <pubDate>Fri, 05 Dec 2025 15:11:32 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/fnet_mixing_tokens_with_fourier_transforms/</guid>
      <description>Paper-reading notes: FNet</description>
    </item>
    <item>
      <title>Linformer: Self-Attention with Linear Complexity</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/linformer_self-attention_with_linear_complexity/</link>
      <pubDate>Thu, 04 Dec 2025 15:10:45 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/linformer_self-attention_with_linear_complexity/</guid>
      <description>Paper-reading notes: Linformer</description>
    </item>
    <item>
      <title>Rethinking Attention with Performers</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/rethinking_attention_with_performers/</link>
      <pubDate>Wed, 03 Dec 2025 15:09:23 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/rethinking_attention_with_performers/</guid>
      <description>Paper-reading notes: Performers</description>
    </item>
    <item>
      <title>On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/on_the_representational_capacity_of_neural_languag/</link>
      <pubDate>Mon, 01 Dec 2025 08:49:03 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/on_the_representational_capacity_of_neural_languag/</guid>
      <description>Paper-reading notes: On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning</description>
    </item>
    <item>
      <title>What Formal Languages Can Transformers Express? A Survey</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/what_formal_languages_can_transformers_express_a_s/</link>
      <pubDate>Sun, 30 Nov 2025 08:47:55 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/what_formal_languages_can_transformers_express_a_s/</guid>
      <description>Paper-reading notes: What Formal Languages Can Transformers Express? A Survey</description>
    </item>
    <item>
      <title>ATLAS: Learning to Optimally Memorize the Context at Test Time</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/atlas_learning_to_optimally_memorize_the_context_a/</link>
      <pubDate>Sat, 29 Nov 2025 08:46:44 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/atlas_learning_to_optimally_memorize_the_context_a/</guid>
      <description>Paper-reading notes: ATLAS</description>
    </item>
    <item>
      <title>Solving olympiad geometry without human demonstrations</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/solving_olympiad_geometry_without_human_demonstrat/</link>
      <pubDate>Fri, 28 Nov 2025 08:42:56 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/solving_olympiad_geometry_without_human_demonstrat/</guid>
      <description>Paper-reading notes: AlphaGeometry</description>
    </item>
    <item>
      <title>Formal Mathematical Reasoning A New Frontier in AI</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/formal_mathematical_reasoning_a_new_frontier_in_ai/</link>
      <pubDate>Thu, 27 Nov 2025 08:42:29 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/formal_mathematical_reasoning_a_new_frontier_in_ai/</guid>
      <description>Paper-reading notes: Formal Mathematical Reasoning A New Frontier in AI</description>
    </item>
    <item>
      <title>Titans: Learning to Memorize at Test Time</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/titans_learning_to_memorize_at_test_time/</link>
      <pubDate>Wed, 26 Nov 2025 08:42:17 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/titans_learning_to_memorize_at_test_time/</guid>
      <description>Paper-reading notes: Titans</description>
    </item>
    <item>
      <title>Roformer: Enhanced Transformer With Rotary Position Embedding</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/roformer_enhanced_transformer_with_rotary_position/</link>
      <pubDate>Tue, 25 Nov 2025 08:40:15 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/roformer_enhanced_transformer_with_rotary_position/</guid>
      <description>Paper-reading notes: Roformer</description>
    </item>
    <item>
      <title>Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/</link>
      <pubDate>Mon, 24 Nov 2025 08:39:45 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/</guid>
      <description>Paper-reading notes: AlphaZero</description>
    </item>
    <item>
      <title>Mastering the game of Go without human knowledge</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/</link>
      <pubDate>Mon, 24 Nov 2025 08:38:30 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/</guid>
      <description>Paper-reading notes: AlphaGo Zero</description>
    </item>
    <item>
      <title>Disentangling Light Fields for Super-Resolution and Disparity Estimation</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/</link>
      <pubDate>Wed, 19 Nov 2025 07:42:14 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/</guid>
      <description>Paper-reading notes: Distangling mechanism, DistgSSR, DistgASR, DistgDisp</description>
    </item>
    <item>
      <title>Hyena Hierarchy: Towards Larger Convolutional Language Models</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/hyena_hierarchy_towards_larger_convolutional_langu/</link>
      <pubDate>Tue, 18 Nov 2025 07:39:29 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/hyena_hierarchy_towards_larger_convolutional_langu/</guid>
      <description>Paper-reading notes: Hyena Hierarchy</description>
    </item>
    <item>
      <title>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/mamba_linear-time_sequence_modeling_with_selective/</link>
      <pubDate>Mon, 17 Nov 2025 07:38:28 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/mamba_linear-time_sequence_modeling_with_selective/</guid>
      <description>Paper-reading notes: Mamba</description>
    </item>
    <item>
      <title>A survey for light field super-resolution</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/</link>
      <pubDate>Fri, 14 Nov 2025 07:41:11 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/</guid>
      <description>Paper-reading notes: A survey for light field super-resolution</description>
    </item>
    <item>
      <title>Efficiently Modeling Long Sequences with Structured State Spaces</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/efficiently_modeling_long_sequences_with_structure/</link>
      <pubDate>Tue, 11 Nov 2025 13:19:33 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/efficiently_modeling_long_sequences_with_structure/</guid>
      <description>Paper-reading notes: S4</description>
    </item>
    <item>
      <title>Retentive Network: A Successor to Transformer for Large Language Models</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/retentive_network_a_successor_to_transformer_for_l/</link>
      <pubDate>Tue, 11 Nov 2025 10:20:33 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/retentive_network_a_successor_to_transformer_for_l/</guid>
      <description>Paper-reading notes: RetNet</description>
    </item>
    <item>
      <title>Exploiting Spatial and Angular Correlations With Deep Efficient Transformers for Light Field Image Super-Resolution</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/exploiting_spatial_and_angular_correlations_with_d/</link>
      <pubDate>Mon, 10 Nov 2025 13:22:13 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/exploiting_spatial_and_angular_correlations_with_d/</guid>
      <description>Paper-reading notes: LF-DET</description>
    </item>
    <item>
      <title>Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/</link>
      <pubDate>Sun, 09 Nov 2025 15:01:10 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/</guid>
      <description>Paper-reading notes: Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</description>
    </item>
    <item>
      <title>Reference-Based Face Super-Resolution Using the Spatial Transformer</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/reference-based_face_super-resolution_using_the_sp/</link>
      <pubDate>Fri, 07 Nov 2025 09:32:10 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/reference-based_face_super-resolution_using_the_sp/</guid>
      <description>Paper-reading notes: Reference-Based Face Super-Resolution Using the Spatial Transformer</description>
    </item>
    <item>
      <title>LMR: A Large-Scale Multi-Reference Dataset for Reference-based Super-Resolution</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/lmr_a_large-scale_multi-reference_dataset_for_refe/</link>
      <pubDate>Fri, 07 Nov 2025 08:38:54 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/lmr_a_large-scale_multi-reference_dataset_for_refe/</guid>
      <description>Paper-reading notes: LMR - A Large-Scale Multi-Reference Dataset for Reference-based Super-Resolution</description>
    </item>
    <item>
      <title>Latent Diffusion Models</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/high-resolution_image_synthesis_with_latent_diffus/</link>
      <pubDate>Thu, 06 Nov 2025 21:19:54 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/high-resolution_image_synthesis_with_latent_diffus/</guid>
      <description>Paper-reading notes: High-Resolution Image Synthesis with Latent Diffusion Models</description>
    </item>
    <item>
      <title>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/deepseek-r1_incentivizing_reasoning_capability_in/</link>
      <pubDate>Tue, 04 Nov 2025 12:06:46 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/deepseek-r1_incentivizing_reasoning_capability_in/</guid>
      <description>Paper-reading notes: DeepSeek-R1 - Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</description>
    </item>
    <item>
      <title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/vit/</link>
      <pubDate>Mon, 03 Nov 2025 12:36:53 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/vit/</guid>
      <description>Paper-reading notes: ViT</description>
    </item>
    <item>
      <title>A Tutorial on Bayesian Optimization</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/a_tutorial_on_bayesian_optimization/</link>
      <pubDate>Sat, 01 Nov 2025 23:05:46 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/a_tutorial_on_bayesian_optimization/</guid>
      <description>Paper-reading notes: A Tutorial on Bayesian Optimization</description>
    </item>
    <item>
      <title>CrossNet&#43;&#43;: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/crossnet&#43;&#43;_cross-scale_large-parallax_warping_for/</link>
      <pubDate>Wed, 29 Oct 2025 08:23:31 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/crossnet&#43;&#43;_cross-scale_large-parallax_warping_for/</guid>
      <description>Paper-reading notes: CrossNet&#43;&#43;: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution</description>
    </item>
    <item>
      <title>xLSTM: Extended Long Short-Term Memory</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/xlstm_extended_long_short-term_memory/</link>
      <pubDate>Tue, 28 Oct 2025 13:18:30 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/xlstm_extended_long_short-term_memory/</guid>
      <description>Paper-reading notes: xLSTM Extended Long Short-Term Memory</description>
    </item>
    <item>
      <title>RWKV: Reinventing RNNs for the Transformer Era</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/rwkv/</link>
      <pubDate>Mon, 27 Oct 2025 22:50:43 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/rwkv/</guid>
      <description>Paper-reading notes: RWKV: Reinventing RNNs for the Transformer Era</description>
    </item>
    <item>
      <title>Mastering the game of Go with MCTS and Deep Neural Networks</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/mastering-go-mcts/</link>
      <pubDate>Fri, 24 Oct 2025 10:00:00 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/mastering-go-mcts/</guid>
      <description>Paper-reading notes: Mastering the game of Go with MCTS and Deep Neural Networks</description>
    </item>
    <item>
      <title>CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/crossnet_an_end-to-end_reference-based_super_resol/</link>
      <pubDate>Tue, 21 Oct 2025 09:40:06 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/crossnet_an_end-to-end_reference-based_super_resol/</guid>
      <description>Paper-reading notes: CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping</description>
    </item>
    <item>
      <title>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/chain-of-thought-prompting/</link>
      <pubDate>Mon, 20 Oct 2025 13:58:55 +0200</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/chain-of-thought-prompting/</guid>
      <description>Paper-reading notes: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</description>
    </item>
    <item>
      <title>Learning‑based light field imaging</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/learningbased_light_field_imaging/</link>
      <pubDate>Mon, 20 Oct 2025 09:50:58 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/learningbased_light_field_imaging/</guid>
      <description>Paper-reading notes: Learning‑based light field imaging</description>
    </item>
    <item>
      <title>From Local to Global A GraphRAG Approach to Query-</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/from_local_to_global_a_graphrag_approach_to_query-/</link>
      <pubDate>Thu, 16 Oct 2025 19:42:01 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/from_local_to_global_a_graphrag_approach_to_query-/</guid>
      <description>Paper-reading notes: From Local to Global A GraphRAG Approach to Query-</description>
    </item>
    <item>
      <title>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/retrieval-augmented_generation_for_knowledge-inten/</link>
      <pubDate>Wed, 15 Oct 2025 09:43:53 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/retrieval-augmented_generation_for_knowledge-inten/</guid>
      <description>Paper-reading notes: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</description>
    </item>
    <item>
      <title>A Bridging Model for Parallel Computation</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/a_bridging_model_for_parallel_computation/</link>
      <pubDate>Fri, 10 Oct 2025 12:30:04 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/a_bridging_model_for_parallel_computation/</guid>
      <description>Paper-reading notes: A Bridging Model for Parallel Computation</description>
    </item>
    <item>
      <title>Attention is All You Need</title>
      <link>https://my-blog-alpha-vert.vercel.app/notes/attention_is_all_you_need/</link>
      <pubDate>Wed, 01 Oct 2025 00:36:58 +0000</pubDate>
      <guid>https://my-blog-alpha-vert.vercel.app/notes/attention_is_all_you_need/</guid>
      <description>Paper-reading notes: Attention is All You Need</description>
    </item>
  </channel>
</rss>
