<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Learning‚Äëbased light field imaging | Home</title>
<meta name="keywords" content="">
<meta name="description" content="Paper-reading notes: Learning‚Äëbased light field imaging">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/notes/learningbased_light_field_imaging/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css" integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx&#43;pA=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/selfile.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/selfile.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/selfile.png">
<link rel="apple-touch-icon" href="http://localhost:1313/selfile.png">
<link rel="mask-icon" href="http://localhost:1313/selfile.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/notes/learningbased_light_field_imaging/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="" crossorigin="anonymous"></script>
<script defer>
  document.addEventListener("DOMContentLoaded", function() {
    if (typeof renderMathInElement === 'function') {
      renderMathInElement(document.body, {
        
        delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "\\[", right: "\\]", display: true},
          {left: "$", right: "$", display: false},
          {left: "\\(", right: "\\)", display: false}
        ],
        
        throwOnError: false,
        
        ignoredTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      });
    }
  });
</script>

  
  <style>
     
    ul#menu .tag-button {
      background: none;
      border: none;
      padding: 0;
      margin: 0;
      font-weight: 500;
      color: inherit;
      cursor: pointer;
      text-decoration: none;
      opacity: 0.95;
      display: inline-block;
    }
    ul#menu .tag-button:hover { text-decoration: underline; }
    ul#menu .tag-button.active { text-decoration: underline; font-weight: 600; }

     
    body.is-home .post-entry { display: none; }
    body.is-home .page-footer .pagination { display: none; }
  </style>

  
  <script>
    (function(){
      function isRootPath() {
        const p = location.pathname.replace(/\/+/g, '/');
        return p === '/' || p === '' || p === '/index.html';
      }

      if (isRootPath()) document.body.classList.add('is-home');

      
      document.addEventListener('DOMContentLoaded', function(){
        const menu = document.getElementById('menu');
        if (!menu) return;
        
        if (menu.querySelector('.tag-button')) return;
        const tags = ['Notes','Thoughts','Projects'];
        tags.forEach(t => {
          const li = document.createElement('li');
          const a = document.createElement('a');
          a.className = 'tag-button';
          a.href = `/tags/${t.toLowerCase()}/`;
          a.textContent = t;
          
          if (location.pathname.toLowerCase().startsWith(`/tags/${t.toLowerCase()}`)) a.classList.add('active');
          li.appendChild(a);
          menu.appendChild(li);
        });
      });
    })();
  </script>
<meta property="og:url" content="http://localhost:1313/notes/learningbased_light_field_imaging/">
  <meta property="og:site_name" content="Home">
  <meta property="og:title" content="Learning‚Äëbased light field imaging">
  <meta property="og:description" content="Paper-reading notes: Learning‚Äëbased light field imaging">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:published_time" content="2025-10-20T09:50:58+00:00">
    <meta property="article:modified_time" content="2025-10-20T09:50:58+00:00">
      <meta property="og:image" content="http://localhost:1313/notes/learningbased_light_field_imaging/24d86bc0-b749-4efe-a843-7697bc1ee29d.png">
      <meta property="og:image" content="http://localhost:1313/notes/learningbased_light_field_imaging/723a41c2-b8dc-40fe-b731-51f898768fc5.png">
      <meta property="og:image" content="http://localhost:1313/notes/learningbased_light_field_imaging/7c3c1972-3fe9-448c-9ba7-8d1f317a76d9.png">
      <meta property="og:image" content="http://localhost:1313/notes/learningbased_light_field_imaging/b25bef84-1e89-45c5-99e6-be4fb1cd2252.png">
      <meta property="og:image" content="http://localhost:1313/notes/learningbased_light_field_imaging/ef74cbaf-cb55-4a3d-99e0-83a8608b8442.png">
      <meta property="og:image" content="http://localhost:1313/notes/learningbased_light_field_imaging/f4e2a250-9bef-4ac9-97a2-9a8e092a88b7.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:1313/notes/learningbased_light_field_imaging/24d86bc0-b749-4efe-a843-7697bc1ee29d.png">
<meta name="twitter:title" content="Learning‚Äëbased light field imaging">
<meta name="twitter:description" content="Paper-reading notes: Learning‚Äëbased light field imaging">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "http://localhost:1313/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Learning‚Äëbased light field imaging",
      "item": "http://localhost:1313/notes/learningbased_light_field_imaging/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Learning‚Äëbased light field imaging",
  "name": "Learning‚Äëbased light field imaging",
  "description": "Paper-reading notes: Learning‚Äëbased light field imaging",
  "keywords": [
    
  ],
  "articleBody": "Initial Impression üëâüèº The light field refres to the representation of a scene. Unlike the tranditional 2D image, it adds an extra dimention: the direction of light. This means each image captures not only the length and width, but also the direction of light.\nTraditional image: 2D spatially (width, height), but each pixel carries a 3-value vector (RGB, 3 channels for color). Light filed(width, heidht, X-direction, Y-direction): 4D images, but each pixel carries a 3-value vector. As a result, light field data is massive and challenging to process.\nTo address it, researchers have turned into machine learning to efficiently handle and enhance light field imaging.\nThe paper mentions 4 main areas that AI can helps:\nDepth estimation Reconstruction Compression Quelity evaluation Abstract üëâüèº Content:\nBackground Motivation Research focus Paper content the existing learning-based solutions frameworks evaluation methods datasets future research directions Background Traditional cameras capture only 2D images. Light field imaging records the direction of light rays, enabling realistic and immersive 3D-like representations. Motivation Light field data provides richer visual information but comes with large data volume and high computational cost. Machine learning and deep networks offer efficient and intelligent ways to process this data. Research Focus Learning-based methods are applied to:\nDepth estimation Reconstruction and super-resolution Compression and quality enhancement Paper Goals The paper surveys existing learning-based techniques, summarizes the most promising frameworks, reviews current datasets, and evaluation methods and outlook for future research directions.\n1. Introduction üëâüèº Content:\nConcept and potential of light field imaging Market growth and application fields Technical challenges and processing tasks Shift toward learning-based solutions Purpose and structure of the paper Concept and Potential Light field imaging is a promising 3D imaging technology that records light rays traveling through every point in space and in every direction. Unlike 2D photography, it captures angular information, providing a sense of depth, realism, and immersion. This enables photo-realistic rendering and supports 6-DoF (Degrees of Freedom) experiences for next-generation immersive media, broadcasting, and gaming. Market and Applications The light field market is expanding rapidly, driven by glasses-free 3D displays and multi-view visualization systems. It supports a range of applications such as virtual reality, augmented reality, 3D reconstruction, and computational photography. Challenges and Processing Tasks High-dimensional light field data introduces issues like data redundancy, storage complexity, and inter-view correlation. Essential processing tasks include: Spatial and angular super-resolution to enhance image quality Compression algorithms for efficient data storage and transmission Depth estimation for 3D scene reconstruction These tasks are more complex than traditional 2D image processing due to added angular dimensions. Shift Toward Learning-Based Solutions Traditional geometry-based methods struggle with large datasets and occlusion problems. Deep learning and data-driven frameworks now dominate, improving efficiency and performance in reconstruction, compression, and depth estimation. Learning frameworks enable automation and scalability, addressing the computational challenges of high-dimensional data. Purpose and Structure of the Paper This review provides a comprehensive overview of learning-based solutions for light field imaging. It discusses: Fundamentals of light field imaging and data acquisition Key processing tasks and related learning frameworks Benchmark datasets and evaluation methods Current challenges and future research directions The goal is to summarize progress, identify open issues, and provide a roadmap for future research in learning-based light field processing. 2. Light field imaging background üëâüèº Content:\nLight field fundamentals Light field acquisition Light field visualization 2.1 Light Field Fundamentals üëâüèº Content:\nConcept of the Plenoptic Function Dimensional Reduction Light Field Representation Concept of the Plenoptic Function To reproduce realistic 3D scenes, cameras must capture light from many viewpoints.\nA light field describes all light rays in 3D space ‚Äî their positions, directions, colors, and intensity.\nThe complete description is given by the plenoptic function, a 7-dimensional (7D) function:\n$$ P(Œ∏, œÜ, Œª, œÑ, V_x, V_y, V_z) $$\nwhich includes direction, wavelength, time, and position of every ray in space.\nDimensional Reduction Capturing the full 7D plenoptic function is practically impossible. Under constant lighting and static scenes, wavelength (Œª) and time (œÑ) can be ignored. This simplifies the representation to a 5D function: $$ P(Œ∏, œÜ, V_x, V_y, V_z) $$\nThe 5D form forms the foundation for Neural Radiance Fields (NeRF), while further simplification leads to a 4D light field representation. Light Field Representation Two-Plane Parameterization\nThe 4D light field assumes light rays travel in straight lines. Each ray is defined by its intersection with two parallel planes: (u, v) ‚Üí spatial coordinates on the image/focal plane (Œ©) (s, t) ‚Üí coordinates on the camera plane (Œ†) The resulting function: $$ P(u, v, s, t) $$\ndefines the light field in terms of spatial and angular information. This two-plane parameterization makes light fields easier to store, process, and compress using 2D image arrays. A 4D light field can be expressed as an array of 2D images indexed by (u, v) for spatial coordinates and (s, t) for different viewpoints. This enables the use of standard 2D codecs for compression and native 2D algorithms for processing. Epipolar Plane Image (EPI)\nEpipolar Plane Image (EPI), a 2D slice of the light field showing spatial‚Äìangular relationships. Looks like a single 2D picture made by stacking the same pixel row from all those camera views. Coordinates ‚Üí (u, s) or (v, t) (only one spatial + one view direction) In the EPI, each object in the scene becomes a slanted line: If an object is close, its line is steeper (because it moves more between views). If it‚Äôs far, its line is flatter (moves less). Lines in the EPI correspond to scene depth ‚Äî analyzing their slopes allows depth estimation and 3D reconstruction. Epipolar Plane Image (EPI)\n2.2 Light Field Acquisition üëâüèº Acquisition methods:\nSingle-Camera Systems Multi-Camera Arrays Other Methods Computer-generated light fields Handheld or SLAM-based systems LiDAR-assisted capture Overview Light fields can be acquired by single plenoptic cameras, camera arrays, or synthetic and LiDAR-based systems, each balancing data density, resolution, and complexity.\nSingle-Camera Systems Plenoptic (lenslet-based) cameras use microlens arrays to capture dense angular information in one shot. Examples: Raytrix and Lytro cameras. Lytro ‚Üí The Lytro camera captures a 4-D light field by recording many tiny viewpoint images through its microlens array ‚Äî those images form the SAI stack. Multi-Camera Arrays Arrays of monocular cameras (planar or spherical) capture scenes from different viewpoints. The camera layout defines the angular sampling and overall field of view. Other Methods Computer-generated light fields provide accurate depth maps for benchmarking. Handheld or SLAM-based systems reconstruct light fields from multiple frames. LiDAR-assisted capture combines sensors and cameras for precise, automated 3D data. 2.3 Light field visualization üëâüèº Content:\nGoal and use cases Visualization approaches Display principles and challenges Perceptual limitations Goal and Use Cases The main goal is to provide a true 3D visual experience, essential for immersive and interactive applications. Visualization may be passive (no interaction) or active (viewer can move or rotate objects). Used in areas like medical imaging, VR/AR, and 3D display systems. Visualization Approaches Based on the 4D light-field representation combining spatial and angular data. Passive use cases render fixed views; active use cases synthesize new views via view interpolation. (View interpolation means creating new viewpoints images that were never actually captured by a real camera, but are mathematically generated from nearby real ones. Rendering quality strongly influences perceptual realism. Display Principles and Challenges Two key properties: Angular resolution (baseline between views) the baseline = the distance between two camera viewpoints. Small baseline ‚Üí cameras are close together Wide baseline ‚Üí cameras are far apart Spatial resolution (image detail) 3D displays must reproduce directional light rays accurately to recreate depth and realism. Capturing dense samples and rendering multiple view angles remain computationally demanding. Perceptual Limitations Standard 2D displays lack realistic cues like vergence and accommodation, limiting immersion. Light-field displays address this by replicating real light rays to the viewer‚Äôs eyes but face: Finite ray sampling Vergence‚Äìaccommodation conflict Restricted field of view (FoV) Horizontal- or vertical-parallax-only designs causing viewing discomfort. 3. Learning‚Äëbased light field processing üëâüèº Content:\nDepth estimation Autoencoders Stereo matching and refinement End‚Äëto‚Äëend feature extraction and disparity regression Light field reconstruction Spatial super‚Äëresolution Single‚Äëview super‚Äëresolution and refinement End‚Äëto‚Äëend residual learning Angular super‚Äëresolution EPI super‚Äëresolution Depth estimation and warping Multi‚Äëplane image generation Spatio‚Äëangular reconstruction Neural scene representation Compression Learning‚Äëbased view synthesis on the decoder side Learning‚Äëbased view synthesis on the encoder and decoder sides End‚Äëto‚Äëend light field compression architecture Other light field imaging applications This section summarizes the most prominent light field processing tasks studied in the literature and highlights the learning-based imaging techniques deployed for each processing task.\nTerms and explanation:\nTerm Simple meaning Example / Analogy EPI volume Stack of many light-field slices showing how points move across views Like stacking many thin image strips to form a 3D block Stereo view Two or more photos of the same scene from different angles Like your left and right eye views SAI stack Group of small 2D images from a light field camera Like a grid of mini photos from slightly different directions Depth map Image showing distance of each pixel (white = near, black = far) Like a 3D scanner‚Äôs output Disparity volume 3D data showing pixel shifts between views Large shift ‚Üí close object, small shift ‚Üí far object Details about the difference between Light Field representation:\nTerm Relation to Two-Plane Parameterization Simple meaning EPI A slice through the Two-Plane representation. Shows pixel movement (as slanted lines) between multiple views ‚Äî used for depth estimation. Stereo view it‚Äôs a simpler / smaller subset of the Two-Plane model (only 2 views). Like taking only the left and right images from the light field. SAI stack it‚Äôs directly sampled from the Two-Plane model (a grid of (s, t) views, each with its own (u, v) image). Many small 2D images from slightly different viewpoints ‚Äî a practical way to store the light field. Relationship between Disparity volume and Depth map:\nConcept Type What it represents Used for Disparity volume 3D data (x, y, disparity) All possible matching shifts between views Intermediate step (to find the best match) Depth map 2D image (x, y), 3D data (x, y, depth) Actual distance of each pixel Final result Disparity and depth are inversely related:\n$$ \\text{Depth} = \\frac{f \\times B}{\\text{Disparity}} $$\nwhere:\n(f) = focal length of the camera, (B) = distance between two camera views (baseline). So:\nLarge disparity (big shift) ‚Üí close object. Small disparity (tiny shift) ‚Üí far object.\nDepth map\n3.1 Depth estimation üëâüèº Three main approaches:\nAutoencoders Stereo matching and refinement End-to-end feature extraction and disparity regression Depth estimation model architecture\nOverview Goal: Estimate the distance of each pixel from the camera to recover the scene‚Äôs 3D structure. Light field imaging enables capturing a scene from multiple viewpoints so depth information is implicitly encoded in the light field representation and can be acquired by computing the inter-view pixel disparity information. disparity volume ‚Üí depth map Main challenges: occlusions, non-Lambertian surfaces, and texture-less regions, which make accurate estimation difficult. A Lambertian surface is an ideal matte surface ‚Äî it reflects light equally in all directions. That means no matter where you look from, the brightness of that point stays the same. Recent progress focuses on learning-based approaches, achieving higher accuracy than traditional geometry-based methods. Autoencoders Take an EPI volume ‚Üí compress it (encoder) ‚Üí learn the hidden features ‚Üí expand it (decoder).\nClassical method:\nHeber \u0026 Pock: five-block CNN estimating line orientation in EPIs; Orientation refers to the direction each camera is facing. later extended to a U-shaped encoder‚Äìdecoder, Further to U-shaped encoder‚Äìdecoder with skip connections and 3D filters. Step Explanation Input Horizontal and vertical EPI volumes (from the light field). Each EPI shows slanted lines ‚Äî the slope encodes depth. Model A 5-block CNN that scans small ‚Äúwindows‚Äù (patches) of the EPI. Each CNN layer extracts features and estimates the orientation (slope) of the EPI lines. Later versions evolved into a U-shaped encoder‚Äìdecoder (U-Net) for better reconstruction. Output A depth map, where every pixel‚Äôs value = estimated distance from the camera. Alperovich et al.:\nan autoencoder that encodes horizontal and vertical EPI stacks simultaneously using six stages of residual blocks to improve robustness. Then, the compressed representation is expanded using three decoder pathways to address the disparity, diffusion, and specularity estimation problems. Step Explanation Input Combined horizontal + vertical EPI stacks from the light field. Model An autoencoder with: 6 residual blocks (for stronger feature extraction) in the encoder, and 3 decoder branches (to handle disparity, diffusion, specularity). It compresses the light field into a latent code, then reconstructs richer outputs. Output A disparity volume ‚Äî a 3D array where each pixel position stores multiple disparity hypotheses (possible shifts). From this volume, a final depth map can be derived later. Analysis:\nPros: captures compact depth features, handles EPI geometry directly. Cons: computationally heavy; limited to 2D EPI slices, less effective in occluded regions. Stereo Matching and Refinement Computes disparity between SAIs using neural stereo-matching networks.\nTypical pipeline:\nCoarse disparity estimation via networks like FlowNet 2.0 or encoder‚Äìdecoder CNNs. Refinement using residual or occlusion-aware learning to correct depth errors. Examples:\nRogge et al. (belief propagation + residual refinement);\nGuo et al. (encoder‚Äìdecoder concatenation of SAIs).\nAnalysis:\nPros: exploits full 4D light-field correlations, good for complex geometry. Cons: high computation cost; sensitive to reflections and non-Lambertian surfaces. End-to-End Feature Extraction and Disparity Regression Fully end-to-end CNNs learn features and regress depth directly.\nMethods:\nEpinet: Horizontal, vertical, and diagonal SAI stacks ‚Üí Multi-stream CNN feature extraction ‚Üí Regression network ‚Üí Depth map Leistner et al.: Vertical \u0026 horizontal SAI stacks ‚Üí Siamese U-Net ‚Üí Autoencoder regression module ‚Üí Classification + regression fusion ‚Üí Depth map Two-stream CNN: Horizontal \u0026 vertical EPIs ‚Üí Multi-scale feature extraction (four convolutional stages) ‚Üí Feature concatenation ‚Üí Multi-label regression ‚Üí Depth map Zhu et al.: Focal stacks + center view + EPIs ‚Üí Hybrid feature extraction ‚Üí Fully connected + softmax layers ‚Üí Pixel-wise disparity classification ‚Üí Depth map Tsai et al.: Multi-view SAIs ‚Üí Residual blocks + spatial pyramid pooling ‚Üí Cost volume construction + attention module ‚Üí Disparity regression ‚Üí Depth map Multi-scale Cost-Volume Method: Shifted SAI feature maps (multi-disparity levels) ‚Üí 4D cost volume (low memory footprint) ‚Üí Multi-scale feature extraction ‚Üí Regression ‚Üí Depth map Analysis:\nPros: highest accuracy, unified optimization of feature + disparity learning. Cons: large data/training demand; reduced performance on wide-baseline scenes. üëâüèº Summary:\nDepth estimation for the wide baseline scenario, with an acceptable trade-off between accuracy and computation, is still an open research problem.\n3.2 Light field reconstruction üëâüèº Content:\nSpatial super‚Äëresolution Single‚Äëview super‚Äëresolution and refinement End‚Äëto‚Äëend residual learning Angular super‚Äëresolution EPI super‚Äëresolution Depth estimation and warping Multi‚Äëplane image generation Spatio‚Äëangular reconstruction Neural scene representation To enable higher spatial and angular resolutions, the development of light field reconstruction/ super-resolution (SR) methods has gained significant attention.\nSpatial SR = make each image (each view) sharper ‚Äî more pixels, more detail. Angular SR = make more viewpoints ‚Äî fill in missing views between existing ones. üí° Spatial super-resolution (SR) focuses on improving the static spatial resolution ‚Äî that is, the sharpness, clarity, and detail of each individual sub-aperture image (still view).\nAngular consistency ensures smooth and coherent transitions between different viewpoints, maintaining stable motion perception and correct 3D geometry when the view changes or the scene is refocused.\nLight-field SR must improve both:\nSpatial resolution ‚Üí each view looks higher spatial resolution. Angular consistency ‚Üí all views agree about geometry and depth. System Spatial resolution (image sharpness) Angular resolution (number of viewpoints) Baseline Notes Plenoptic camera Low High (dense) Narrow Compact but blurry Camera rig High Low (sparse) Wide Sharp but heavy and complex Spatial super‚Äëresolution Examples of architectures for the spatial, angular, and spatio-angular super-resolution (SR) frameworks.\na ‚Üí Single-view SR using single image super-resolution (SISR) network and inter-view enhancement, b ‚Üí end-to-end residual learning, c ‚Üí warping and residual learning for refinement, d ‚Üí multi-plane image generation, e ‚Üí residual learning using 4D CNNs and refinement, **** f ‚Üí GAN-based method Single‚Äëview super‚Äëresolution and refinement\nStep Explanation Input (SAI stack LR) SAI stack LR: all the sub-aperture images captured by the light-field camera. low-resolution ‚Äî each view is blurry and lacks details (because each microlens only captures a small number of pixels). For example, 9√ó9¬†SAIs¬†of¬†60√ó60¬†pixels. SISR Single-Image Super-Resolution: This block applies a 2D super-resolution network (e.g., VDSR) to each SAI individually to ****improve the spatial resolution. HR(init)LF High-Resolution (initial) Light Field: Each image looks sharper, but the views may no longer be perfectly aligned (angular inconsistency). EPI EPIs (Epipolar Plane Images) are extracted. EPIs capture the geometric relationships between views ‚Äî how pixels shift across viewpoints (slanted lines). Refinement module This is a learning network (often a CNN) that analyzes the EPIs to find and correct misalignments between neighboring SAIs. Making sure all views agree in depth and structure. Residual addition Combine sharpness from the first stage and consistency from the second stage. Output (HR LF) High-Resolution Light Field: The final output light field has: High spatial resolution (sharp individual views), and High angular consistency (smooth geometry across views). For example, 9√ó9¬†SAIs¬†of¬†240√ó240¬†pixels. End‚Äëto‚Äëend residual learning\nStep Explanation Input (SAI stack) SAI stacks (Horizontal, Vertical, Diagonal, Center View), Center view ‚Üí the middle image, used as a geometric reference Feature Extraction Each SAI stack is passed through a feature extraction CNN. Extract features from each SAI direction, like ‚Äúwhat the object looks like‚Äù and ‚Äúhow it moves between views.‚Äù Feature Integration \u0026 Processing The feature maps from all directions (horizontal, vertical, diagonal, etc.) are merged or fused here. Upsampling Network Increases the spatial resolution (i.e., number of pixels). Output (HR LF) The final output is a high-resolution light field ‚Äî a grid of SAIs that are: Spatially sharper and Angularly consistent. Upsampling means making an image larger ‚Äî that is, increasing its resolution by creating more pixels.\nAngular super‚Äëresolution Angular Super-Resolution (SR) means synthesizing new in-between views to make the light field smoother and more complete. (view synthesis)\nEPI super‚Äëresolution\nAll of these models have the same basic structure:\nStart from a blurry / low-angular-resolution light field (few viewpoints). Use deep networks to predict high-frequency details (the fine geometry and textures missing in the low version). Reconstruct the high-angular-resolution (HR) light field, which includes the new views. Step Explanation Input (SAI stack LR) SAI stack LR: all the sub-aperture images captured by the light-field camera. low-resolution ‚Äî each view is blurry and lacks details (because each microlens only captures a small number of pixels). For example, 9√ó9¬†SAIs¬†of¬†60√ó60¬†pixels. SISR Single-Image Super-Resolution: This block applies a 2D super-resolution network (e.g., VDSR) to each SAI individually to ****improve the spatial resolution. HR(init)LF High-Resolution (initial) Light Field: Each image looks sharper, but the views may no longer be perfectly aligned (angular inconsistency). EPI EPIs (Epipolar Plane Images) are extracted. EPIs capture the geometric relationships between views ‚Äî how pixels shift across viewpoints (slanted lines). Refinement module This is a learning network (often a CNN) that analyzes the EPIs to find and correct misalignments between neighboring SAIs. Making sure all views agree in depth and structure. Residual addition Combine sharpness from the first stage and consistency from the second stage. Output (HR LF) High-Resolution Light Field: The final output light field has: High spatial resolution (sharp individual views), and High angular consistency (smooth geometry across views). For example, 9√ó9¬†SAIs¬†of¬†240√ó240¬†pixels. Depth estimation and warping\nDisparity is the shift of the same object‚Äôs position between two different views. Warping = using disparity to reposition pixels. Step Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Depth Estimator Predicts a depth map (distance information) for each SAI or for the entire light field. It learns how far each pixel is from the camera by analyzing geometric cues across views. (wraps_by_an_amount_that_depends_on_its_depth) Warping Module Uses the estimated depth maps to warp (geometrically align) all SAIs toward a common viewpoint ‚Äî usually the center view. ‚ÄúWarping‚Äù means shifting pixels according to their depth so that corresponding points from different views line up. Refinement Module Fine-tunes the warped images using a CNN. Corrects small errors from imperfect depth estimation or warping. Output (HR LF) The final reconstructed light field: All SAIs are now high-resolution (sharp textures). The views are geometrically aligned (consistent depth perception). Some basics:\nConcept Meaning Why it matters Depth map Tells how far each pixel is Needed to compute correct pixel shift Disparity The actual shift caused by viewpoint change Derived from depth Warping Moves pixels according to disparity Aligns all views Without depth All pixels shift equally ‚Üí wrong alignment Causes blur and ghosting Multi‚Äëplane image generation (MPI generation)\nStep Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Warping Module The Warping Module aligns all SAIs to a reference view (usually the center view) using a range of hypothetical depth planes. Plane-sweep volume After warping, stacks all these reprojected images together ‚Äî forming a plane-sweep volume. Create a 3D data structure that encodes how well each depth hypothesis aligns across views. No explicit depth estimator ‚Äî depth is implicitly encoded in the plane-sweep volume. 3D CNN The 3D convolutional neural network processes the plane-sweep volume to analyze spatial and depth correlations. Multi-plane image A multi-plane image (MPI) ‚Äî a set of 2D images, each representing a scene layer at a specific depth, with color + transparency (Œ±) values. Blending Module Combines (blends) all the multi-plane images (depth layers) into a single coherent high-resolution light field. Output (HR LF) The final light field: All SAIs are now high-resolution and geometrically aligned. Spatio‚Äëangular reconstruction Residual learning using 4D CNNs and refinement\nStep Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Warping Module The Warping Module aligns all SAIs to a reference view (usually the center view) using a range of hypothetical depth planes. 4D CNN A convolutional network that operates directly on the 4D light-field volume. It jointly learns: Spatial features (edges, textures inside each SAI) and Angular features (parallax). The 4D CNN predicts the missing high-frequency details for all views simultaneously. Residual Connection This ‚Äúresidual learning‚Äù means the 4D CNN only learns the difference (the missing fine details) rather than reconstructing the entire image from scratch. Refinement Module Polish the HR LF and ensure angular consistency. Output (HR LF) Output: HR LFThe final output = high-resolution light field ‚Üí same number of views as the input, ‚Üí each view sharper (higher spatial resolution) and smoothly aligned (angular consistency). GAN-based method\nStep Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Generator The generator learns to upsample the SAIs and restore missing details (edges, textures, angular consistency). GAN Discriminator It is trained on real HR light fields (ground truth) and fake HR light fields (from the generator). Its job is to classify them as real or fake. Force the generator to create results that are indistinguishable from real data ‚Äî not just pixel-wise accurate but also visually realistic (better textures, depth edges, lighting). Output (HR LF) The generator alone can take any new LR light field and output an HR version that: has high spatial detail, maintains angular consistency, and looks visually realistic (not over-smoothed). üí° Summary: Limitations of LF Reconstruction Techniques: Early reconstruction methods Slow to run. Trained for fixed view sampling patterns ‚Üí hard to generalize to new setups. Single-view SR methods (Fig. 3a) Each sub-aperture image (SAI) is processed separately. Causes geometric inconsistency between views because inter-view information isn‚Äôt used. Depth-based \u0026 warping methods (Fig. 3c) Work better for wide-baseline cases (larger view spacing). Depend heavily on accurate depth maps ‚Üí errors lead to tearing, ghosting, and problems with non-Lambertian (reflective) surfaces. MPI-based methods (Fig. 3d) Memory-hungry and slow to train (often need multiple GPUs and days of training). Model size grows with number of depth planes (larger depth budget ‚Üí bigger model). Can assign wrong opacity to layers ‚Üí causes blurry reconstructions. 4D CNN methods (Fig. 3e) Produce high-quality results, but have high computational cost due to expensive 4D convolutions. GAN-based methods (Fig. 3f) Need large training datasets. Training can suffer from instability and mode collapse (generator producing limited or repetitive outputs). Neural scene representation Researchers started using neural networks to represent 3D scenes which replaces 2D pictures.\nFrom image-based to neural 3D representations\nEarlier works used image-based rendering (combine nearby views). Recent advances use neural networks to represent and render 3D scenes directly. 3D representations can be: Explicit: meshes, voxels, point clouds. Implicit: continuous functions learned by networks + differentiable ray marching. NeRF ‚Äî the core idea\nNeural Radiance Fields (NeRF) represent a scene using an MLP (multi-layer perceptron). The network takes 5D coordinates ‚Üí (x, y, z, Œ∏, œÜ): spatial position + viewing direction. It outputs: Density (geometry) Color (view-dependent radiance). Rendering is done by volume rendering along camera rays. üí° How NeRF learns the scene:\nFeed the network 2D photos of the same scene taken from different camera angles, and you must know exactly where each camera was (its pose = position + orientation). ‚Üí So NeRF knows which pixel in which image corresponds to which ray in 3D space. NeRF renders its current guess (density and color) of those images using its internal 3D representation. It compares them to the real photos (pixel by pixel). It updates its weights using backpropagation to minimize the difference. To render one image (for a camera view):\nFor each pixel, shoot a ray through the 3D scene.\nSample many points along that ray (like tiny steps through space).\nAt each point, ask the network for its color and density.\nCombine (accumulate) all samples along the ray using a differentiable volume rendering formula:\n$$ C = \\sum_i T_i (1 - e^{-\\sigma_i \\Delta_i}) c_i $$\nwhere:\nC = final pixel color, $\\sigma_i$ = density, $c_i$ = color, $T_i$ = how much light passes through before reaching this point. This process is known as differentiable ray marching.\nSo the process works like this:\nCollect multiple 2D photos of the same scene, taken from different camera angles (with known position and orientation: x, y, z, Œ∏, œÜ). For each pixel in each photo, sample many 3D points along the corresponding camera ray. For each sampled point, use the network to predict its color and density (or depth). Aggregate the information from all sampled points to estimate the final color of the pixel. Compare the predicted image to the real photo, pixel by pixel, and update the model based on the error. NeRF uses a coarse network (for rough geometry) and a fine network (for detailed structure). Analysis: Pros: realistic view synthesis. Cons: very slow training and rendering. Methods to improve speed and efficiency\nSeveral approaches speed up NeRF by changing how the scene is represented, they replaced the big neural network with simpler structures:\nMethod Representation Key idea Pros / Cons Voxel-based NeRFs (Fridovich et al.) Sparse voxel grid Store opacity + spherical harmonic coefficients Faster, but memory-heavy TensoRF 4D tensors (low-rank decomposition) Factorize radiance field into compact tensor components Efficient, compact SNeRG Sparse 3D voxel grid with color \u0026 features Encodes view-dependent effects Fast rendering; still large memory Octree NeRF (Yu et al.) Octree structure (adaptive voxels) Sample dense regions more finely Faster, but higher memory cost NeX Extended MPI (Multi-Plane Image) Model color as function of viewing angle using spherical bases Better view-dependent rendering Ray-space embedding 4D ray embedding ‚Üí latent space Compact feature embedding Memory efficient but slower rendering KiloNeRF Thousands of tiny MLPs Divide scene into grid cells, each with a small network Great speed, coarse structure 3D Gaussian representation Continuous Gaussians Skip empty-space computation Near real-time rendering Instant-NGP (M√ºller et al.) Hash-based encoding Store features in hash table for fast lookup Very fast, compact, GPU-friendly Methods to improve quality\nVariant Key idea Benefit Mip-NeRF Multi-scale cones for anti-aliased rendering Handles different resolutions \u0026 reduces aliasing NeRF++ Two networks: near-field \u0026 far-field (spherical) Better for unbounded, complex scenes Mip-NeRF 360 Extends Mip-NeRF for unbounded scenes; uses nonlinear parameterization \u0026 regularization High-quality large-scene rendering Trade-offs and current challenges:\nFast methods (gaussian) ‚Üí train quickly, but lose visual quality. High-quality methods (NeRF, Mip-NeRF 360) ‚Üí photorealistic but slow to train. Explicit methods also can‚Äôt be optimized directly with gradients ‚Üí need to convert trained implicit NeRFs into their format (adds complexity). Real-time + high-quality rendering remains an open research challenge. 3.3 Compression üí° Three main strategies:\nLearning-based view synthesis on the decoder side; Learning-based view synthesis on the encoder and decoder sides; End-to-end light field compression architecture; An efficient codec should be able to explore:\nnot only the spatial and angular redundancies independently (as two-dimensional data), but also the combined spatial‚Äìangular redundancy (4D data). The key idea of this compression architecture is bitrate saving by sparsely encoding the views.\nLearning-based view synthesis on the decoder side Step Explanation Input light fields These are the original dense light-field data, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints. Key SAIs selection Instead of sending all the views, we select a few key SAIs. These key SAIs contain enough angular and spatial information to reconstruct the missing ones later. Encoder The encoder compresses these key SAIs into a bitstream (binary data) for transmission or storage. Bitstream This is the compressed data that‚Äôs transmitted or saved. It contains only the encoded information of the key SAIs (no non-key views). Decoder The decoder reconstructs the key SAIs from the bitstream. Learning-based view synthesis This is a deep neural network trained to synthesize new views (non-key SAIs) from the nearby key SAIs. Generate all non-key views ‚Üí fill in the gaps to recreate the full light field. Decoded light fields Produce and output the complete high-quality light field (same size as the original) after combining Decoded key SAIs and Generated non-key SAIs. Learning-based view synthesis on the encoder and decoder sides Idea:\nEncode only a few key views (the main ones), Use deep learning to predict or reconstruct the missing views (non-key views), Send only the residual errors to refine those predictions. Step Explanation Input light fields These are the original dense light-field data, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints. Key SAIs and Non-key SAIs The system splits the light field into: Key SAIs: a few representative images selected for transmission (e.g., every 3rd or 4th view). Non-key SAIs: the remaining views that will be predicted rather than transmitted. Encoder Takes two inputs: Key SAIs and Residuals (for the non-key SAIs), compresses both into a compact bitstream (binary data) for transmission or storage. Learning-based view synthesis (encoder side) This neural network predicts the non-key SAIs from the available key SAIs. so the encoder can calculate the prediction error (residual) ‚Üí the difference between the predicted and real non-key SAIs is computed as a residual. Residue The encoder subtracts the predicted non-key SAI (from the network) from the actual non-key SAI. only encode the difference, not the entire image ‚Äî saves lots of bitrate. Bitstream The data sent or stored ‚Äî contains compressed key SAIs + residuals. Decoder Receives and decompresses the bitstream. Reconstructs the key SAIs first. Then passes them to the learning-based view synthesis network to generate predicted non-key SAIs. Learning-based view synthesis (decoder side) Same (or similar) neural network as on the encoder side. It uses the decoded key SAIs to synthesize (predict) the non-key SAIs. Then it adds the residual (decoded correction data) to refine those synthesized views. Addition (+) and output The synthesized non-key SAIs are added to the decoded residuals ‚Üí final accurate non-key views. Combine key + non-key SAIs ‚Üí get the fully decoded light field. üí° Su et al. ‚Üí\nInstead of treating every pixel separately, they group light rays that belong to the same 3D point in the scene. After grouping rays, some nearby super-rays may still be very similar. So they merge them into larger super-rays to save even more space. Compress each super-ray using a 4D Discrete Cosine Transform (DCT). a light field varies in both: Spatial dimensions (x, y) ‚Äî inside each image, Angular dimensions (u, v) ‚Äî across different viewpoints. Better captures 4D spatial‚Äìangular redundancy but is more complex. End-to-end light field compression architecture Step Explanation Input light fields These are the original dense light-field data, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints. Encoder This is the main compression network.It takes the input light field and learns to represent it using fewer numbers (features). Bitstream The bitstream is the final compressed data produced by the encoder. It contains the quantized latent features. Decoder The decoder takes the bitstream and reconstructs (decodes) the light field. It performs the reverse of the encoder: expands the compact features back into full-resolution sub-aperture images. Decoded Light Fields These are the reconstructed sub-aperture images (SAIs).Ideally, they look almost identical to the input views, with small errors due to compression. End-to-end schemes are gaining more attention due to their effectiveness in image compression.\nüí° View synthesis drawbacks can be circumvented by neural representations that achieve a level of detail that is challenging for traditional methods.\n3.4 Other light field imaging applications Light-field (LF) images are more powerful than normal 2D photos because they capture depth, focus, and parallax ‚Äî this allows better performance in many computer vision tasks.\nDeep learning methods are now being used to apply light-field data to various new areas.\nSaliency Detection (SOD) Goal: detect which objects or regions attract human attention. LF advantage: provides both spatial and angular information, giving richer clues about object boundaries and depth. Typical model: encoder‚Äìdecoder two-stream networks: One stream uses all-in-focus (center) images, The other stream uses focal stacks or multi-view features. A comprehensive review compares deep LF-SOD models with standard RGB-D models. Face Recognition Goal: identify faces more accurately using multi-view data from light fields. LF advantage: combines intra-view (within one image) and inter-view (across multiple angles) features. Methods: VGG features with LSTM layers to model view changes. Capsule networks with a pose matrix to handle viewpoint shifts. Datasets introduced: LFFW (Light Field Faces in the Wild), LFFC (Light Field Face Constrained) ‚Äî for benchmarking LF face recognition. Light Field Microscopy Goal: use LF imaging to capture and reconstruct 3D biological structures quickly. LF advantage: captures 3D spatial information in one camera shot ‚Üí instant 3D imaging. Deep learning usage: improves speed and quality of reconstructions. Methods: Encoder‚Äìdecoder networks convert 2D LF inputs to 3D volume data. Networks with 2D and 3D residual blocks enhance reconstruction quality. Convolutional sparse coding (CSC) networks use EPIs as input for fast neuron localization. Applications: real-time visualization of cardiovascular or neuronal activity. The network uses EPIs as inputs and generates sparse codes, representing depth data, as outputs. Other applications Image classification ‚Äî improves feature learning using angular cues. Low-light imaging ‚Äî LF data helps reconstruct clear images in dark conditions. Overall, these works show that learning-based light-field imaging provides richer 3D understanding and better accuracy than normal 2D or RGB-D methods.\n4. Datasets and quality assessment Datasets Characteristics of the light field datasets used to benchmark the light field imaging systems\nQuality Assessment Light field imaging algorithms are typically evaluated using quantitative methods by comparing generated data to a ground-truth. Due to the diversity of light field acquisition procedures, distortions, and rendering processes, light field quality assessment remains a challenging task. A recent focus has been on developing more accurate objective algorithms that extract features from both spatial and angular domains for light field quality assessment. Metrics can be classified into three categories based on the availability of the reference image: full-reference (FR), reduced-reference (RR), no-reference (NR). Developing NR metrics are gaining more attention due to their success in improving accuracy. Current learning-based light field algorithms are still only evaluated using conventional PSNR and SSIM methods. In this context, the IEEE established a new standard called ‚ÄôIEEE P3333.1.4‚Äô, which defines metrics and provides recommended practices for light field quality assessment. A standardization activity, namely ‚ÄôJPEG Pleno Quality Assessment‚Äô, was recently initiated within the JPEG committee aiming to explore the most promising subjective quality assessment practices as well as the objective methodologies for plenoptic modalities in the context of multiple use cases. Summary of the objective quality assessment methods for light fields\n5. Discussion, challenges and perspectives While parallel to light fields, other plenoptic modalities like point cloud and holography have also been developed. Even though point clouds and holographic content processing and compression have advanced significantly in recent years, these content types may eventually need to be converted to light field views for visualization on the display. Light fields provide more comprehensive information when it comes to capturing scenes. Light fields capture not only the 3D objects but also the entire scene information, which can be essential in many applications like autonomous driving, that require accurate 3D recreation of the vehicle surroundings. Recent advances in using deep learning for spatio-angular reconstruction and the emergence of the NeRF-based approaches. More recent methods, such as 3D Gaussian splatting, show improvements in the quality‚Äìspeed trade-off. Enhancing the quality‚Äìspeed trade-off could enable new use cases such as real-time telepresence and robotic tasks with fewer views for reconstruction. Two neural scene representations of light fields: an explicit representation based on multiple SAIs an implicit neural representation encodes light fields as parameters of an MLP. Advances in deep learning frameworks are expected to significantly improve the performance of light field processing algorithms and solve the existing challenges. Better depth estimation or depth-free approaches are critical. Advanced methods are needed to efficiently exploit the huge amount of redundant information about the light rays in the same scene that conveys angular and spatial information Now targeting the creation of a learning-based coding standard to provide competitive compression efficiency compared to state-of-the-art light field coding solutions. The evaluation of light field imaging systems has several shortfalls related to the content and assessment approaches available. Advanced methods are needed to efficiently exploit the huge amount of redundant information about the light rays in the same scene that conveys angular and spatial information. It is essential to provide more comprehensive light field datasets from both the quantitative and content diversity perspectives. The assessment of plenoptic image quality also faces various challenges because of the variety of quality aspects and complexity of the content when compared to the assessment of 2D images. JPEG has begun developing a light field quality assessment standard, defining a framework with subjective quality assessment protocols and objective quality assessment procedures for lossy decoding of light field data within the context of multiple use cases. The IEEE is also developing a standard called ‚ÄúP3333.1.4‚ÄîRecommended Practice for the Quality Assessment of light field Imaging‚Äù that targets to establish methods of quality assessment of light field imaging based on psychophysical studies 6. Conclusions üëâüèº Content:\nCore Focus Progress and trends Challenges Future outlook Core Focus Main Tasks Reviewed:\nDepth estimation, reconstruction, super-resolution, and compression. Other Tasks:\nMicroscopy, saliency, face recognition, refocusing, and relighting also benefit from learning-based methods. Progress and Trends Deep Learning Integration:\nAI frameworks now appear in almost every stage of light field processing. Growth Drivers:\nBetter capture and display hardware and larger datasets will accelerate progress. Challenges Limited Realism:\nCurrent systems have narrow Field of View (FoV) and Depth of Field (DoF); Still far from true 6-DoF free-view exploration. Data Burden:\nExpanding datasets increase computational cost and reduce processing efficiency. Future Outlook Compression Evolution:\nLearning-based image compression is expected to greatly improve light field storage and transmission, making real-world applications more feasible. ",
  "wordCount" : "6550",
  "inLanguage": "en",
  "image": "http://localhost:1313/notes/learningbased_light_field_imaging/24d86bc0-b749-4efe-a843-7697bc1ee29d.png","datePublished": "2025-10-20T09:50:58Z",
  "dateModified": "2025-10-20T09:50:58Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/notes/learningbased_light_field_imaging/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Home",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/selfile.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Home (Alt + H)">Home</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a class="tag-button" href="http://localhost:1313/tags/notes/">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a class="tag-button" href="http://localhost:1313/tags/thoughts/">
                    <span>Thoughts</span>
                </a>
            </li>
            <li>
                <a class="tag-button" href="http://localhost:1313/tags/projects/">
                    <span>Projects</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;¬ª&nbsp;<a href="/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Learning‚Äëbased light field imaging
    </h1>
    <div class="post-description">
      Paper-reading notes: Learning‚Äëbased light field imaging
    </div>
    <div class="post-meta"><span title='2025-10-20 09:50:58 +0000 +0000'>October 20, 2025</span>&nbsp;¬∑&nbsp;<span>6550 words</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#initial-impression" aria-label="Initial Impression">Initial Impression</a></li>
                <li>
                    <a href="#abstract" aria-label="Abstract">Abstract</a><ul>
                        <ul>
                        
                <li>
                    <a href="#background" aria-label="Background">Background</a></li>
                <li>
                    <a href="#motivation" aria-label="Motivation">Motivation</a></li>
                <li>
                    <a href="#research-focus" aria-label="Research Focus">Research Focus</a></li>
                <li>
                    <a href="#paper-goals" aria-label="Paper Goals">Paper Goals</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#1-introduction" aria-label="1. Introduction">1. Introduction</a><ul>
                        <ul>
                        
                <li>
                    <a href="#concept-and-potential" aria-label="Concept and Potential">Concept and Potential</a></li>
                <li>
                    <a href="#market-and-applications" aria-label="Market and Applications">Market and Applications</a></li>
                <li>
                    <a href="#challenges-and-processing-tasks" aria-label="Challenges and Processing Tasks">Challenges and Processing Tasks</a></li>
                <li>
                    <a href="#shift-toward-learning-based-solutions" aria-label="Shift Toward Learning-Based Solutions">Shift Toward Learning-Based Solutions</a></li>
                <li>
                    <a href="#purpose-and-structure-of-the-paper" aria-label="Purpose and Structure of the Paper">Purpose and Structure of the Paper</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#2-light-field-imaging-background" aria-label="2. Light field imaging background">2. Light field imaging background</a><ul>
                        
                <li>
                    <a href="#21-light-field-fundamentals" aria-label="2.1 Light Field Fundamentals">2.1 Light Field Fundamentals</a><ul>
                        
                <li>
                    <a href="#concept-of-the-plenoptic-function" aria-label="Concept of the Plenoptic Function">Concept of the Plenoptic Function</a></li>
                <li>
                    <a href="#dimensional-reduction" aria-label="Dimensional Reduction">Dimensional Reduction</a></li>
                <li>
                    <a href="#light-field-representation" aria-label="Light Field Representation">Light Field Representation</a></li></ul>
                </li>
                <li>
                    <a href="#22-light-field-acquisition" aria-label="2.2 Light Field Acquisition">2.2 Light Field Acquisition</a><ul>
                        
                <li>
                    <a href="#overview" aria-label="Overview">Overview</a></li>
                <li>
                    <a href="#single-camera-systems" aria-label="Single-Camera Systems">Single-Camera Systems</a></li>
                <li>
                    <a href="#multi-camera-arrays" aria-label="Multi-Camera Arrays">Multi-Camera Arrays</a></li>
                <li>
                    <a href="#other-methods" aria-label="Other Methods">Other Methods</a></li></ul>
                </li>
                <li>
                    <a href="#23-light-field-visualization" aria-label="2.3 Light field visualization">2.3 Light field visualization</a><ul>
                        
                <li>
                    <a href="#goal-and-use-cases" aria-label="Goal and Use Cases">Goal and Use Cases</a></li>
                <li>
                    <a href="#visualization-approaches" aria-label="Visualization Approaches">Visualization Approaches</a></li>
                <li>
                    <a href="#display-principles-and-challenges" aria-label="Display Principles and Challenges">Display Principles and Challenges</a></li>
                <li>
                    <a href="#perceptual-limitations" aria-label="Perceptual Limitations">Perceptual Limitations</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#3-learningbased-light-field-processing" aria-label="3. Learning‚Äëbased light field processing">3. Learning‚Äëbased light field processing</a><ul>
                        
                <li>
                    <a href="#31-depth-estimation" aria-label="3.1 Depth estimation">3.1 Depth estimation</a><ul>
                        
                <li>
                    <a href="#overview-1" aria-label="Overview">Overview</a></li>
                <li>
                    <a href="#autoencoders" aria-label="Autoencoders">Autoencoders</a></li>
                <li>
                    <a href="#stereo-matching-and-refinement" aria-label="Stereo Matching and Refinement">Stereo Matching and Refinement</a></li>
                <li>
                    <a href="#end-to-end-feature-extraction-and-disparity-regression" aria-label="End-to-End Feature Extraction and Disparity Regression">End-to-End Feature Extraction and Disparity Regression</a></li></ul>
                </li>
                <li>
                    <a href="#32-light-field-reconstruction" aria-label="3.2 Light field reconstruction">3.2 Light field reconstruction</a><ul>
                        
                <li>
                    <a href="#spatial-superresolution" aria-label="Spatial super‚Äëresolution">Spatial super‚Äëresolution</a></li>
                <li>
                    <a href="#angular-superresolution" aria-label="Angular super‚Äëresolution">Angular super‚Äëresolution</a></li>
                <li>
                    <a href="#spatioangular-reconstruction" aria-label="Spatio‚Äëangular reconstruction">Spatio‚Äëangular reconstruction</a></li>
                <li>
                    <a href="#summary-limitations-of-lf-reconstruction-techniques" aria-label="Summary: Limitations of LF Reconstruction Techniques:">Summary: Limitations of LF Reconstruction Techniques:</a></li>
                <li>
                    <a href="#neural-scene-representation" aria-label="Neural scene representation">Neural scene representation</a></li></ul>
                </li>
                <li>
                    <a href="#33-compression" aria-label="3.3 Compression">3.3 Compression</a><ul>
                        
                <li>
                    <a href="#learning-based-view-synthesis-on-the-decoder-side" aria-label="Learning-based view synthesis on the decoder side">Learning-based view synthesis on the decoder side</a></li>
                <li>
                    <a href="#learning-based-view-synthesis-on-the-encoder-and-decoder-sides" aria-label="Learning-based view synthesis on the encoder and decoder sides">Learning-based view synthesis on the encoder and decoder sides</a></li>
                <li>
                    <a href="#end-to-end-light-field-compression-architecture" aria-label="End-to-end light field compression architecture">End-to-end light field compression architecture</a></li></ul>
                </li>
                <li>
                    <a href="#34-other-light-field-imaging-applications" aria-label="3.4 Other light field imaging applications">3.4 Other light field imaging applications</a><ul>
                        
                <li>
                    <a href="#saliency-detection-sod" aria-label="Saliency Detection (SOD)">Saliency Detection (SOD)</a></li>
                <li>
                    <a href="#face-recognition" aria-label="Face Recognition">Face Recognition</a></li>
                <li>
                    <a href="#light-field-microscopy" aria-label="Light Field Microscopy">Light Field Microscopy</a></li>
                <li>
                    <a href="#other-applications" aria-label="Other applications">Other applications</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#4-datasets-and-quality-assessment" aria-label="4. Datasets and quality assessment">4. Datasets and quality assessment</a><ul>
                        
                <li>
                    <a href="#datasets" aria-label="Datasets">Datasets</a></li>
                <li>
                    <a href="#quality-assessment" aria-label="Quality Assessment">Quality Assessment</a></li></ul>
                </li>
                <li>
                    <a href="#5-discussion-challenges-and-perspectives" aria-label="5. Discussion, challenges and perspectives">5. Discussion, challenges and perspectives</a></li>
                <li>
                    <a href="#6-conclusions" aria-label="6. Conclusions">6. Conclusions</a><ul>
                        <ul>
                        
                <li>
                    <a href="#core-focus" aria-label="Core Focus">Core Focus</a></li>
                <li>
                    <a href="#progress-and-trends" aria-label="Progress and Trends">Progress and Trends</a></li>
                <li>
                    <a href="#challenges" aria-label="Challenges">Challenges</a></li>
                <li>
                    <a href="#future-outlook" aria-label="Future Outlook">Future Outlook</a>
                </li>
            </ul>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="initial-impression">Initial Impression<a hidden class="anchor" aria-hidden="true" href="#initial-impression">#</a></h1>
<aside>
üëâüèº
<p>The light field refres to the representation of a scene. Unlike the tranditional 2D image, it adds an extra dimention: the direction of light. This means each image captures not only the length and width, but also the direction of light.</p>
<ul>
<li>Traditional image: <strong>2D spatially</strong> (width, height), but each pixel carries a <strong>3-value vector</strong> (RGB, 3 channels for color).</li>
<li>Light filed(width, heidht, X-direction, Y-direction): <strong>4D images</strong>, but each pixel carries a <strong>3-value vector.</strong></li>
</ul>
<p>As a result, light field data is massive and challenging to process.</p>
<p>To address it, researchers have turned into machine learning to efficiently handle and enhance light field imaging.</p>
</aside>
<p>The paper mentions 4 main areas that AI can helps:</p>
<ol>
<li>Depth estimation</li>
<li>Reconstruction</li>
<li>Compression</li>
<li>Quelity evaluation</li>
</ol>
<h1 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h1>
<aside>
üëâüèº
<p><strong>Content:</strong></p>
<ol>
<li>Background</li>
<li>Motivation</li>
<li>Research focus</li>
<li>Paper content
<ol>
<li>the existing learning-based solutions</li>
<li>frameworks</li>
<li>evaluation methods</li>
<li>datasets</li>
<li>future research directions</li>
</ol>
</li>
</ol>
</aside>
<h3 id="background">Background<a hidden class="anchor" aria-hidden="true" href="#background">#</a></h3>
<ul>
<li>Traditional cameras capture only 2D images.</li>
<li>Light field imaging records the direction of light rays, enabling realistic and immersive 3D-like representations.</li>
</ul>
<h3 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h3>
<ul>
<li>Light field data provides richer visual information but comes with large data volume and high computational cost.</li>
<li>Machine learning and deep networks offer efficient and intelligent ways to process this data.</li>
</ul>
<h3 id="research-focus">Research Focus<a hidden class="anchor" aria-hidden="true" href="#research-focus">#</a></h3>
<p>Learning-based methods are applied to:</p>
<ul>
<li>Depth estimation</li>
<li>Reconstruction and super-resolution</li>
<li>Compression and quality enhancement</li>
</ul>
<h3 id="paper-goals">Paper Goals<a hidden class="anchor" aria-hidden="true" href="#paper-goals">#</a></h3>
<p>The paper surveys existing learning-based techniques, summarizes the most promising frameworks, reviews current datasets, and evaluation methods and outlook for future research directions.</p>
<hr>
<h1 id="1-introduction">1. Introduction<a hidden class="anchor" aria-hidden="true" href="#1-introduction">#</a></h1>
<aside>
üëâüèº
<p><strong>Content:</strong></p>
<ol>
<li>Concept and potential of light field imaging</li>
<li>Market growth and application fields</li>
<li>Technical challenges and processing tasks</li>
<li>Shift toward learning-based solutions</li>
<li>Purpose and structure of the paper</li>
</ol>
</aside>
<h3 id="concept-and-potential">Concept and Potential<a hidden class="anchor" aria-hidden="true" href="#concept-and-potential">#</a></h3>
<ul>
<li><strong>Light field imaging</strong> is a promising <strong>3D imaging technology</strong> that records light rays traveling through every point in space and in every direction.</li>
<li>Unlike 2D photography, it captures <strong>angular information</strong>, providing a sense of <strong>depth</strong>, <strong>realism</strong>, and <strong>immersion</strong>.</li>
<li>This enables <strong>photo-realistic rendering</strong> and supports <strong>6-DoF (Degrees of Freedom)</strong> experiences for next-generation immersive media, broadcasting, and gaming.</li>
</ul>
<h3 id="market-and-applications">Market and Applications<a hidden class="anchor" aria-hidden="true" href="#market-and-applications">#</a></h3>
<ul>
<li>The <strong>light field market</strong> is expanding rapidly, driven by <strong>glasses-free 3D displays</strong> and <strong>multi-view visualization systems</strong>.</li>
<li>It supports a range of applications such as <strong>virtual reality</strong>, <strong>augmented reality</strong>, <strong>3D reconstruction</strong>, and <strong>computational photography</strong>.</li>
</ul>
<h3 id="challenges-and-processing-tasks">Challenges and Processing Tasks<a hidden class="anchor" aria-hidden="true" href="#challenges-and-processing-tasks">#</a></h3>
<ul>
<li>High-dimensional light field data introduces issues like <strong>data redundancy</strong>, <strong>storage complexity</strong>, and <strong>inter-view correlation</strong>.</li>
<li>Essential processing tasks include:
<ul>
<li><strong>Spatial and angular super-resolution</strong> to enhance image quality</li>
<li><strong>Compression algorithms</strong> for efficient data storage and transmission</li>
<li><strong>Depth estimation</strong> for 3D scene reconstruction</li>
</ul>
</li>
<li>These tasks are <strong>more complex</strong> than traditional 2D image processing due to added angular dimensions.</li>
</ul>
<h3 id="shift-toward-learning-based-solutions">Shift Toward Learning-Based Solutions<a hidden class="anchor" aria-hidden="true" href="#shift-toward-learning-based-solutions">#</a></h3>
<ul>
<li>Traditional geometry-based methods struggle with large datasets and occlusion problems.</li>
<li><strong>Deep learning</strong> and <strong>data-driven frameworks</strong> now dominate, improving efficiency and performance in reconstruction, compression, and depth estimation.</li>
<li>Learning frameworks enable <strong>automation</strong> and <strong>scalability</strong>, addressing the computational challenges of high-dimensional data.</li>
</ul>
<h3 id="purpose-and-structure-of-the-paper">Purpose and Structure of the Paper<a hidden class="anchor" aria-hidden="true" href="#purpose-and-structure-of-the-paper">#</a></h3>
<ul>
<li>This review provides a <strong>comprehensive overview</strong> of <strong>learning-based solutions</strong> for light field imaging.</li>
<li>It discusses:
<ol>
<li><strong>Fundamentals of light field imaging</strong> and data acquisition</li>
<li><strong>Key processing tasks</strong> and related learning frameworks</li>
<li><strong>Benchmark datasets</strong> and <strong>evaluation methods</strong></li>
<li><strong>Current challenges</strong> and <strong>future research directions</strong></li>
</ol>
</li>
<li>The goal is to summarize progress, identify open issues, and provide a <strong>roadmap for future research</strong> in learning-based light field processing.</li>
</ul>
<hr>
<h1 id="2-light-field-imaging-background">2. Light field imaging background<a hidden class="anchor" aria-hidden="true" href="#2-light-field-imaging-background">#</a></h1>
<aside>
üëâüèº
<p><strong>Content:</strong></p>
<ol>
<li>Light field fundamentals</li>
<li>Light field <strong>acquisition</strong></li>
<li>Light field <strong>visualization</strong></li>
</ol>
</aside>
<h2 id="21-light-field-fundamentals">2.1 Light Field Fundamentals<a hidden class="anchor" aria-hidden="true" href="#21-light-field-fundamentals">#</a></h2>
<aside>
üëâüèº
<p><strong>Content:</strong></p>
<ol>
<li>Concept of the Plenoptic Function</li>
<li>Dimensional Reduction</li>
<li>Light Field Representation</li>
</ol>
</aside>
<h3 id="concept-of-the-plenoptic-function">Concept of the Plenoptic Function<a hidden class="anchor" aria-hidden="true" href="#concept-of-the-plenoptic-function">#</a></h3>
<ul>
<li>
<p>To reproduce realistic 3D scenes, cameras must capture light from <strong>many viewpoints</strong>.</p>
</li>
<li>
<p>A <strong>light field</strong> describes all light rays in 3D space ‚Äî their <strong>positions</strong>, <strong>directions</strong>, <strong>colors</strong>, and <strong>intensity</strong>.</p>
</li>
<li>
<p>The complete description is given by the <strong>plenoptic function</strong>, a 7-dimensional (7D) function:</p>
<p>$$
P(Œ∏, œÜ, Œª, œÑ, V_x, V_y, V_z)
$$</p>
<p>which includes direction, wavelength, time, and position of every ray in space.</p>
</li>
</ul>
<h3 id="dimensional-reduction">Dimensional Reduction<a hidden class="anchor" aria-hidden="true" href="#dimensional-reduction">#</a></h3>
<ul>
<li>Capturing the full 7D plenoptic function is practically impossible.</li>
<li>Under constant lighting and static scenes, <strong>wavelength (Œª)</strong> and <strong>time (œÑ)</strong> can be ignored.</li>
<li>This simplifies the representation to a <strong>5D function</strong>:</li>
</ul>
<p>$$
P(Œ∏, œÜ, V_x, V_y, V_z)
$$</p>
<ul>
<li>The 5D form forms the foundation for <strong>Neural Radiance Fields (NeRF)</strong>, while further simplification leads to a <strong>4D light field</strong> representation.</li>
</ul>
<h3 id="light-field-representation">Light Field Representation<a hidden class="anchor" aria-hidden="true" href="#light-field-representation">#</a></h3>
<ul>
<li>
<p><strong>Two-Plane Parameterization</strong></p>
<ul>
<li>The <strong>4D light field</strong> assumes light rays travel in straight lines.</li>
<li>Each ray is defined by its intersection with two parallel planes:
<ul>
<li><strong>(u, v)</strong> ‚Üí spatial coordinates on the image/focal plane (Œ©)</li>
<li><strong>(s, t)</strong> ‚Üí coordinates on the camera plane (Œ†)</li>
</ul>
</li>
<li>The resulting function:</li>
</ul>
<p>$$
P(u, v, s, t)
$$</p>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image.png"></p>
<ul>
<li>defines the light field in terms of <strong>spatial</strong> and <strong>angular</strong> information.</li>
<li>This <strong>two-plane parameterization</strong> makes light fields easier to store, process, and compress using 2D image arrays.</li>
<li>A 4D light field can be expressed as <strong>an array of 2D images</strong> indexed by (u, v) for spatial coordinates and (s, t) for different viewpoints.</li>
<li>This enables the use of <strong>standard 2D codecs</strong> for compression and <strong>native 2D algorithms</strong> for processing.</li>
</ul>
</li>
<li>
<p><strong>Epipolar Plane Image (EPI)</strong></p>
<ul>
<li><strong>Epipolar Plane Image (EPI)</strong>, a 2D slice of the light field showing spatial‚Äìangular relationships.
<ul>
<li>Looks like a single <strong>2D picture</strong> made by stacking the same pixel row from all those camera views.</li>
<li>Coordinates ‚Üí (u, s) or (v, t) (only one spatial + one view direction)</li>
</ul>
</li>
<li>In the EPI, each object in the scene becomes a <strong>slanted line</strong>:
<ul>
<li>If an object is <strong>close</strong>, its line is <strong>steeper</strong> (because it moves more between views).</li>
<li>If it‚Äôs <strong>far</strong>, its line is <strong>flatter</strong> (moves less).</li>
</ul>
</li>
<li>Lines in the EPI correspond to scene depth ‚Äî analyzing their slopes allows <strong>depth estimation</strong> and <strong>3D reconstruction</strong>.</li>
</ul>
<p><img alt="<strong>Epipolar Plane Image (EPI)</strong>" loading="lazy" src="/notes/learningbased_light_field_imaging/image_1.png"></p>
<p><strong>Epipolar Plane Image (EPI)</strong></p>
</li>
</ul>
<h2 id="22-light-field-acquisition">2.2 Light Field Acquisition<a hidden class="anchor" aria-hidden="true" href="#22-light-field-acquisition">#</a></h2>
<aside>
üëâüèº
<p><strong>Acquisition methods:</strong></p>
<ol>
<li>Single-Camera Systems</li>
<li>Multi-Camera Arrays</li>
<li>Other Methods
<ol>
<li>Computer-generated light fields</li>
<li>Handheld or SLAM-based systems</li>
<li>LiDAR-assisted capture</li>
</ol>
</li>
</ol>
</aside>
<h3 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h3>
<p>Light fields can be acquired by single plenoptic cameras, camera arrays, or synthetic and LiDAR-based systems, each balancing data density, resolution, and complexity.</p>
<h3 id="single-camera-systems">Single-Camera Systems<a hidden class="anchor" aria-hidden="true" href="#single-camera-systems">#</a></h3>
<ul>
<li><strong>Plenoptic (lenslet-based) cameras</strong> use microlens arrays to capture dense angular information in one shot.</li>
<li>Examples: <strong>Raytrix</strong> and <strong>Lytro</strong> cameras.
<ul>
<li>Lytro ‚Üí The <strong>Lytro camera</strong> captures a 4-D light field by recording many tiny viewpoint images through its microlens array ‚Äî those images form the <strong>SAI stack.</strong></li>
</ul>
</li>
</ul>
<h3 id="multi-camera-arrays">Multi-Camera Arrays<a hidden class="anchor" aria-hidden="true" href="#multi-camera-arrays">#</a></h3>
<ul>
<li><strong>Arrays of monocular cameras</strong> (planar or spherical) capture scenes from different viewpoints.</li>
<li>The camera layout defines the angular sampling and overall field of view.</li>
</ul>
<h3 id="other-methods">Other Methods<a hidden class="anchor" aria-hidden="true" href="#other-methods">#</a></h3>
<ul>
<li><strong>Computer-generated light fields</strong> provide accurate depth maps for benchmarking.</li>
<li><strong>Handheld or SLAM-based systems</strong> reconstruct light fields from multiple frames.</li>
<li><strong>LiDAR-assisted capture</strong> combines sensors and cameras for precise, automated 3D data.</li>
</ul>
<h2 id="23-light-field-visualization">2.3 Light field visualization<a hidden class="anchor" aria-hidden="true" href="#23-light-field-visualization">#</a></h2>
<aside>
üëâüèº
<p><strong>Content:</strong></p>
<ol>
<li>Goal and use cases</li>
<li>Visualization approaches</li>
<li>Display principles and challenges</li>
<li>Perceptual limitations</li>
</ol>
</aside>
<h3 id="goal-and-use-cases">Goal and Use Cases<a hidden class="anchor" aria-hidden="true" href="#goal-and-use-cases">#</a></h3>
<ul>
<li>The main goal is to <strong>provide a true 3D visual experience</strong>, essential for immersive and interactive applications.</li>
<li>Visualization may be <strong>passive</strong> (no interaction) or <strong>active</strong> (viewer can move or rotate objects).</li>
<li>Used in areas like <strong>medical imaging</strong>, <strong>VR/AR</strong>, and <strong>3D display systems</strong>.</li>
</ul>
<h3 id="visualization-approaches">Visualization Approaches<a hidden class="anchor" aria-hidden="true" href="#visualization-approaches">#</a></h3>
<ul>
<li>Based on the <strong>4D light-field representation</strong> combining spatial and angular data.</li>
<li><strong>Passive</strong> use cases render fixed views; <strong>active</strong> use cases synthesize new views via <strong>view interpolation</strong>. (<strong>View interpolation</strong> means <strong>creating new viewpoints</strong> images that were <strong>never actually captured by a real camera</strong>, but are <strong>mathematically generated</strong> from nearby real ones.</li>
<li>Rendering quality strongly influences perceptual realism.</li>
</ul>
<h3 id="display-principles-and-challenges">Display Principles and Challenges<a hidden class="anchor" aria-hidden="true" href="#display-principles-and-challenges">#</a></h3>
<ul>
<li>Two key properties:
<ul>
<li><strong>Angular resolution</strong> (baseline between views)
<ul>
<li>the <strong>baseline</strong> = the <strong>distance between two camera viewpoints</strong>.
<ul>
<li><strong>Small baseline</strong> ‚Üí cameras are <strong>close together</strong></li>
<li><strong>Wide baseline</strong> ‚Üí cameras are <strong>far apart</strong></li>
</ul>
</li>
</ul>
</li>
<li><strong>Spatial resolution</strong> (image detail)</li>
</ul>
</li>
<li>3D displays must reproduce directional light rays accurately to recreate depth and realism.</li>
<li>Capturing dense samples and rendering multiple view angles remain <strong>computationally demanding</strong>.</li>
</ul>
<h3 id="perceptual-limitations">Perceptual Limitations<a hidden class="anchor" aria-hidden="true" href="#perceptual-limitations">#</a></h3>
<ul>
<li>Standard 2D displays lack realistic cues like <strong>vergence</strong> and <strong>accommodation</strong>, limiting immersion.</li>
<li><strong>Light-field displays</strong> address this by replicating real light rays to the viewer‚Äôs eyes but face:
<ul>
<li>Finite ray sampling</li>
<li><strong>Vergence‚Äìaccommodation conflict</strong></li>
<li>Restricted field of view (FoV)</li>
<li><strong>Horizontal- or vertical-parallax-only designs</strong> causing viewing discomfort.</li>
</ul>
</li>
</ul>
<hr>
<h1 id="3-learningbased-light-field-processing">3. Learning‚Äëbased light field processing<a hidden class="anchor" aria-hidden="true" href="#3-learningbased-light-field-processing">#</a></h1>
<aside>
üëâüèº
<p>Content:</p>
<ol>
<li>Depth estimation
<ol>
<li>Autoencoders</li>
<li>Stereo matching and refinement</li>
<li>End‚Äëto‚Äëend feature extraction and disparity regression</li>
</ol>
</li>
<li>Light field reconstruction
<ol>
<li>Spatial super‚Äëresolution
<ol>
<li>Single‚Äëview super‚Äëresolution and refinement</li>
<li>End‚Äëto‚Äëend residual learning</li>
</ol>
</li>
<li>Angular super‚Äëresolution
<ol>
<li>EPI super‚Äëresolution</li>
<li>Depth estimation and warping</li>
<li>Multi‚Äëplane image generation</li>
</ol>
</li>
<li>Spatio‚Äëangular reconstruction</li>
<li>Neural scene representation</li>
</ol>
</li>
<li>Compression
<ol>
<li>Learning‚Äëbased view synthesis on the decoder side</li>
<li>Learning‚Äëbased view synthesis on the encoder and decoder sides</li>
<li>End‚Äëto‚Äëend light field compression architecture</li>
</ol>
</li>
<li>Other light field imaging applications</li>
</ol>
</aside>
<p>This section summarizes the most prominent <strong>light field processing tasks</strong> studied in the literature and highlights the <strong>learning-based imaging techniques</strong> deployed for each processing task.</p>
<p><strong>Terms</strong> and explanation:</p>
<table>
  <thead>
      <tr>
          <th>Term</th>
          <th>Simple meaning</th>
          <th>Example / Analogy</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>EPI volume</strong></td>
          <td>Stack of many light-field slices showing how points move across views</td>
          <td>Like stacking many thin image strips to form a 3D block</td>
      </tr>
      <tr>
          <td><strong>Stereo view</strong></td>
          <td>Two or more photos of the same scene from different angles</td>
          <td>Like your left and right eye views</td>
      </tr>
      <tr>
          <td><strong>SAI stack</strong></td>
          <td>Group of small 2D images from a light field camera</td>
          <td>Like a grid of mini photos from slightly different directions</td>
      </tr>
      <tr>
          <td><strong>Depth map</strong></td>
          <td>Image showing distance of each pixel (white = near, black = far)</td>
          <td>Like a 3D scanner‚Äôs output</td>
      </tr>
      <tr>
          <td><strong>Disparity volume</strong></td>
          <td>3D data showing pixel shifts between views</td>
          <td>Large shift ‚Üí close object, small shift ‚Üí far object</td>
      </tr>
  </tbody>
</table>
<p>Details about the difference between <strong>Light Field representation</strong>:</p>
<table>
  <thead>
      <tr>
          <th>Term</th>
          <th>Relation to Two-Plane Parameterization</th>
          <th>Simple meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>EPI</strong></td>
          <td>A <strong>slice</strong> through the Two-Plane representation.</td>
          <td>Shows pixel movement (as slanted lines) between multiple views ‚Äî used for depth estimation.</td>
      </tr>
      <tr>
          <td><strong>Stereo view</strong></td>
          <td>it‚Äôs a <strong>simpler / smaller subset</strong> of the Two-Plane model (only 2 views).</td>
          <td>Like taking only the left and right images from the light field.</td>
      </tr>
      <tr>
          <td><strong>SAI stack</strong></td>
          <td>it‚Äôs directly sampled from the Two-Plane model (a grid of (s, t) views, each with its own (u, v) image).</td>
          <td>Many small 2D images from slightly different viewpoints ‚Äî a practical way to store the light field.</td>
      </tr>
  </tbody>
</table>
<p>Relationship between <strong>Disparity volume</strong> and <strong>Depth map:</strong></p>
<table>
  <thead>
      <tr>
          <th>Concept</th>
          <th>Type</th>
          <th>What it represents</th>
          <th>Used for</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Disparity volume</strong></td>
          <td>3D data (x, y, disparity)</td>
          <td>All possible matching shifts between views</td>
          <td>Intermediate step (to find the best match)</td>
      </tr>
      <tr>
          <td><strong>Depth map</strong></td>
          <td>2D image (x, y), 3D data (x, y, depth)</td>
          <td>Actual distance of each pixel</td>
          <td>Final result</td>
      </tr>
  </tbody>
</table>
<p>Disparity and depth are <strong>inversely related</strong>:</p>
<p>$$
\text{Depth} = \frac{f \times B}{\text{Disparity}}
$$</p>
<p>where:</p>
<ul>
<li>(f) = focal length of the camera,</li>
<li>(B) = distance between two camera views (baseline).</li>
</ul>
<p>So:</p>
<blockquote>
<p>Large disparity (big shift) ‚Üí close object.
Small disparity (tiny shift) ‚Üí far object.</p>
</blockquote>
<p><img alt="Depth map" loading="lazy" src="/notes/learningbased_light_field_imaging/image_2.png"></p>
<p>Depth map</p>
<h2 id="31-depth-estimation">3.1 Depth estimation<a hidden class="anchor" aria-hidden="true" href="#31-depth-estimation">#</a></h2>
<aside>
üëâüèº
<p>Three main approaches:</p>
<ol>
<li>Autoencoders</li>
<li>Stereo matching and refinement</li>
<li>End-to-end feature extraction and disparity regression</li>
</ol>
</aside>
<p><img alt="Depth estimation model architecture" loading="lazy" src="/notes/learningbased_light_field_imaging/image_3.png"></p>
<p>Depth estimation model architecture</p>
<h3 id="overview-1">Overview<a hidden class="anchor" aria-hidden="true" href="#overview-1">#</a></h3>
<ul>
<li><strong>Goal:</strong> Estimate the <strong>distance of each pixel</strong> from the camera to recover the scene‚Äôs 3D structure.</li>
<li>Light field imaging enables capturing a scene from multiple viewpoints so depth information is implicitly encoded in the light field representation and can be acquired by computing the inter-view pixel disparity information. <strong>disparity volume ‚Üí depth map</strong></li>
<li>Main challenges: <strong>occlusions</strong>, <strong>non-Lambertian surfaces</strong>, and <strong>texture-less regions</strong>, which make accurate estimation difficult.
<ul>
<li>A <strong>Lambertian surface</strong> is an <em>ideal matte surface</em> ‚Äî it reflects light <strong>equally in all directions</strong>. That means no matter where you look from, the brightness of that point <strong>stays the same</strong>.</li>
</ul>
</li>
<li>Recent progress focuses on <strong>learning-based approaches</strong>, achieving higher accuracy than traditional geometry-based methods.</li>
</ul>
<h3 id="autoencoders">Autoencoders<a hidden class="anchor" aria-hidden="true" href="#autoencoders">#</a></h3>
<ul>
<li>
<p>Take an <strong>EPI volume</strong> ‚Üí compress it (encoder) ‚Üí learn the hidden features ‚Üí expand it (decoder).</p>
</li>
<li>
<p>Classical <strong>method:</strong></p>
<ul>
<li><strong>Heber &amp; Pock</strong>:
<ul>
<li><strong>five-block CNN</strong> estimating line orientation in EPIs;
<ul>
<li><strong>Orientation</strong> refers to the <strong>direction each camera is facing.</strong></li>
</ul>
</li>
<li>later extended to a <strong>U-shaped encoder‚Äìdecoder</strong>,</li>
<li>Further to  <strong>U-shaped encoder‚Äìdecoder</strong> with skip connections and 3D filters.</li>
</ul>
</li>
</ul>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/ef74cbaf-cb55-4a3d-99e0-83a8608b8442.png"></p>
<table>
  <thead>
      <tr>
          <th>Step</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Input</strong></td>
          <td>Horizontal and vertical <strong>EPI volumes</strong> (from the light field). Each EPI shows slanted lines ‚Äî the slope encodes depth.</td>
      </tr>
      <tr>
          <td><strong>Model</strong></td>
          <td>A <strong>5-block CNN</strong> that scans small ‚Äúwindows‚Äù (patches) of the EPI. Each CNN layer extracts features and estimates the <strong>orientation (slope)</strong> of the EPI lines. Later versions evolved into a <strong>U-shaped encoder‚Äìdecoder</strong> (U-Net) for better reconstruction.</td>
      </tr>
      <tr>
          <td><strong>Output</strong></td>
          <td>A <strong>depth map</strong>, where every pixel‚Äôs value = estimated distance from the camera.</td>
      </tr>
  </tbody>
</table>
<ul>
<li>
<p><strong>Alperovich et al.</strong>:</p>
<ul>
<li>an <strong>autoencoder</strong> that <strong>encodes</strong> horizontal and vertical EPI stacks simultaneously using six stages of <strong>residual blocks</strong> to improve robustness.</li>
<li>Then, the compressed representation is expanded using three <strong>decoder</strong> pathways to
address the disparity, diffusion, and specularity estimation problems.</li>
</ul>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image_4.png"></p>
<table>
  <thead>
      <tr>
          <th>Step</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Input</strong></td>
          <td>Combined <strong>horizontal + vertical EPI stacks</strong> from the light field.</td>
      </tr>
      <tr>
          <td><strong>Model</strong></td>
          <td>An <strong>autoencoder</strong> with: 6 <strong>residual blocks</strong> (for stronger feature extraction) in the encoder, and 3 <strong>decoder branches</strong> (to handle disparity, diffusion, specularity). It compresses the light field into a latent code, then reconstructs richer outputs.</td>
      </tr>
      <tr>
          <td><strong>Output</strong></td>
          <td>A <strong>disparity volume</strong> ‚Äî a 3D array where each pixel position stores multiple disparity hypotheses (possible shifts). From this volume, a final <strong>depth map</strong> can be derived later.</td>
      </tr>
  </tbody>
</table>
</li>
</ul>
</li>
<li>
<p>Analysis:</p>
<ul>
<li><strong>Pros:</strong> captures compact depth features, handles EPI geometry directly.</li>
<li><strong>Cons:</strong> computationally heavy; limited to 2D EPI slices, less effective in occluded regions.</li>
</ul>
</li>
</ul>
<h3 id="stereo-matching-and-refinement">Stereo Matching and Refinement<a hidden class="anchor" aria-hidden="true" href="#stereo-matching-and-refinement">#</a></h3>
<ul>
<li>
<p>Computes <strong>disparity between SAIs</strong> using neural stereo-matching networks.</p>
</li>
<li>
<p>Typical pipeline:</p>
<ol>
<li>Coarse disparity estimation via networks like <strong>FlowNet 2.0</strong> or encoder‚Äìdecoder CNNs.</li>
<li><strong>Refinement</strong> using residual or occlusion-aware learning to correct depth errors.</li>
</ol>
</li>
<li>
<p><strong>Examples:</strong></p>
<ul>
<li>
<p><strong>Rogge et al.</strong> (belief propagation + residual refinement);</p>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/723a41c2-b8dc-40fe-b731-51f898768fc5.png"></p>
</li>
<li>
<p><strong>Guo et al.</strong> (encoder‚Äìdecoder concatenation of SAIs).</p>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/7c3c1972-3fe9-448c-9ba7-8d1f317a76d9.png"></p>
</li>
</ul>
</li>
<li>
<p><strong>Analysis:</strong></p>
<ul>
<li><strong>Pros:</strong> exploits full 4D light-field correlations, good for complex geometry.</li>
<li><strong>Cons:</strong> high computation cost; sensitive to reflections and non-Lambertian surfaces.</li>
</ul>
</li>
</ul>
<h3 id="end-to-end-feature-extraction-and-disparity-regression">End-to-End Feature Extraction and Disparity Regression<a hidden class="anchor" aria-hidden="true" href="#end-to-end-feature-extraction-and-disparity-regression">#</a></h3>
<ul>
<li>
<p>Fully end-to-end CNNs learn features and regress depth directly.</p>
</li>
<li>
<p><strong>Methods:</strong></p>
<ul>
<li><strong>Epinet:</strong> Horizontal, vertical, and diagonal SAI stacks ‚Üí Multi-stream CNN feature extraction ‚Üí Regression network ‚Üí <strong>Depth map</strong></li>
</ul>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/24d86bc0-b749-4efe-a843-7697bc1ee29d.png"></p>
<ul>
<li><strong>Leistner et al.:</strong> Vertical &amp; horizontal SAI stacks ‚Üí Siamese U-Net ‚Üí Autoencoder regression module ‚Üí Classification + regression fusion ‚Üí <strong>Depth map</strong></li>
</ul>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/f4e2a250-9bef-4ac9-97a2-9a8e092a88b7.png"></p>
<ul>
<li><strong>Two-stream CNN:</strong> Horizontal &amp; vertical EPIs ‚Üí Multi-scale feature extraction (four convolutional stages) ‚Üí Feature concatenation ‚Üí Multi-label regression ‚Üí <strong>Depth map</strong></li>
<li><strong>Zhu et al.:</strong> Focal stacks + center view + EPIs ‚Üí Hybrid feature extraction ‚Üí Fully connected + softmax layers ‚Üí Pixel-wise disparity classification ‚Üí <strong>Depth map</strong></li>
<li><strong>Tsai et al.:</strong> Multi-view SAIs ‚Üí Residual blocks + spatial pyramid pooling ‚Üí Cost volume construction + attention module ‚Üí Disparity regression ‚Üí <strong>Depth map</strong></li>
</ul>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/b25bef84-1e89-45c5-99e6-be4fb1cd2252.png"></p>
<ul>
<li><strong>Multi-scale Cost-Volume Method:</strong> Shifted SAI feature maps (multi-disparity levels) ‚Üí 4D cost volume (low memory footprint) ‚Üí Multi-scale feature extraction ‚Üí Regression ‚Üí <strong>Depth map</strong></li>
</ul>
</li>
<li>
<p><strong>Analysis:</strong></p>
<ul>
<li><strong>Pros:</strong> highest accuracy, unified optimization of feature + disparity learning.</li>
<li><strong>Cons:</strong> large data/training demand; reduced performance on wide-baseline scenes.</li>
</ul>
</li>
</ul>
<aside>
üëâüèº
<p><strong>Summary</strong>:</p>
<p>Depth estimation for the wide baseline scenario, with an acceptable trade-off between accuracy and computation, is still an open research problem.</p>
</aside>
<h2 id="32-light-field-reconstruction">3.2 Light field reconstruction<a hidden class="anchor" aria-hidden="true" href="#32-light-field-reconstruction">#</a></h2>
<aside>
üëâüèº
<p><strong>Content:</strong></p>
<ol>
<li>Spatial super‚Äëresolution
<ol>
<li>Single‚Äëview super‚Äëresolution and refinement</li>
<li>End‚Äëto‚Äëend residual learning</li>
</ol>
</li>
<li>Angular super‚Äëresolution
<ol>
<li>EPI super‚Äëresolution</li>
<li>Depth estimation and warping</li>
<li>Multi‚Äëplane image generation</li>
</ol>
</li>
<li>Spatio‚Äëangular reconstruction</li>
<li>Neural scene representation</li>
</ol>
</aside>
<p>To enable <strong>higher spatial</strong> and <strong>angular</strong> resolutions, the development of light field reconstruction/ super-resolution (SR) methods has gained significant attention.</p>
<ul>
<li><strong>Spatial SR</strong> = make each image (each view) sharper ‚Äî more pixels, more detail.</li>
<li><strong>Angular SR</strong> = make more <strong>viewpoints</strong> ‚Äî fill in <strong>missing views</strong> between existing ones.</li>
</ul>
<aside>
üí°
<p><strong>Spatial super-resolution (SR)</strong> focuses on improving the <strong>static spatial resolution</strong> ‚Äî that is, the sharpness, clarity, and detail of each individual sub-aperture image (still view).</p>
<p><strong>Angular consistency</strong> ensures <strong>smooth and coherent transitions</strong> between different viewpoints, maintaining stable <strong>motion perception</strong> and correct 3D geometry when the view changes or the scene is refocused.</p>
</aside>
<p>Light-field SR must improve both:</p>
<ol>
<li><strong>Spatial</strong> resolution ‚Üí each view looks higher spatial resolution.</li>
<li><strong>Angular</strong> consistency ‚Üí all views agree about geometry and depth.</li>
</ol>
<table>
  <thead>
      <tr>
          <th>System</th>
          <th>Spatial resolution (image sharpness)</th>
          <th>Angular resolution (number of viewpoints)</th>
          <th>Baseline</th>
          <th>Notes</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Plenoptic camera</strong></td>
          <td>Low</td>
          <td>High (dense)</td>
          <td>Narrow</td>
          <td>Compact but blurry</td>
      </tr>
      <tr>
          <td><strong>Camera rig</strong></td>
          <td>High</td>
          <td>Low (sparse)</td>
          <td>Wide</td>
          <td>Sharp but heavy and complex</td>
      </tr>
  </tbody>
</table>
<h3 id="spatial-superresolution">Spatial super‚Äëresolution<a hidden class="anchor" aria-hidden="true" href="#spatial-superresolution">#</a></h3>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image_5.png"></p>
<p>Examples of architectures for the spatial, angular, and spatio-angular super-resolution (SR) frameworks.</p>
<ul>
<li><strong>a</strong> ‚Üí Single-view SR using single image super-resolution (SISR) network and inter-view enhancement,</li>
<li><strong>b</strong> ‚Üí end-to-end residual learning,</li>
<li><strong>c</strong> ‚Üí warping and residual learning for refinement,</li>
<li><strong>d</strong> ‚Üí multi-plane image generation,</li>
<li>e ‚Üí residual learning using 4D CNNs and refinement, ****</li>
<li><strong>f</strong> ‚Üí GAN-based method</li>
</ul>
<hr>
<p><strong>Single‚Äëview super‚Äëresolution and refinement</strong></p>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image_6.png"></p>
<table>
  <thead>
      <tr>
          <th>Step</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Input (SAI stack LR)</strong></td>
          <td><strong>SAI stack LR:</strong> all the sub-aperture images captured by the light-field camera. <strong>low-resolution</strong> ‚Äî each view is blurry and lacks details (because each microlens only captures a small number of pixels). For example, <strong>9√ó9¬†SAIs¬†of¬†60√ó60¬†pixels.</strong></td>
      </tr>
      <tr>
          <td><strong>SISR</strong></td>
          <td><strong>Single-Image Super-Resolution</strong>: This block applies a <strong>2D super-resolution network (e.g., VDSR)</strong> to each SAI <strong>individually</strong> to ****improve the <strong>spatial resolution.</strong></td>
      </tr>
      <tr>
          <td><strong>HR(init)LF</strong></td>
          <td><strong>High-Resolution (initial) Light Field</strong>: Each image looks sharper, but the views may no longer be perfectly aligned (angular inconsistency).</td>
      </tr>
      <tr>
          <td><strong>EPI</strong></td>
          <td><strong>EPIs</strong> (Epipolar Plane Images) are extracted. EPIs capture the <strong>geometric relationships</strong> between views ‚Äî how pixels shift across viewpoints (slanted lines).</td>
      </tr>
      <tr>
          <td><strong>Refinement module</strong></td>
          <td>This is a <strong>learning network</strong> (often a CNN) that analyzes the EPIs to find and correct <strong>misalignments</strong> between neighboring SAIs. Making sure all views agree in depth and structure.</td>
      </tr>
      <tr>
          <td><strong>Residual addition</strong></td>
          <td>Combine sharpness from the first stage and consistency from the second stage.</td>
      </tr>
      <tr>
          <td><strong>Output (HR LF)</strong></td>
          <td><strong>High-Resolution Light Field:</strong> The final output light field has: <strong>High spatial resolution</strong> (sharp individual views), and <strong>High angular consistency</strong> (smooth geometry across views). For example, <strong>9√ó9¬†SAIs¬†of¬†240√ó240¬†pixels.</strong></td>
      </tr>
  </tbody>
</table>
<p><strong>End‚Äëto‚Äëend residual learning</strong></p>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image_7.png"></p>
<table>
  <thead>
      <tr>
          <th>Step</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Input (SAI stack)</strong></td>
          <td><strong>SAI stacks (Horizontal, Vertical, Diagonal, Center View), Center view</strong> ‚Üí the middle image, used as a geometric reference</td>
      </tr>
      <tr>
          <td><strong>Feature Extraction</strong></td>
          <td>Each SAI stack is passed through a <strong>feature extraction CNN.</strong> Extract features from each SAI direction, like ‚Äúwhat the object looks like‚Äù and ‚Äúhow it moves between views.‚Äù</td>
      </tr>
      <tr>
          <td><strong>Feature Integration &amp; Processing</strong></td>
          <td>The feature maps from all directions (horizontal, vertical, diagonal, etc.) are <strong>merged</strong> or <strong>fused</strong> here.</td>
      </tr>
      <tr>
          <td><strong>Upsampling Network</strong></td>
          <td>Increases the <strong>spatial resolution</strong> (i.e., number of pixels).</td>
      </tr>
      <tr>
          <td><strong>Output (HR LF)</strong></td>
          <td>The final output is a <strong>high-resolution light field</strong> ‚Äî a grid of SAIs that are: <strong>Spatially sharper</strong> and <strong>Angularly consistent.</strong></td>
      </tr>
  </tbody>
</table>
<p><strong>Upsampling</strong> means making an image <strong>larger</strong> ‚Äî that is, <strong>increasing its resolution</strong> by creating <strong>more pixels</strong>.</p>
<hr>
<h3 id="angular-superresolution">Angular super‚Äëresolution<a hidden class="anchor" aria-hidden="true" href="#angular-superresolution">#</a></h3>
<p><strong>Angular Super-Resolution (SR)</strong> means <strong>synthesizing new in-between views</strong> to make the light field smoother and more complete. (<strong>view synthesis</strong>)</p>
<p><strong>EPI super‚Äëresolution</strong></p>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image_8.png"></p>
<p>All of these models have the same <strong>basic structure</strong>:</p>
<ol>
<li>Start from a <strong>blurry / low-angular-resolution</strong> light field (few viewpoints).</li>
<li>Use <strong>deep networks</strong> to predict <strong>high-frequency details</strong> (the fine geometry and textures missing in the low version).</li>
<li>Reconstruct the <strong>high-angular-resolution (HR) light field</strong>, which includes the new views.</li>
</ol>
<table>
  <thead>
      <tr>
          <th>Step</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Input (SAI stack LR)</strong></td>
          <td><strong>SAI stack LR:</strong> all the sub-aperture images captured by the light-field camera. <strong>low-resolution</strong> ‚Äî each view is blurry and lacks details (because each microlens only captures a small number of pixels). For example, <strong>9√ó9¬†SAIs¬†of¬†60√ó60¬†pixels.</strong></td>
      </tr>
      <tr>
          <td><strong>SISR</strong></td>
          <td><strong>Single-Image Super-Resolution</strong>: This block applies a <strong>2D super-resolution network (e.g., VDSR)</strong> to each SAI <strong>individually</strong> to ****improve the <strong>spatial resolution.</strong></td>
      </tr>
      <tr>
          <td><strong>HR(init)LF</strong></td>
          <td><strong>High-Resolution (initial) Light Field</strong>: Each image looks sharper, but the views may no longer be perfectly aligned (angular inconsistency).</td>
      </tr>
      <tr>
          <td><strong>EPI</strong></td>
          <td><strong>EPIs</strong> (Epipolar Plane Images) are extracted. EPIs capture the <strong>geometric relationships</strong> between views ‚Äî how pixels shift across viewpoints (slanted lines).</td>
      </tr>
      <tr>
          <td><strong>Refinement module</strong></td>
          <td>This is a <strong>learning network</strong> (often a CNN) that analyzes the EPIs to find and correct <strong>misalignments</strong> between neighboring SAIs. Making sure all views agree in depth and structure.</td>
      </tr>
      <tr>
          <td><strong>Residual addition</strong></td>
          <td>Combine sharpness from the first stage and consistency from the second stage.</td>
      </tr>
      <tr>
          <td><strong>Output (HR LF)</strong></td>
          <td><strong>High-Resolution Light Field:</strong> The final output light field has: <strong>High spatial resolution</strong> (sharp individual views), and <strong>High angular consistency</strong> (smooth geometry across views). For example, <strong>9√ó9¬†SAIs¬†of¬†240√ó240¬†pixels.</strong></td>
      </tr>
  </tbody>
</table>
<p><strong>Depth estimation and warping</strong></p>
<ul>
<li><strong>Disparity</strong> is the <strong>shift</strong> of the same object‚Äôs position between two different views.</li>
<li>Warping = using disparity to reposition pixels.</li>
</ul>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image_9.png"></p>
<table>
  <thead>
      <tr>
          <th>Step</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Input (SAI stack)</strong></td>
          <td>A set of <strong>low-resolution sub-aperture images (SAIs)</strong></td>
      </tr>
      <tr>
          <td><strong>Depth Estimator</strong></td>
          <td>Predicts a <strong>depth map</strong> (distance information) for each SAI or for the entire light field. It learns how far each pixel is from the camera by analyzing geometric cues across views. (wraps_by_an_amount_that_depends_on_its_<strong>depth</strong>)</td>
      </tr>
      <tr>
          <td><strong>Warping Module</strong></td>
          <td>Uses the estimated <strong>depth maps</strong> to <strong>warp</strong> (geometrically align) all SAIs toward a common viewpoint ‚Äî usually the <strong>center view</strong>. ‚ÄúWarping‚Äù means shifting pixels according to their depth so that corresponding points from different views line up.</td>
      </tr>
      <tr>
          <td><strong>Refinement Module</strong></td>
          <td>Fine-tunes the warped images using a CNN. Corrects small errors from imperfect depth estimation or warping.</td>
      </tr>
      <tr>
          <td><strong>Output (HR LF)</strong></td>
          <td>The final reconstructed light field: All SAIs are now <strong>high-resolution</strong> (sharp textures). The views are <strong>geometrically aligned</strong> (consistent depth perception).</td>
      </tr>
  </tbody>
</table>
<p>Some basics:</p>
<table>
  <thead>
      <tr>
          <th>Concept</th>
          <th>Meaning</th>
          <th>Why it matters</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Depth map</strong></td>
          <td>Tells how far each pixel is</td>
          <td>Needed to compute correct pixel shift</td>
      </tr>
      <tr>
          <td><strong>Disparity</strong></td>
          <td>The actual shift caused by viewpoint change</td>
          <td>Derived from depth</td>
      </tr>
      <tr>
          <td><strong>Warping</strong></td>
          <td>Moves pixels according to disparity</td>
          <td>Aligns all views</td>
      </tr>
      <tr>
          <td><strong>Without depth</strong></td>
          <td>All pixels shift equally ‚Üí wrong alignment</td>
          <td>Causes blur and ghosting</td>
      </tr>
  </tbody>
</table>
<p><strong>Multi‚Äëplane image generation (MPI generation)</strong></p>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image_10.png"></p>
<table>
  <thead>
      <tr>
          <th>Step</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Input (SAI stack)</strong></td>
          <td>A set of <strong>low-resolution sub-aperture images (SAIs)</strong></td>
      </tr>
      <tr>
          <td><strong>Warping Module</strong></td>
          <td>The <strong>Warping Module</strong> aligns all SAIs to a <strong>reference view (usually the center view)</strong> using a range of <strong>hypothetical depth planes</strong>.</td>
      </tr>
      <tr>
          <td><strong>Plane-sweep volume</strong></td>
          <td>After warping, stacks all these reprojected images together ‚Äî forming a <strong>plane-sweep volume</strong>. Create a 3D data structure that encodes <strong>how well each depth hypothesis aligns</strong> across views. No explicit depth estimator ‚Äî depth is <strong>implicitly encoded</strong> in the plane-sweep volume.</td>
      </tr>
      <tr>
          <td><strong>3D CNN</strong></td>
          <td>The <strong>3D convolutional neural network</strong> processes the <strong>plane-sweep volume</strong> to analyze spatial and depth correlations.</td>
      </tr>
      <tr>
          <td><strong>Multi-plane image</strong></td>
          <td>A <strong>multi-plane image (MPI)</strong> ‚Äî a set of 2D images, each representing a scene layer at a specific depth, with color + transparency (Œ±) values.</td>
      </tr>
      <tr>
          <td><strong>Blending Module</strong></td>
          <td>Combines (blends) all the <strong>multi-plane images (depth layers)</strong> into a single coherent high-resolution light field.</td>
      </tr>
      <tr>
          <td><strong>Output (HR LF)</strong></td>
          <td>The final light field: All SAIs are now <strong>high-resolution</strong> and <strong>geometrically aligned</strong>.</td>
      </tr>
  </tbody>
</table>
<h3 id="spatioangular-reconstruction">Spatio‚Äëangular reconstruction<a hidden class="anchor" aria-hidden="true" href="#spatioangular-reconstruction">#</a></h3>
<p><strong>Residual learning using 4D CNNs and refinement</strong></p>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image_11.png"></p>
<table>
  <thead>
      <tr>
          <th>Step</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Input (SAI stack)</strong></td>
          <td>A set of <strong>low-resolution sub-aperture images (SAIs)</strong></td>
      </tr>
      <tr>
          <td><strong>Warping Module</strong></td>
          <td>The <strong>Warping Module</strong> aligns all SAIs to a <strong>reference view (usually the center view)</strong> using a range of <strong>hypothetical depth planes</strong>.</td>
      </tr>
      <tr>
          <td><strong>4D CNN</strong></td>
          <td>A convolutional network that operates directly on the <strong>4D light-field volume</strong>. It jointly learns: <strong>Spatial features</strong> (edges, textures inside each SAI) and <strong>Angular features</strong> (parallax).   The 4D CNN predicts the <strong>missing high-frequency details</strong> for all views simultaneously.</td>
      </tr>
      <tr>
          <td><strong>Residual Connection</strong></td>
          <td>This ‚Äúresidual learning‚Äù means the 4D CNN only learns the <em><strong>difference</strong></em> (the missing fine details) rather than reconstructing the entire image from scratch.</td>
      </tr>
      <tr>
          <td><strong>Refinement Module</strong></td>
          <td>Polish the HR LF and ensure <strong>angular consistency.</strong></td>
      </tr>
      <tr>
          <td><strong>Output (HR LF)</strong></td>
          <td><strong>Output: HR LF</strong>The final output = <strong>high-resolution light field</strong></td>
      </tr>
      <tr>
          <td>‚Üí same number of views as the input,</td>
          <td></td>
      </tr>
      <tr>
          <td>‚Üí each view sharper (higher spatial resolution) and smoothly aligned (angular consistency).</td>
          <td></td>
      </tr>
  </tbody>
</table>
<p><strong>GAN-based method</strong></p>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image_12.png"></p>
<table>
  <thead>
      <tr>
          <th>Step</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Input (SAI stack)</strong></td>
          <td>A set of <strong>low-resolution sub-aperture images (SAIs)</strong></td>
      </tr>
      <tr>
          <td><strong>Generator</strong></td>
          <td>The generator learns to <strong>upsample</strong> the SAIs and <strong>restore missing details</strong> (edges, textures, angular consistency).</td>
      </tr>
      <tr>
          <td><strong>GAN Discriminator</strong></td>
          <td>It is trained on <strong>real HR light fields</strong> (ground truth) and <strong>fake HR light fields</strong> (from the generator). Its job is to <strong>classify</strong> them as <em>real</em> or <em>fake</em>. Force the generator to create results that are <strong>indistinguishable from real data</strong> ‚Äî not just pixel-wise accurate but also visually realistic (better textures, depth edges, lighting).</td>
      </tr>
      <tr>
          <td><strong>Output (HR LF)</strong></td>
          <td>The <strong>generator alone</strong> can take any new LR light field and output an HR version that: has <strong>high spatial detail</strong>, maintains <strong>angular consistency</strong>, and looks visually realistic (not over-smoothed).</td>
      </tr>
  </tbody>
</table>
<aside>
üí°
<h3 id="summary-limitations-of-lf-reconstruction-techniques"><strong>Summary: Limitations of LF Reconstruction Techniques:</strong><a hidden class="anchor" aria-hidden="true" href="#summary-limitations-of-lf-reconstruction-techniques">#</a></h3>
<ol>
<li><strong>Early reconstruction methods</strong>
<ul>
<li>Slow to run.</li>
<li>Trained for fixed view sampling patterns ‚Üí hard to generalize to new setups.</li>
</ul>
</li>
<li><strong>Single-view SR methods (Fig. 3a)</strong>
<ul>
<li>Each sub-aperture image (SAI) is processed separately.</li>
<li>Causes <strong>geometric inconsistency</strong> between views because inter-view information isn‚Äôt used.</li>
</ul>
</li>
<li><strong>Depth-based &amp; warping methods (Fig. 3c)</strong>
<ul>
<li>Work better for <strong>wide-baseline</strong> cases (larger view spacing).</li>
<li>Depend heavily on <strong>accurate depth maps</strong> ‚Üí errors lead to <strong>tearing</strong>, <strong>ghosting</strong>, and problems with <strong>non-Lambertian (reflective) surfaces</strong>.</li>
</ul>
</li>
<li><strong>MPI-based methods (Fig. 3d)</strong>
<ul>
<li><strong>Memory-hungry</strong> and <strong>slow to train</strong> (often need multiple GPUs and days of training).</li>
<li>Model size grows with number of <strong>depth planes</strong> (larger depth budget ‚Üí bigger model).</li>
<li>Can assign wrong <strong>opacity</strong> to layers ‚Üí causes <strong>blurry reconstructions</strong>.</li>
</ul>
</li>
<li><strong>4D CNN methods (Fig. 3e)</strong>
<ul>
<li>Produce <strong>high-quality results</strong>,</li>
<li>but have <strong>high computational cost</strong> due to expensive 4D convolutions.</li>
</ul>
</li>
<li><strong>GAN-based methods (Fig. 3f)</strong>
<ul>
<li>Need <strong>large training datasets</strong>.</li>
<li>Training can suffer from <strong>instability</strong> and <strong>mode collapse</strong> (generator producing limited or repetitive outputs).</li>
</ul>
</li>
</ol>
</aside>
<h3 id="neural-scene-representation"><strong>Neural scene representation</strong><a hidden class="anchor" aria-hidden="true" href="#neural-scene-representation">#</a></h3>
<p>Researchers started using <strong>neural networks</strong> to represent 3D scenes which replaces 2D pictures.</p>
<p><strong>From image-based to neural 3D representations</strong></p>
<ul>
<li>Earlier works used <strong>image-based rendering</strong> (combine nearby views).</li>
<li>Recent advances use <strong>neural networks</strong> to represent and render 3D scenes directly.</li>
<li>3D representations can be:
<ul>
<li><strong>Explicit:</strong> meshes, voxels, point clouds.</li>
<li><strong>Implicit:</strong> continuous functions learned by networks + <strong>differentiable ray marching</strong>.</li>
</ul>
</li>
</ul>
<hr>
<p><strong>NeRF ‚Äî the core idea</strong></p>
<ul>
<li><strong>Neural Radiance Fields (NeRF)</strong> represent a scene using an <strong>MLP</strong> (multi-layer perceptron).
<ul>
<li>The network takes <strong>5D coordinates</strong> ‚Üí (x, y, z, Œ∏, œÜ): spatial position + viewing direction.</li>
<li>It outputs:
<ul>
<li><strong>Density (geometry)</strong></li>
<li><strong>Color (view-dependent radiance)</strong>.</li>
</ul>
</li>
</ul>
</li>
<li>Rendering is done by <strong>volume rendering</strong> along camera rays.</li>
</ul>
<aside>
üí°
<p><strong>How NeRF learns the scene</strong>:</p>
<ol>
<li>Feed the network <strong>2D photos</strong> of the same scene taken from <strong>different camera angles</strong>, and you must know exactly where each camera was (its <strong>pose</strong> = position + orientation). ‚Üí So NeRF knows <em>which pixel in which image</em> corresponds to <em>which ray</em> in 3D space.</li>
<li>NeRF <strong>renders</strong> its current guess (density and color) of those images using its internal 3D representation.</li>
<li>It compares them to the real photos (pixel by pixel).</li>
<li>It updates its <strong>weights</strong> using backpropagation to minimize the difference.</li>
</ol>
<p><strong>To render one image</strong> (for a camera view):</p>
<ol>
<li>
<p>For each <strong>pixel</strong>, shoot a <strong>ray</strong> through the 3D scene.</p>
</li>
<li>
<p><strong>Sample many points</strong> along that ray (like tiny steps through space).</p>
</li>
<li>
<p>At each point, ask the network for its <strong>color and density</strong>.</p>
</li>
<li>
<p>Combine (accumulate) all samples along the ray using a <strong>differentiable volume rendering formula</strong>:</p>
<p>$$
C = \sum_i T_i (1 - e^{-\sigma_i \Delta_i}) c_i
$$</p>
<p>where:</p>
<ul>
<li>C = final pixel color,</li>
<li>$\sigma_i$ = density,</li>
<li>$c_i$ = color,</li>
<li>$T_i$ = how much light passes through before reaching this point.</li>
</ul>
</li>
</ol>
<p>This process is known as <strong>differentiable ray marching.</strong></p>
<hr>
<p><strong>So the process works like this:</strong></p>
<ol>
<li>Collect multiple 2D photos of the same scene, taken from different camera angles (with known position and orientation: x, y, z, Œ∏, œÜ).</li>
<li>For each pixel in each photo, sample many 3D points along the corresponding camera ray.</li>
<li>For each sampled point, use the network to predict its color and density (or depth).</li>
<li>Aggregate the information from all sampled points to estimate the final color of the pixel.</li>
<li>Compare the predicted image to the real photo, pixel by pixel, and update the model based on the error.</li>
</ol>
</aside>
<ul>
<li>NeRF uses a <strong>coarse network</strong> (for rough geometry) and a <strong>fine network</strong> (for detailed structure).</li>
<li>Analysis:
<ul>
<li>Pros: realistic view synthesis.</li>
<li>Cons: very <strong>slow training and rendering</strong>.</li>
</ul>
</li>
</ul>
<hr>
<p><strong>Methods to improve speed and efficiency</strong></p>
<p>Several approaches speed up NeRF by changing how the scene is represented, <strong>they replaced the big neural network with simpler structures</strong>:</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Representation</th>
          <th>Key idea</th>
          <th>Pros / Cons</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Voxel-based NeRFs</strong> (Fridovich et al.)</td>
          <td>Sparse voxel grid</td>
          <td>Store opacity + spherical harmonic coefficients</td>
          <td>Faster, but memory-heavy</td>
      </tr>
      <tr>
          <td><strong>TensoRF</strong></td>
          <td>4D tensors (low-rank decomposition)</td>
          <td>Factorize radiance field into compact tensor components</td>
          <td>Efficient, compact</td>
      </tr>
      <tr>
          <td><strong>SNeRG</strong></td>
          <td>Sparse 3D voxel grid with color &amp; features</td>
          <td>Encodes view-dependent effects</td>
          <td>Fast rendering; still large memory</td>
      </tr>
      <tr>
          <td><strong>Octree NeRF</strong> (Yu et al.)</td>
          <td>Octree structure (adaptive voxels)</td>
          <td>Sample dense regions more finely</td>
          <td>Faster, but higher memory cost</td>
      </tr>
      <tr>
          <td><strong>NeX</strong></td>
          <td>Extended MPI (Multi-Plane Image)</td>
          <td>Model color as function of viewing angle using spherical bases</td>
          <td>Better view-dependent rendering</td>
      </tr>
      <tr>
          <td><strong>Ray-space embedding</strong></td>
          <td>4D ray embedding ‚Üí latent space</td>
          <td>Compact feature embedding</td>
          <td>Memory efficient but slower rendering</td>
      </tr>
      <tr>
          <td><strong>KiloNeRF</strong></td>
          <td>Thousands of tiny MLPs</td>
          <td>Divide scene into grid cells, each with a small network</td>
          <td>Great speed, coarse structure</td>
      </tr>
      <tr>
          <td><strong>3D Gaussian representation</strong></td>
          <td>Continuous Gaussians</td>
          <td>Skip empty-space computation</td>
          <td>Near real-time rendering</td>
      </tr>
      <tr>
          <td><strong>Instant-NGP (M√ºller et al.)</strong></td>
          <td>Hash-based encoding</td>
          <td>Store features in hash table for fast lookup</td>
          <td>Very fast, compact, GPU-friendly</td>
      </tr>
  </tbody>
</table>
<hr>
<p><strong>Methods to improve quality</strong></p>
<table>
  <thead>
      <tr>
          <th>Variant</th>
          <th>Key idea</th>
          <th>Benefit</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Mip-NeRF</strong></td>
          <td>Multi-scale cones for anti-aliased rendering</td>
          <td>Handles different resolutions &amp; reduces aliasing</td>
      </tr>
      <tr>
          <td><strong>NeRF++</strong></td>
          <td>Two networks: near-field &amp; far-field (spherical)</td>
          <td>Better for unbounded, complex scenes</td>
      </tr>
      <tr>
          <td><strong>Mip-NeRF 360</strong></td>
          <td>Extends Mip-NeRF for unbounded scenes; uses nonlinear parameterization &amp; regularization</td>
          <td>High-quality large-scene rendering</td>
      </tr>
  </tbody>
</table>
<p>Trade-offs and current challenges:</p>
<ul>
<li><strong>Fast methods</strong> (gaussian) ‚Üí train quickly, but lose visual quality.</li>
<li><strong>High-quality methods</strong> (NeRF, Mip-NeRF 360) ‚Üí photorealistic but <strong>slow to train</strong>.
<ul>
<li>Explicit methods also can‚Äôt be optimized directly with gradients ‚Üí need to <strong>convert</strong> trained implicit NeRFs into their format (adds complexity).</li>
</ul>
</li>
<li><strong>Real-time + high-quality rendering</strong> remains an <strong>open research challenge</strong>.</li>
</ul>
<h2 id="33-compression">3.3 Compression<a hidden class="anchor" aria-hidden="true" href="#33-compression">#</a></h2>
<aside>
üí°
<p>Three main strategies:</p>
<ol>
<li>Learning-based view synthesis on the decoder side;</li>
<li>Learning-based view synthesis on the encoder and decoder sides;</li>
<li>End-to-end light field compression architecture;</li>
</ol>
</aside>
<p>An efficient codec should be able to explore:</p>
<ul>
<li>not only the spatial and angular redundancies independently (as two-dimensional data),</li>
<li>but also the combined spatial‚Äìangular redundancy (4D data).</li>
</ul>
<blockquote>
<p><strong>The key idea of this compression architecture is bitrate saving by sparsely encoding
the views.</strong></p>
</blockquote>
<hr>
<h3 id="learning-based-view-synthesis-on-the-decoder-side"><strong>Learning-based view synthesis on the decoder side</strong><a hidden class="anchor" aria-hidden="true" href="#learning-based-view-synthesis-on-the-decoder-side">#</a></h3>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image_13.png"></p>
<table>
  <thead>
      <tr>
          <th>Step</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Input light fields</strong></td>
          <td>These are the original <strong>dense light-field data</strong>, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints.</td>
      </tr>
      <tr>
          <td><strong>Key SAIs selection</strong></td>
          <td>Instead of sending all the views, we select a few <strong>key SAIs</strong>. These key SAIs contain enough angular and spatial information to reconstruct the missing ones later.</td>
      </tr>
      <tr>
          <td><strong>Encoder</strong></td>
          <td>The <strong>encoder</strong> compresses these key SAIs into a <strong>bitstream</strong> (binary data) for transmission or storage.</td>
      </tr>
      <tr>
          <td><strong>Bitstream</strong></td>
          <td>This is the compressed data that‚Äôs transmitted or saved. It contains only <strong>the encoded information of the key SAIs</strong> (no non-key views).</td>
      </tr>
      <tr>
          <td><strong>Decoder</strong></td>
          <td>The decoder reconstructs the <strong>key SAIs</strong> from the bitstream.</td>
      </tr>
      <tr>
          <td><strong>Learning-based view synthesis</strong></td>
          <td>This is a <strong>deep neural network</strong> trained to <strong>synthesize new views (non-key SAIs)</strong> from the nearby <strong>key SAIs</strong>. Generate all <strong>non-key views</strong> ‚Üí fill in the gaps to recreate the full light field.</td>
      </tr>
      <tr>
          <td><strong>Decoded light fields</strong></td>
          <td>Produce and output the complete <strong>high-quality light field</strong> (same size as the original) after combining Decoded <strong>key SAIs</strong> and Generated <strong>non-key SAIs</strong>.</td>
      </tr>
  </tbody>
</table>
<h3 id="learning-based-view-synthesis-on-the-encoder-and-decoder-sides"><strong>Learning-based view synthesis on the encoder and decoder sides</strong><a hidden class="anchor" aria-hidden="true" href="#learning-based-view-synthesis-on-the-encoder-and-decoder-sides">#</a></h3>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image_14.png"></p>
<p><strong>Idea:</strong></p>
<ul>
<li><strong>Encode only a few key views</strong> (the main ones),</li>
<li><strong>Use deep learning</strong> to <strong>predict or reconstruct</strong> the missing views (non-key views),</li>
<li><strong>Send only the residual errors</strong> to refine those predictions.</li>
</ul>
<table>
  <thead>
      <tr>
          <th>Step</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Input light fields</strong></td>
          <td>These are the original <strong>dense light-field data</strong>, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints.</td>
      </tr>
      <tr>
          <td><strong>Key SAIs and Non-key SAIs</strong></td>
          <td>The system splits the light field into: <strong>Key SAIs:</strong> a few representative images selected for transmission (e.g., every 3rd or 4th view). <strong>Non-key SAIs:</strong> the remaining views that will be <strong>predicted</strong> rather than transmitted.</td>
      </tr>
      <tr>
          <td><strong>Encoder</strong></td>
          <td>Takes two inputs: <strong>Key SAIs</strong> and <strong>Residuals</strong> (for the non-key SAIs), compresses both into a compact <strong>bitstream</strong> (binary data) for transmission or storage.</td>
      </tr>
      <tr>
          <td><strong>Learning-based view synthesis  (encoder side)</strong></td>
          <td>This neural network <strong>predicts the non-key SAIs</strong> from the available <strong>key SAIs</strong>. so the encoder can calculate the prediction error (residual) ‚Üí the difference between the predicted and real non-key SAIs is computed as a <strong>residual</strong>.</td>
      </tr>
      <tr>
          <td><strong>Residue</strong></td>
          <td>The encoder subtracts the <strong>predicted non-key SAI</strong> (from the network) from the <strong>actual non-key SAI</strong>. only encode the <em>difference</em>, not the entire image ‚Äî saves lots of bitrate.</td>
      </tr>
      <tr>
          <td><strong>Bitstream</strong></td>
          <td>The data sent or stored ‚Äî contains compressed key SAIs + residuals.</td>
      </tr>
      <tr>
          <td><strong>Decoder</strong></td>
          <td>Receives and decompresses the bitstream. Reconstructs the <strong>key SAIs</strong> first. Then passes them to the <strong>learning-based view synthesis network</strong> to generate <strong>predicted non-key SAIs</strong>.</td>
      </tr>
      <tr>
          <td><strong>Learning-based view synthesis (decoder side)</strong></td>
          <td>Same (or similar) neural network as on the encoder side. It uses the decoded key SAIs to <strong>synthesize</strong> (predict) the non-key SAIs. Then it adds the <strong>residual</strong> (decoded correction data) to refine those synthesized views.</td>
      </tr>
      <tr>
          <td><strong>Addition (+) and output</strong></td>
          <td>The synthesized non-key SAIs are <strong>added</strong> to the decoded residuals ‚Üí final accurate <strong>non-key views</strong>. Combine <strong>key + non-key SAIs</strong> ‚Üí get the <strong>fully decoded light field</strong>.</td>
      </tr>
  </tbody>
</table>
<aside>
üí°
<p>Su et al. ‚Üí</p>
<ul>
<li>Instead of treating every pixel separately, they group <strong>light rays that belong to the same 3D point in the scene</strong>.</li>
<li>After grouping rays, some nearby super-rays may still be very similar. So they merge them into <strong>larger super-rays</strong> to save even more space.</li>
<li>Compress each super-ray using a <strong>4D Discrete Cosine Transform (DCT)</strong>.
<ul>
<li>a light field varies in both:
<ul>
<li><strong>Spatial dimensions (x, y)</strong> ‚Äî inside each image,</li>
<li><strong>Angular dimensions (u, v)</strong> ‚Äî across different viewpoints.</li>
</ul>
</li>
</ul>
</li>
<li>Better captures 4D spatial‚Äìangular redundancy but is more complex.</li>
</ul>
</aside>
<h3 id="end-to-end-light-field-compression-architecture"><strong>End-to-end light field compression architecture</strong><a hidden class="anchor" aria-hidden="true" href="#end-to-end-light-field-compression-architecture">#</a></h3>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image_15.png"></p>
<table>
  <thead>
      <tr>
          <th>Step</th>
          <th>Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Input light fields</strong></td>
          <td>These are the original <strong>dense light-field data</strong>, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints.</td>
      </tr>
      <tr>
          <td><strong>Encoder</strong></td>
          <td>This is the <strong>main compression network</strong>.It takes the input light field and <strong>learns to represent it using fewer numbers (features)</strong>.</td>
      </tr>
      <tr>
          <td><strong>Bitstream</strong></td>
          <td>The <strong>bitstream</strong> is the final compressed data produced by the encoder. It contains the quantized latent features.</td>
      </tr>
      <tr>
          <td><strong>Decoder</strong></td>
          <td>The <strong>decoder</strong> takes the bitstream and reconstructs (decodes) the light field. It performs the reverse of the encoder: expands the compact features back into full-resolution sub-aperture images.</td>
      </tr>
      <tr>
          <td><strong>Decoded Light Fields</strong></td>
          <td>These are the reconstructed sub-aperture images (SAIs).Ideally, they look almost identical to the input views, with small errors due to compression.</td>
      </tr>
  </tbody>
</table>
<blockquote>
<p><strong>End-to-end schemes</strong> are gaining more attention due to their effectiveness in image compression.</p>
</blockquote>
<aside>
üí°
<p><strong>View synthesis</strong> drawbacks can be circumvented by <strong>neural representations</strong> that achieve a level of detail that is challenging for traditional methods.</p>
</aside>
<h2 id="34-other-light-field-imaging-applications">3.4 Other light field imaging applications<a hidden class="anchor" aria-hidden="true" href="#34-other-light-field-imaging-applications">#</a></h2>
<p>Light-field (LF) images are more powerful than normal 2D photos because they capture <strong>depth, focus, and parallax</strong> ‚Äî this allows better performance in many <strong>computer vision tasks</strong>.</p>
<p>Deep learning methods are now being used to apply light-field data to various new areas.</p>
<h3 id="saliency-detection-sod"><strong>Saliency Detection (SOD)</strong><a hidden class="anchor" aria-hidden="true" href="#saliency-detection-sod">#</a></h3>
<ul>
<li><strong>Goal:</strong> detect which objects or regions attract human attention.</li>
<li><strong>LF advantage:</strong> provides both <strong>spatial</strong> and <strong>angular</strong> information, giving richer clues about object boundaries and depth.</li>
<li><strong>Typical model:</strong> encoder‚Äìdecoder <strong>two-stream networks:</strong>
<ul>
<li>One stream uses all-in-focus (center) images,</li>
<li>The other stream uses focal stacks or multi-view features.</li>
</ul>
</li>
<li>A comprehensive review compares deep LF-SOD models with standard RGB-D models.</li>
</ul>
<h3 id="face-recognition"><strong>Face Recognition</strong><a hidden class="anchor" aria-hidden="true" href="#face-recognition">#</a></h3>
<ul>
<li><strong>Goal:</strong> identify faces more accurately using multi-view data from light fields.</li>
<li><strong>LF advantage:</strong> combines <strong>intra-view</strong> (within one image) and <strong>inter-view</strong> (across multiple angles) features.</li>
<li><strong>Methods:</strong>
<ul>
<li>VGG features with <strong>LSTM</strong> layers to model view changes.</li>
<li><strong>Capsule networks</strong> with a <strong>pose matrix</strong> to handle viewpoint shifts.</li>
</ul>
</li>
<li><strong>Datasets introduced:</strong>
<ul>
<li><strong>LFFW (Light Field Faces in the Wild)</strong>,</li>
<li><strong>LFFC (Light Field Face Constrained)</strong> ‚Äî for benchmarking LF face recognition.</li>
</ul>
</li>
</ul>
<h3 id="light-field-microscopy"><strong>Light Field Microscopy</strong><a hidden class="anchor" aria-hidden="true" href="#light-field-microscopy">#</a></h3>
<ul>
<li><strong>Goal:</strong> use LF imaging to capture and reconstruct <strong>3D biological structures</strong> quickly.</li>
<li><strong>LF advantage:</strong> captures 3D spatial information in one camera shot ‚Üí <strong>instant 3D imaging</strong>.</li>
<li><strong>Deep learning usage:</strong> improves <strong>speed and quality</strong> of reconstructions.</li>
<li><strong>Methods:</strong>
<ul>
<li>Encoder‚Äìdecoder networks convert 2D LF inputs to <strong>3D volume data</strong>.</li>
<li>Networks with <strong>2D and 3D residual blocks</strong> enhance reconstruction quality.</li>
<li><strong>Convolutional sparse coding (CSC)</strong> networks use <strong>EPIs</strong> as input for fast neuron localization.</li>
</ul>
</li>
<li><strong>Applications:</strong> real-time visualization of cardiovascular or neuronal activity.</li>
<li>The network uses <strong>EPIs as inputs</strong> and generates sparse codes, representing depth data, as
outputs.</li>
</ul>
<h3 id="other-applications"><strong>Other applications</strong><a hidden class="anchor" aria-hidden="true" href="#other-applications">#</a></h3>
<ul>
<li><strong>Image classification</strong> ‚Äî improves feature learning using angular cues.</li>
<li><strong>Low-light imaging</strong> ‚Äî LF data helps reconstruct clear images in dark conditions.</li>
</ul>
<p>Overall, these works show that <strong>learning-based light-field imaging</strong> provides richer 3D understanding and better accuracy than normal 2D or RGB-D methods.</p>
<h1 id="4-datasets-and-quality-assessment">4. Datasets and quality assessment<a hidden class="anchor" aria-hidden="true" href="#4-datasets-and-quality-assessment">#</a></h1>
<h2 id="datasets">Datasets<a hidden class="anchor" aria-hidden="true" href="#datasets">#</a></h2>
<p><img alt="Characteristics of the light field datasets used to benchmark the light field imaging systems" loading="lazy" src="/notes/learningbased_light_field_imaging/image_16.png"></p>
<p>Characteristics of the light field datasets used to benchmark the light field imaging systems</p>
<h2 id="quality-assessment">Quality Assessment<a hidden class="anchor" aria-hidden="true" href="#quality-assessment">#</a></h2>
<ul>
<li>Light field imaging algorithms are typically evaluated using <strong>quantitative methods</strong> by
<strong>comparing generated data to a ground-truth</strong>.</li>
<li>Due to the diversity of light field acquisition procedures, distortions, and rendering processes, <strong>light field quality assessment remains a challenging task</strong>.</li>
<li>A recent focus has been on developing more accurate objective algorithms that <strong>extract features from both spatial and angular domains</strong> for light field quality assessment.</li>
<li>Metrics can be classified into three categories based on the <strong>availability of the reference image</strong>:
<ul>
<li>full-reference (FR),</li>
<li>reduced-reference (RR),</li>
<li>no-reference (NR).</li>
</ul>
</li>
<li>Developing <strong>NR metrics</strong> are gaining more attention due to their success in <strong>improving accuracy</strong>.</li>
<li>Current learning-based light field algorithms are still only evaluated using <strong>conventional PSNR and SSIM methods</strong>.</li>
<li>In this context, the IEEE established a new standard called ‚Äô<strong>IEEE P3333.1.4</strong>‚Äô, which defines metrics and provides recommended practices for light field quality assessment.</li>
<li>A standardization activity, namely ‚Äô<strong>JPEG Pleno Quality Assessment</strong>‚Äô, was recently initiated within the JPEG committee aiming to explore the most promising <strong>subjective quality assessment</strong> practices as well as the objective methodologies for plenoptic modalities in the context of multiple use cases.</li>
</ul>
<p><img alt="image.png" loading="lazy" src="/notes/learningbased_light_field_imaging/image_17.png"></p>
<p><img alt="Summary of the objective quality assessment methods for light fields" loading="lazy" src="/notes/learningbased_light_field_imaging/image_18.png"></p>
<p>Summary of the objective quality assessment methods for light fields</p>
<h1 id="5-discussion-challenges-and-perspectives">5. Discussion, challenges and perspectives<a hidden class="anchor" aria-hidden="true" href="#5-discussion-challenges-and-perspectives">#</a></h1>
<ul>
<li>While parallel to light fields, other plenoptic modalities like <strong>point cloud</strong> and <strong>holography</strong> have also been developed.
<ul>
<li>Even though <strong>point clouds</strong> and <strong>holographic</strong> content processing and compression have advanced significantly in recent years, these content types may eventually need to be converted to light field views for visualization on the display.</li>
<li>Light fields provide more <strong>comprehensive information</strong> when it comes to <strong>capturing scenes.</strong>
<ul>
<li>Light fields capture not only the 3D objects but also <strong>the entire scene information</strong>, which can be essential in many applications like autonomous driving, that require accurate 3D recreation of the vehicle surroundings.</li>
</ul>
</li>
</ul>
</li>
<li>Recent advances in using deep learning for spatio-angular reconstruction and the emergence of the NeRF-based approaches.</li>
<li>More recent methods, such as 3D Gaussian splatting, show improvements in
the quality‚Äìspeed trade-off.
<ul>
<li>Enhancing the quality‚Äìspeed trade-off could enable new use cases such as real-time telepresence and robotic tasks with fewer views for reconstruction.</li>
</ul>
</li>
<li>Two neural scene representations of light fields:
<ul>
<li>an explicit representation based on multiple SAIs</li>
<li>an implicit neural representation encodes light fields as parameters of an MLP.</li>
</ul>
</li>
<li>Advances in <strong>deep learning frameworks</strong> are expected to significantly improve the performance of light field processing algorithms and solve the existing challenges.
<ul>
<li>Better depth estimation or depth-free approaches are critical.</li>
</ul>
</li>
<li>Advanced methods are needed to efficiently exploit the huge amount of redundant information about the light rays in the same scene that conveys angular and spatial information
<ul>
<li>Now targeting the creation of a learning-based coding standard to provide competitive
compression efficiency compared to state-of-the-art light field coding solutions.</li>
</ul>
</li>
<li>The evaluation of <strong>light field imaging systems</strong> has several shortfalls related to the content and assessment approaches available.
<ul>
<li>Advanced methods are needed to efficiently exploit the huge amount of redundant information about the light rays in the same scene that conveys angular and spatial information.</li>
<li>It is essential to provide more comprehensive light field datasets from both the
quantitative and content diversity perspectives.</li>
</ul>
</li>
<li>The assessment of plenoptic image quality also faces various challenges because of the variety of quality aspects and complexity of the content when compared to the assessment of 2D images.
<ul>
<li>JPEG has begun developing a light field quality assessment standard, defining a framework with subjective quality assessment protocols and objective quality assessment procedures for lossy decoding of light field data within the context of multiple use cases.</li>
<li>The IEEE is also developing a standard called ‚ÄúP3333.1.4‚ÄîRecommended Practice for the
Quality Assessment of light field Imaging‚Äù that targets to establish methods of quality
assessment of light field imaging based on psychophysical studies</li>
</ul>
</li>
</ul>
<h1 id="6-conclusions">6. Conclusions<a hidden class="anchor" aria-hidden="true" href="#6-conclusions">#</a></h1>
<aside>
üëâüèº
<p><strong>Content:</strong></p>
<ol>
<li>Core Focus</li>
<li>Progress and trends</li>
<li>Challenges</li>
<li>Future outlook</li>
</ol>
</aside>
<h3 id="core-focus">Core Focus<a hidden class="anchor" aria-hidden="true" href="#core-focus">#</a></h3>
<p><strong>Main Tasks Reviewed:</strong></p>
<ul>
<li>Depth estimation, reconstruction, super-resolution, and compression.</li>
</ul>
<p><strong>Other Tasks:</strong></p>
<ul>
<li>Microscopy, saliency, face recognition, refocusing, and relighting also benefit from learning-based methods.</li>
</ul>
<h3 id="progress-and-trends">Progress and Trends<a hidden class="anchor" aria-hidden="true" href="#progress-and-trends">#</a></h3>
<p><strong>Deep Learning Integration:</strong></p>
<ul>
<li>AI frameworks now appear in almost every stage of light field processing.</li>
</ul>
<p><strong>Growth Drivers:</strong></p>
<ul>
<li>Better capture and display hardware and larger datasets will accelerate progress.</li>
</ul>
<h3 id="challenges">Challenges<a hidden class="anchor" aria-hidden="true" href="#challenges">#</a></h3>
<p><strong>Limited Realism:</strong></p>
<ul>
<li>Current systems have narrow Field of View (FoV) and Depth of Field (DoF);</li>
<li>Still far from true 6-DoF free-view exploration.</li>
</ul>
<p><strong>Data Burden:</strong></p>
<ul>
<li>Expanding datasets increase computational cost and reduce processing efficiency.</li>
</ul>
<h3 id="future-outlook">Future Outlook<a hidden class="anchor" aria-hidden="true" href="#future-outlook">#</a></h3>
<p><strong>Compression Evolution:</strong></p>
<ul>
<li>Learning-based image compression is expected to greatly improve light field storage and transmission, making real-world applications more feasible.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
