<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RWKV: Reinventing RNNs for the Transformer Era | Home</title>
<meta name="keywords" content="">
<meta name="description" content="Paper-reading notes: RWKV: Reinventing RNNs for the Transformer Era">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/notes/rwkv/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css" integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx&#43;pA=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/selfile.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/selfile.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/selfile.png">
<link rel="apple-touch-icon" href="http://localhost:1313/selfile.png">
<link rel="mask-icon" href="http://localhost:1313/selfile.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/notes/rwkv/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="" crossorigin="anonymous"></script>
<script defer>
  document.addEventListener("DOMContentLoaded", function() {
    if (typeof renderMathInElement === 'function') {
      renderMathInElement(document.body, {
        
        delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "\\[", right: "\\]", display: true},
          {left: "$", right: "$", display: false},
          {left: "\\(", right: "\\)", display: false}
        ],
        
        throwOnError: false,
        
        ignoredTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      });
    }
  });
</script>

  
  <style>
     
    ul#menu .tag-button {
      background: none;
      border: none;
      padding: 0;
      margin: 0;
      font-weight: 500;
      color: inherit;
      cursor: pointer;
      text-decoration: none;
      opacity: 0.95;
      display: inline-block;
    }
    ul#menu .tag-button:hover { text-decoration: underline; }
    ul#menu .tag-button.active { text-decoration: underline; font-weight: 600; }

     
    body.is-home .post-entry { display: none; }
    body.is-home .page-footer .pagination { display: none; }
  </style>

  
  <script>
    (function(){
      function isRootPath() {
        const p = location.pathname.replace(/\/+/g, '/');
        return p === '/' || p === '' || p === '/index.html';
      }

      if (isRootPath()) document.body.classList.add('is-home');

      
      document.addEventListener('DOMContentLoaded', function(){
        const menu = document.getElementById('menu');
        if (!menu) return;
        
        if (menu.querySelector('.tag-button')) return;
        const tags = ['Notes','Thoughts','Projects'];
        tags.forEach(t => {
          const li = document.createElement('li');
          const a = document.createElement('a');
          a.className = 'tag-button';
          a.href = `/tags/${t.toLowerCase()}/`;
          a.textContent = t;
          
          if (location.pathname.toLowerCase().startsWith(`/tags/${t.toLowerCase()}`)) a.classList.add('active');
          li.appendChild(a);
          menu.appendChild(li);
        });
      });
    })();
  </script>
<meta property="og:url" content="http://localhost:1313/notes/rwkv/">
  <meta property="og:site_name" content="Home">
  <meta property="og:title" content="RWKV: Reinventing RNNs for the Transformer Era">
  <meta property="og:description" content="Paper-reading notes: RWKV: Reinventing RNNs for the Transformer Era">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:published_time" content="2025-10-27T22:50:43+00:00">
    <meta property="article:modified_time" content="2025-10-27T22:50:43+00:00">
      <meta property="og:image" content="http://localhost:1313/notes/rwkv/41ddb454-a856-431b-b37d-b115d0022cfa.png">
      <meta property="og:image" content="http://localhost:1313/notes/rwkv/821fad7d-7a21-460e-81c3-e054ffa0a6ae.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:1313/notes/rwkv/41ddb454-a856-431b-b37d-b115d0022cfa.png">
<meta name="twitter:title" content="RWKV: Reinventing RNNs for the Transformer Era">
<meta name="twitter:description" content="Paper-reading notes: RWKV: Reinventing RNNs for the Transformer Era">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "http://localhost:1313/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "RWKV: Reinventing RNNs for the Transformer Era",
      "item": "http://localhost:1313/notes/rwkv/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RWKV: Reinventing RNNs for the Transformer Era",
  "name": "RWKV: Reinventing RNNs for the Transformer Era",
  "description": "Paper-reading notes: RWKV: Reinventing RNNs for the Transformer Era",
  "keywords": [
    
  ],
  "articleBody": "Abstract Transformers are very powerful for language tasks and training is ****faster on GPUs because of parallization, but they use a lot of memory and computing power, especially when processing long text, their cost grows very fast (quadratically).\nRNNs, on the other hand, use less memory and computation cause they grow linearly but are slower to train and not as good at handling long sentences.\nThe new model RWKV mixes the good parts of both, it trains quickly and gets good performance like a Transformer and also runs efficiently like an RNN.\nMemory and computation:\nIn a Transformer, each token looks at every other token through self-attention. If you have N tokens in a sentence, it compares every pair, so total comparisons = N × N = N². That’s why Memory and computation both grow quadratically. In an RNN, the model reads tokens one by one, passing information step by step. So for N tokens, it just does N steps, the total cost = N (linear). 1 Introduction RWKV reformulates the attention mechanism with a variant of linear attention, replacing traditional dot-product token interaction with more effective channel-directed attention. This implementation, without approximation, offers the lowest computational and memory complexity.\n2 Background 2.1 Recurrent Neural Networks (RNNs) Popular RNN architectures such as LSTM and GRU. Although these RNNs can be factored into two linear blocks (W and U) and an RNN-specific block, the data dependency relying on previous time steps prohibits parallelizing these typical RNNs.\n$$ \\begin{aligned} f_t \u0026= \\sigma_g\\left(W_f x_t + U_f h_{t-1} + b_f\\right), \\\\ i_t \u0026= \\sigma_g\\left(W_i x_t + U_i h_{t-1} + b_i\\right), \\\\ o_t \u0026= \\sigma_g\\left(W_o x_t + U_o h_{t-1} + b_o\\right), \\\\ \\tilde{c}_t \u0026= \\sigma_c\\left(W_c x_t + U_c h_{t-1} + b_c\\right), \\\\ c_t \u0026= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t, \\\\ h_t \u0026= o_t \\odot \\sigma_h(c_t). \\end{aligned} $$ Model Depends on Parallelizable? RNN computed previous state ❌ No RWKV raw previous input ✅ Yes (for training) 2.2 Transformers and AFT Standard Transformer self-attention Matrix form\n$$ \\text{Attn}{Attn}(Q,K,V)=\\text{softmax}(QK^\\top)V $$ Per token (t)\n$$ \\text{Attn}{Attn}(Q,K,V)_t=\\frac{\\sum_{i=1}^{T}\\exp(q_t^\\top k_i) \\odot v_i}{\\sum_{i=1}^{T}\\exp(q_t^\\top k_i)} $$ The weighted value equation in multi-head attention. Weight of token $i$ for query $t$ is $\\exp(q_t^\\top k_i)$. Output is a weighted average of $v_i$ AFT (Attention-Free Transformer) variant $$ \\text{Attn}{Attn}^{+}(W,K,V)_t=\\frac{\\sum_{i=1}^{t}\\exp(w_{t,i}+k_i)\\odot v_i}{\\sum_{i=1}^{t}\\exp(w_{t,i}+k_i)} $$ Replace $q_t^\\top k_i$ with (learned) position bias $w_{t,i}$ + a key score $k_i$. Causal: sum only $i\\le t$. Still a normalized weighted average, but weights depend on position via $w_{t,i}$. Variables:\ni: past token index, what we’re reading from memory t: current token index, what we’re generating now RWKV’s simplification Define the bias as a decay with distance:\n$$ w_{t,i}=-(t-i)w \\qquad w\\in(\\mathbb{R}{\\ge 0})^d $$ Per-channel vector $w$ ⇒ older tokens get weight $e^{-(t-i)w}\\le 1$. Now weights depend only on how far back a token is, not on $q_t$. This structure lets us keep running sums, so we don’t need all pairwise scores. Result:\nSame attention-like effect, but computed with O(T) time and O(1) memory per step (like an RNN), while still trainable in parallel across tokens (like a Transformer) by prefix-scan tricks.\nModel Has Query? How it computes weights Complexity Key idea Transformer ✅ Yes all tokens attend to all O(N²) Full pairwise attention, strong but heavy AFT ❌ No Uses position bias O(N) Removes query, uses learned positional weights RWKV ❌ No Adds time-decay weights O(N) AFT idea + RNN-style update (linear, efficient) 3 RWKV Four fundamental elements:\nR: The Receptance vector acts as the receiver of past information. W: The Weight signifies the positional weight decay vector, a trainable parameter within the model. K: The Key vector performs a role analogous to K in traditional attention mechanisms. V: The Value vector functions similarly to V in conventional attention processes. The difference between Time Mix and Channel Mix:\nTime Mix: Builds each token’s vector using time order and previous token info Channel Mix: Refines each token’s vector by mixing internal dimensions (features). It processes each token separately to mix and refine its feature channels. 3.1 Architecture The RWKV model is composed of stacked residual blocks. Each block consists of a time-mixing and a channel-mixing sub-block, embodying recurrent structures to leverage past information.\nThis model uses a unique attention-like score update process, which includes a time-dependent softmax operation improving numerical stability and mitigating vanishing gradients. It ensures that the gradient is propagated along the most relevant path. Additionally, layer normalization incorporated within the architecture aids in stabilizing the gradients, effectively addressing both vanishing and exploding gradient issues.\n3.1.1 Token Shift In this architecture, all linear projection vectors (R, K, V in time-mixing, and R′, K′ in channel- mixing) involved in computations are produced by linear interpolation between current and previous timestep inputs, facilitating a token shift.\nThe vectors for time-mixing computation are linear projections of linear combinations of the current and previous inputs of the block:\n$$ \\begin{aligned} r_t \u0026= W_r \\cdot \\left(\\mu_r \\odot x_t + (1 - \\mu_r) \\odot x_{t-1}\\right), \\\\ k_t \u0026= W_k \\cdot \\left(\\mu_k \\odot x_t + (1 - \\mu_k) \\odot x_{t-1}\\right), \\\\ v_t \u0026= W_v \\cdot \\left(\\mu_v \\odot x_t + (1 - \\mu_v) \\odot x_{t-1}\\right). \\end{aligned} $$ The channel-mixing inputs:\n$$ \\begin{aligned} r'_t \u0026= W'_{r} \\cdot \\left(\\mu'_{r} \\odot x_t + (1 - \\mu'_{r}) \\odot x_{t-1}\\right), \\\\ k'_t \u0026= W'_{k} \\cdot \\left(\\mu'_{k} \\odot x_t + (1 - \\mu'_{k}) \\odot x_{t-1}\\right). \\end{aligned} $$ 3.1.2 WKV Operator $$ wkv_t = \\frac{\\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} \\odot v_i + e^{u + k_t} \\odot v_t}{\\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} + e^{u + k_t}} $$ The difference of treating W between AFT and RWKV:\nPairwise matrix: Each token pair has its own weight Channel-wise vector: One decay weight per feature channel 3.1.3 Output Gating Time Mixing:\n$$ o_t = W_o \\cdot (\\sigma(r_t) \\odot wkv_t) $$ Channel Mixing:\n$$ o'_t = \\sigma(r'_t) \\odot (W'_v \\cdot \\max(k'_t, 0)^2) $$ 4 Trained Models and Computing Costs Adam optimizer, bfloat16 precision, the auxiliary loss introduced by PaLM…\n4.2 Scaling Laws Scaling laws in language models refer to the mathematical relationships that describe how the performance of a language model changes with respect to various factors.\nScaling laws are important for two primary reasons:\nthey allow us to make predictions and plans regarding the costs and performance of large models before they are trained via interpolation and extrapolation. the contexts in which they fail provides rich feedback on important areas for future research. Explain Interpolation and Extrapolation:\nInterpolation: predicting within the range of data you already know. For example, you trained models with 1B, 5B, and 10B parameters, then you interpolate to guess how a 7B model will perform. Extrapolation: predicting beyond the known range. For example, you trained up to 10B, and now you try to estimate performance of a 100B model. 5 Evaluation Evaluation direction and questions:\nCompetitiveness: Tests if RWKV performs as well as Transformers when both use the same computing power.\nLong Context: Tests if RWKV can handle very long texts better than Transformers,\nespecially when Transformers become too slow or costly for long sequences.\n6 Inference Experiments float32 precision, the HuggingFace Transformers,\n7 Future Work 1. Increase Model Expressivity\nImprove time-decay formulas. Explore better initialization of model states. Goal: more powerful representations without losing efficiency. 2. Improve Computational Efficiency\nUse parallel scan in $wkv_t$ step. Target complexity: $O(B \\log(T)d)$. 3. Apply to Encoder–Decoder Architectures\nReplace cross-attention with RWKV mechanism. Useful for seq2seq and multimodal models. Boosts efficiency in both training and inference. 4. Use of Model State (Context)\nUsed for interpretability and predictability. Could enhance safety and control via prompt tuning. Modify hidden states to guide model behavior. 5. Larger Internal States\nImprove long-term memory and context understanding. Increase performance across various tasks. 8 Conclusion We introduced RWKV, a new approach to RNN models exploiting the potential of time-based mixing components. RWKV introduces several key strategies that allow it to capture locality and long-range dependencies while addressing limitations of current architectures by: (1) replacing the quadratic QK attention with a scalar formulation at linear cost, (2) reformulating recurrence and sequential inductive biases to enable efficient training parallelization and efficient inference, and (3) enhancing training dynamics using custom initializations.\nWe benchmark the proposed architecture in a wide variety of NLP tasks and show comparable performance to SoTA with reduced cost. Further experiments on expressivity, interpretability, and scaling showcase the model capabilities and draw parallels in behavior between RWKV and other LLMs.\nRWKV opens a new route for scalable and efficient architectures to model complex relationships in sequential data. While many alternatives to Transformers have been proposed with similar claims, ours is the first to back up those claims with pretrained models with tens of billions of parameters.\n9 Limitations 1. Performance Limitation (Memory Loss over Long Contexts)\nLinear attention is efficient but may lose fine details over long sequences. RWKV compresses history into a single vector, unlike Transformers that keep all token interactions. Its recurrent design limits ability to fully “look back” at distant tokens. 2. Dependence on Prompt Engineering\nRWKV relies more on well-designed prompts than Transformers. Poor prompts may cause information loss between prompt and continuation. ",
  "wordCount" : "1499",
  "inLanguage": "en",
  "image": "http://localhost:1313/notes/rwkv/41ddb454-a856-431b-b37d-b115d0022cfa.png","datePublished": "2025-10-27T22:50:43Z",
  "dateModified": "2025-10-27T22:50:43Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/notes/rwkv/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Home",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/selfile.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Home (Alt + H)">Home</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a class="tag-button" href="http://localhost:1313/tags/notes/">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a class="tag-button" href="http://localhost:1313/tags/thoughts/">
                    <span>Thoughts</span>
                </a>
            </li>
            <li>
                <a class="tag-button" href="http://localhost:1313/tags/projects/">
                    <span>Projects</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      RWKV: Reinventing RNNs for the Transformer Era
    </h1>
    <div class="post-description">
      Paper-reading notes: RWKV: Reinventing RNNs for the Transformer Era
    </div>
    <div class="post-meta"><span title='2025-10-27 22:50:43 +0000 +0000'>October 27, 2025</span>&nbsp;·&nbsp;<span>1499 words</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#abstract" aria-label="Abstract">Abstract</a></li>
                <li>
                    <a href="#1-introduction" aria-label="1 Introduction">1 Introduction</a></li>
                <li>
                    <a href="#2-background" aria-label="2 Background">2 Background</a><ul>
                        
                <li>
                    <a href="#21-recurrent-neural-networks-rnns" aria-label="2.1 Recurrent Neural Networks (RNNs)">2.1 Recurrent Neural Networks (RNNs)</a></li>
                <li>
                    <a href="#22-transformers-and-aft" aria-label="2.2 Transformers and AFT">2.2 Transformers and AFT</a><ul>
                        
                <li>
                    <a href="#standard-transformer-self-attention" aria-label="Standard Transformer self-attention">Standard Transformer self-attention</a></li>
                <li>
                    <a href="#aft-attention-free-transformer-variant" aria-label="AFT (Attention-Free Transformer) variant">AFT (Attention-Free Transformer) variant</a></li>
                <li>
                    <a href="#rwkvs-simplification" aria-label="RWKV’s simplification">RWKV’s simplification</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#3-rwkv" aria-label="3 RWKV">3 RWKV</a><ul>
                        
                <li>
                    <a href="#31-architecture" aria-label="3.1 Architecture">3.1 Architecture</a><ul>
                        
                <li>
                    <a href="#311-token-shift" aria-label="3.1.1 Token Shift">3.1.1 Token Shift</a></li>
                <li>
                    <a href="#312-wkv-operator" aria-label="3.1.2 WKV Operator">3.1.2 WKV Operator</a></li>
                <li>
                    <a href="#313-output-gating" aria-label="3.1.3 Output Gating">3.1.3 Output Gating</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#4-trained-models-and-computing-costs" aria-label="4 Trained Models and Computing Costs">4 Trained Models and Computing Costs</a><ul>
                        
                <li>
                    <a href="#42-scaling-laws" aria-label="4.2 Scaling Laws">4.2 Scaling Laws</a></li></ul>
                </li>
                <li>
                    <a href="#5-evaluation" aria-label="5 Evaluation">5 Evaluation</a></li>
                <li>
                    <a href="#6-inference-experiments" aria-label="6 Inference Experiments">6 Inference Experiments</a></li>
                <li>
                    <a href="#7-future-work" aria-label="7 Future Work">7 Future Work</a></li>
                <li>
                    <a href="#8-conclusion" aria-label="8 Conclusion">8 Conclusion</a></li>
                <li>
                    <a href="#9-limitations" aria-label="9 Limitations">9 Limitations</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h1>
<p>Transformers are very powerful for language tasks and training is ****faster on GPUs because of <strong>parallization</strong>, but they use <strong>a lot of memory and computing power</strong>, especially when processing long text, their cost grows <strong>very fast</strong> (quadratically).</p>
<p>RNNs, on the other hand, use <strong>less memory and computation</strong> cause they grow linearly but are <strong>slower to train</strong> and not as good at handling long sentences.</p>
<p>The new model <strong>RWKV</strong> mixes the good parts of both, it trains quickly and gets good performance like a Transformer and also runs efficiently like an RNN.</p>
<aside>
<p><strong>Memory and computation:</strong></p>
<ul>
<li>In a Transformer, <strong>each token looks at every other token</strong> through <em>self-attention</em>. If you have <strong>N tokens</strong> in a sentence, it compares every pair, so total comparisons = <strong>N × N = N².</strong> That’s why <strong>Memory</strong> and <strong>computation</strong> both grow quadratically.</li>
<li>In an RNN, the model reads tokens <strong>one by one</strong>, passing information step by step. So for <strong>N tokens</strong>, it just does <strong>N steps</strong>, the total cost = <strong>N</strong> (linear).</li>
</ul>
</aside>
<h1 id="1-introduction">1 Introduction<a hidden class="anchor" aria-hidden="true" href="#1-introduction">#</a></h1>
<p>RWKV reformulates the attention mechanism with a variant of <strong>linear attention</strong>, replacing traditional <strong>dot-product</strong> token interaction with more effective <strong>channel-directed attention</strong>. This implementation, without approximation, offers the lowest computational and memory complexity.</p>
<h1 id="2-background">2 Background<a hidden class="anchor" aria-hidden="true" href="#2-background">#</a></h1>
<h2 id="21-recurrent-neural-networks-rnns">2.1 Recurrent Neural Networks (RNNs)<a hidden class="anchor" aria-hidden="true" href="#21-recurrent-neural-networks-rnns">#</a></h2>
<p>Popular RNN architectures such as <strong>LSTM</strong> and <strong>GRU.</strong> Although these RNNs can be factored into two <strong>linear blocks</strong> (W and U) and an RNN-specific block, the data dependency relying on previous time steps <strong>prohibits parallelizing</strong> these typical RNNs.</p>
<div class="math">
$$
\begin{aligned}
f_t &= \sigma_g\left(W_f x_t + U_f h_{t-1} + b_f\right), \\
i_t &= \sigma_g\left(W_i x_t + U_i h_{t-1} + b_i\right), \\
o_t &= \sigma_g\left(W_o x_t + U_o h_{t-1} + b_o\right), \\
\tilde{c}_t &= \sigma_c\left(W_c x_t + U_c h_{t-1} + b_c\right), \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t, \\
h_t &= o_t \odot \sigma_h(c_t).
\end{aligned}
$$
</div>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>Depends on</th>
          <th>Parallelizable?</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>RNN</strong></td>
          <td>computed previous state</td>
          <td>❌ No</td>
      </tr>
      <tr>
          <td><strong>RWKV</strong></td>
          <td>raw previous input</td>
          <td>✅ Yes (for training)</td>
      </tr>
  </tbody>
</table>
<h2 id="22-transformers-and-aft">2.2 Transformers and AFT<a hidden class="anchor" aria-hidden="true" href="#22-transformers-and-aft">#</a></h2>
<h3 id="standard-transformer-self-attention">Standard Transformer self-attention<a hidden class="anchor" aria-hidden="true" href="#standard-transformer-self-attention">#</a></h3>
<p><strong>Matrix form</strong></p>
<div class="math">
$$
\text{Attn}{Attn}(Q,K,V)=\text{softmax}(QK^\top)V
$$
</div>
<p><strong>Per token (t)</strong></p>
<div class="math">
$$
\text{Attn}{Attn}(Q,K,V)_t=\frac{\sum_{i=1}^{T}\exp(q_t^\top k_i) \odot v_i}{\sum_{i=1}^{T}\exp(q_t^\top k_i)}
$$
</div>
<ul>
<li>The <strong>weighted value</strong> equation in multi-head attention.
<ul>
<li>Weight of token $i$ for query $t$ is $\exp(q_t^\top k_i)$.</li>
<li>Output is a <strong>weighted average</strong> of $v_i$</li>
</ul>
</li>
</ul>
<h3 id="aft-attention-free-transformer-variant">AFT (Attention-Free Transformer) variant<a hidden class="anchor" aria-hidden="true" href="#aft-attention-free-transformer-variant">#</a></h3>
<div class="math">
$$
\text{Attn}{Attn}^{+}(W,K,V)_t=\frac{\sum_{i=1}^{t}\exp(w_{t,i}+k_i)\odot v_i}{\sum_{i=1}^{t}\exp(w_{t,i}+k_i)}
$$
</div>
<ul>
<li>Replace $q_t^\top k_i$ with <strong>(learned) position bias</strong> $w_{t,i}$ + a key score $k_i$.</li>
<li>Causal: sum only $i\le t$.</li>
<li>Still a normalized weighted average, but <strong>weights depend on position</strong> via $w_{t,i}$.</li>
</ul>
<aside>
<p>Variables:</p>
<ul>
<li><strong>i</strong>: past token index, what we’re reading from memory</li>
<li><strong>t:</strong> current token index, what we’re generating now</li>
</ul>
</aside>
<h3 id="rwkvs-simplification">RWKV’s simplification<a hidden class="anchor" aria-hidden="true" href="#rwkvs-simplification">#</a></h3>
<p>Define the bias as a <strong>decay with distance</strong>:</p>
<div class="math">
$$
w_{t,i}=-(t-i)w \qquad w\in(\mathbb{R}{\ge 0})^d
$$
</div>
<ul>
<li>Per-channel vector $w$ ⇒ older tokens get weight $e^{-(t-i)w}\le 1$.</li>
<li>Now weights depend only on <strong>how far back</strong> a token is, not on $q_t$.</li>
<li>This structure lets us keep <strong>running sums</strong>, so we don’t need all pairwise scores.</li>
</ul>
<p><strong>Result:</strong></p>
<p><strong>Same attention-like effect</strong>, but computed with <strong>O(T)</strong> time and <strong>O(1)</strong> memory per step (like an RNN), while still trainable in parallel across tokens (like a Transformer) by prefix-scan tricks.</p>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>Has Query?</th>
          <th>How it computes weights</th>
          <th>Complexity</th>
          <th>Key idea</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Transformer</strong></td>
          <td>✅ Yes</td>
          <td>all tokens attend to all</td>
          <td><strong>O(N²)</strong></td>
          <td>Full pairwise attention, strong but heavy</td>
      </tr>
      <tr>
          <td><strong>AFT</strong></td>
          <td>❌ No</td>
          <td>Uses <strong>position bias</strong></td>
          <td><strong>O(N)</strong></td>
          <td>Removes query, uses learned positional weights</td>
      </tr>
      <tr>
          <td><strong>RWKV</strong></td>
          <td>❌ No</td>
          <td>Adds <strong>time-decay weights</strong></td>
          <td><strong>O(N)</strong></td>
          <td>AFT idea + RNN-style update (linear, efficient)</td>
      </tr>
  </tbody>
</table>
<h1 id="3-rwkv">3 RWKV<a hidden class="anchor" aria-hidden="true" href="#3-rwkv">#</a></h1>
<p>Four fundamental elements:</p>
<ul>
<li><strong>R</strong>: The Receptance vector acts as the receiver of past information.</li>
<li><strong>W</strong>: The Weight signifies the <strong>positional weight decay</strong> vector, a trainable parameter within the model.</li>
<li><strong>K</strong>: The Key vector performs a role analogous to K in traditional attention mechanisms.</li>
<li><strong>V</strong>: The Value vector functions similarly to V in conventional attention processes.</li>
</ul>
<p><img alt="image.png" loading="lazy" src="/notes/rwkv/821fad7d-7a21-460e-81c3-e054ffa0a6ae.png"></p>
<aside>
<p>The difference between <strong>Time Mix</strong> and <strong>Channel Mix</strong>:</p>
<ul>
<li><strong>Time Mix:</strong>  Builds each token’s vector using <strong>time order</strong> and <strong>previous token info</strong></li>
<li><strong>Channel Mix:</strong> Refines each token’s vector by <strong>mixing internal dimensions</strong> (features). It processes <strong>each token separately</strong> to mix and refine its feature channels.</li>
</ul>
</aside>
<h2 id="31-architecture">3.1 Architecture<a hidden class="anchor" aria-hidden="true" href="#31-architecture">#</a></h2>
<p>The RWKV model is composed of <strong>stacked residual blocks</strong>. Each block consists of a <strong>time-mixing</strong> and a <strong>channel-mixing</strong> sub-block, embodying recurrent structures to leverage past information.</p>
<p>This model uses a unique <strong>attention-like score update process</strong>, which includes a time-dependent <strong>softmax</strong> operation improving numerical stability and mitigating vanishing gradients. It ensures that the gradient is propagated along the <strong>most relevant path</strong>. Additionally, layer normalization incorporated within the architecture aids in stabilizing the gradients, effectively addressing both vanishing and exploding gradient issues.</p>
<h3 id="311-token-shift">3.1.1 Token Shift<a hidden class="anchor" aria-hidden="true" href="#311-token-shift">#</a></h3>
<p>In this architecture, all <strong>linear projection vectors</strong> (R, K, V in time-mixing, and R′, K′ in channel-
mixing) involved in computations are produced by <strong>linear interpolation</strong> between current and previous timestep inputs, facilitating <strong>a token shift</strong>.</p>
<p>The vectors for time-mixing computation are linear projections of linear combinations of the current and previous inputs of the block:</p>
<div class="math">
$$
\begin{aligned}
r_t &= W_r \cdot \left(\mu_r \odot x_t + (1 - \mu_r) \odot x_{t-1}\right), \\
k_t &= W_k \cdot \left(\mu_k \odot x_t + (1 - \mu_k) \odot x_{t-1}\right), \\
v_t &= W_v \cdot \left(\mu_v \odot x_t + (1 - \mu_v) \odot x_{t-1}\right).
\end{aligned}
$$
</div>
<p>The channel-mixing inputs:</p>
<div class="math">
$$
\begin{aligned}
r'_t &= W'_{r} \cdot \left(\mu'_{r} \odot x_t + (1 - \mu'_{r}) \odot x_{t-1}\right), \\
k'_t &= W'_{k} \cdot \left(\mu'_{k} \odot x_t + (1 - \mu'_{k}) \odot x_{t-1}\right).
\end{aligned}
$$
</div>
<p><img alt="image.png" loading="lazy" src="/notes/rwkv/41ddb454-a856-431b-b37d-b115d0022cfa.png"></p>
<h3 id="312-wkv-operator">3.1.2 WKV Operator<a hidden class="anchor" aria-hidden="true" href="#312-wkv-operator">#</a></h3>
<div class="math">
$$
wkv_t = \frac{\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} \odot v_i + e^{u + k_t} \odot v_t}{\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} + e^{u + k_t}}
$$
</div>
<aside>
<p>The difference of treating <strong>W</strong> between <strong>AFT</strong> and <strong>RWKV</strong>:</p>
<ul>
<li><strong>Pairwise matrix:</strong> Each token pair has its own weight</li>
<li><strong>Channel-wise vector:</strong> One decay weight per feature channel</li>
</ul>
</aside>
<h3 id="313-output-gating">3.1.3 Output Gating<a hidden class="anchor" aria-hidden="true" href="#313-output-gating">#</a></h3>
<p><strong>Time Mixing:</strong></p>
<div class="math">
$$
o_t = W_o \cdot (\sigma(r_t) \odot wkv_t)
$$
</div>
<p><strong>Channel Mixing:</strong></p>
<div class="math">
$$
o'_t = \sigma(r'_t) \odot (W'_v \cdot \max(k'_t, 0)^2)
$$
</div>
<h1 id="4-trained-models-and-computing-costs">4 Trained Models and Computing Costs<a hidden class="anchor" aria-hidden="true" href="#4-trained-models-and-computing-costs">#</a></h1>
<p>Adam optimizer, bfloat16 precision, the auxiliary loss introduced by PaLM…</p>
<h2 id="42-scaling-laws">4.2 Scaling Laws<a hidden class="anchor" aria-hidden="true" href="#42-scaling-laws">#</a></h2>
<p>Scaling laws in language models refer to the <strong>mathematical relationships</strong> that describe how the <strong>performance</strong> of a language model <strong>changes</strong> with respect to <strong>various factors</strong>.</p>
<p>Scaling laws are important for two primary reasons:</p>
<ol>
<li>they allow us to make predictions and plans regarding the <strong>costs and performance</strong> of large models before they are trained via interpolation and extrapolation.</li>
<li>the contexts in which they fail provides rich feedback on important areas for future research.</li>
</ol>
<aside>
<p>Explain <strong>Interpolation</strong> and <strong>Extrapolation:</strong></p>
<ul>
<li><strong>Interpolation:</strong> <strong>predicting within</strong> the range of data you already know. For example, you trained models with 1B, 5B, and 10B parameters, then you <strong>interpolate</strong> to guess how a 7B model will perform.</li>
<li><strong>Extrapolation: predicting beyond</strong> the known range. For example, you trained up to 10B, and now you try to <strong>estimate</strong> performance of a 100B model.</li>
</ul>
</aside>
<h1 id="5-evaluation">5 Evaluation<a hidden class="anchor" aria-hidden="true" href="#5-evaluation">#</a></h1>
<p><strong>Evaluation direction and questions:</strong></p>
<ul>
<li>
<p><strong>Competitiveness:</strong> Tests if <strong>RWKV</strong> performs as well as <strong>Transformers</strong> when both use the <strong>same computing power</strong>.</p>
</li>
<li>
<p><strong>Long Context:</strong> Tests if <strong>RWKV</strong> can handle <strong>very long texts</strong> better than Transformers,</p>
<p>especially when Transformers become <strong>too slow or costly</strong> for long sequences.</p>
</li>
</ul>
<h1 id="6-inference-experiments">6 Inference Experiments<a hidden class="anchor" aria-hidden="true" href="#6-inference-experiments">#</a></h1>
<p>float32 precision, the HuggingFace Transformers,</p>
<h1 id="7-future-work">7 Future Work<a hidden class="anchor" aria-hidden="true" href="#7-future-work">#</a></h1>
<p><strong>1. Increase Model Expressivity</strong></p>
<ul>
<li>Improve time-decay formulas.</li>
<li>Explore better initialization of model states.</li>
<li>Goal: more powerful representations without losing efficiency.</li>
</ul>
<p><strong>2. Improve Computational Efficiency</strong></p>
<ul>
<li>Use <strong>parallel scan</strong> in $wkv_t$  step.</li>
<li>Target complexity: $O(B \log(T)d)$.</li>
</ul>
<p><strong>3. Apply to Encoder–Decoder Architectures</strong></p>
<ul>
<li>Replace cross-attention with RWKV mechanism.</li>
<li>Useful for <strong>seq2seq</strong> and <strong>multimodal</strong> models.</li>
<li>Boosts efficiency in both training and inference.</li>
</ul>
<p><strong>4. Use of Model State (Context)</strong></p>
<ul>
<li>Used for interpretability and predictability.</li>
<li>Could enhance safety and control via <strong>prompt tuning</strong>.</li>
<li>Modify hidden states to guide model behavior.</li>
</ul>
<p><strong>5. Larger Internal States</strong></p>
<ul>
<li>Improve long-term memory and context understanding.</li>
<li>Increase performance across various tasks.</li>
</ul>
<h1 id="8-conclusion">8 Conclusion<a hidden class="anchor" aria-hidden="true" href="#8-conclusion">#</a></h1>
<p>We introduced <strong>RWKV</strong>, a new approach to RNN models exploiting the potential of time-based mixing components. RWKV introduces several key strategies that allow it to capture locality and long-range dependencies while addressing limitations of current architectures by: (1) replacing the quadratic QK attention with <strong>a scalar formulation at linear cost</strong>, (2) reformulating recurrence and sequential inductive biases to enable efficient <strong>training parallelization and efficient inference</strong>, and (3) enhancing training dynamics using <strong>custom initializations</strong>.</p>
<p>We benchmark the proposed architecture in a wide variety of NLP tasks and show comparable performance to <strong>SoTA</strong> with reduced cost. Further experiments on expressivity, interpretability, and
scaling showcase the model capabilities and draw parallels in behavior between RWKV and other
LLMs.</p>
<p>RWKV opens a new route for <strong>scalable and efficient architectures</strong> to model complex relationships in <strong>sequential data</strong>. While many alternatives to Transformers have been proposed with similar claims, ours is the first to back up those claims with pretrained models with tens of billions of parameters.</p>
<h1 id="9-limitations">9 Limitations<a hidden class="anchor" aria-hidden="true" href="#9-limitations">#</a></h1>
<p><strong>1. Performance Limitation (Memory Loss over Long Contexts)</strong></p>
<ul>
<li>Linear attention is efficient but may <strong>lose fine details</strong> over long sequences.</li>
<li>RWKV compresses history into a <strong>single vector</strong>, unlike Transformers that keep all token interactions.</li>
<li>Its <strong>recurrent design</strong> limits ability to fully “look back” at distant tokens.</li>
</ul>
<p><strong>2. Dependence on Prompt Engineering</strong></p>
<ul>
<li>RWKV relies more on <strong>well-designed prompts</strong> than Transformers.</li>
<li>Poor prompts may cause information loss between prompt and continuation.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
