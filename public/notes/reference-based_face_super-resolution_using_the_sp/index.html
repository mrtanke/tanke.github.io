<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Reference-Based Face Super-Resolution Using the Spatial Transformer | Home</title>
<meta name="keywords" content="">
<meta name="description" content="Paper-reading notes: Reference-Based Face Super-Resolution Using the Spatial Transformer">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/notes/reference-based_face_super-resolution_using_the_sp/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css" integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx&#43;pA=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/selfile.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/selfile.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/selfile.png">
<link rel="apple-touch-icon" href="http://localhost:1313/selfile.png">
<link rel="mask-icon" href="http://localhost:1313/selfile.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/notes/reference-based_face_super-resolution_using_the_sp/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="" crossorigin="anonymous"></script>
<script defer>
  document.addEventListener("DOMContentLoaded", function() {
    if (typeof renderMathInElement === 'function') {
      renderMathInElement(document.body, {
        
        delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "\\[", right: "\\]", display: true},
          {left: "$", right: "$", display: false},
          {left: "\\(", right: "\\)", display: false}
        ],
        
        throwOnError: false,
        
        ignoredTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      });
    }
  });
</script>

  
  <style>
     
    ul#menu .tag-button {
      background: none;
      border: none;
      padding: 0;
      margin: 0;
      font-weight: 500;
      color: inherit;
      cursor: pointer;
      text-decoration: none;
      opacity: 0.95;
      display: inline-block;
    }
    ul#menu .tag-button:hover { text-decoration: underline; }
    ul#menu .tag-button.active { text-decoration: underline; font-weight: 600; }

     
    body.is-home .post-entry { display: none; }
    body.is-home .page-footer .pagination { display: none; }
  </style>

  
  <script>
    (function(){
      function isRootPath() {
        const p = location.pathname.replace(/\/+/g, '/');
        return p === '/' || p === '' || p === '/index.html';
      }

      if (isRootPath()) document.body.classList.add('is-home');

      
      document.addEventListener('DOMContentLoaded', function(){
        const menu = document.getElementById('menu');
        if (!menu) return;
        
        if (menu.querySelector('.tag-button')) return;
        const tags = ['Notes','Thoughts','Projects'];
        const map = { Notes: '/notes/', Thoughts: '/thoughts/', Projects: '/projects/' };
        tags.forEach(t => {
          const li = document.createElement('li');
          const a = document.createElement('a');
          a.className = 'tag-button';
          a.href = map[t];
          a.textContent = t;
          
          if (location.pathname.toLowerCase().startsWith(map[t])) a.classList.add('active');
          li.appendChild(a);
          menu.appendChild(li);
        });
      });
    })();
  </script>
<meta property="og:url" content="http://localhost:1313/notes/reference-based_face_super-resolution_using_the_sp/">
  <meta property="og:site_name" content="Home">
  <meta property="og:title" content="Reference-Based Face Super-Resolution Using the Spatial Transformer">
  <meta property="og:description" content="Paper-reading notes: Reference-Based Face Super-Resolution Using the Spatial Transformer">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:published_time" content="2025-11-07T09:32:10+00:00">
    <meta property="article:modified_time" content="2025-11-07T09:32:10+00:00">
      <meta property="og:image" content="http://localhost:1313/notes/reference-based_face_super-resolution_using_the_sp/image.png">
      <meta property="og:image" content="http://localhost:1313/notes/reference-based_face_super-resolution_using_the_sp/image_1.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:1313/notes/reference-based_face_super-resolution_using_the_sp/image.png">
<meta name="twitter:title" content="Reference-Based Face Super-Resolution Using the Spatial Transformer">
<meta name="twitter:description" content="Paper-reading notes: Reference-Based Face Super-Resolution Using the Spatial Transformer">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "http://localhost:1313/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Reference-Based Face Super-Resolution Using the Spatial Transformer",
      "item": "http://localhost:1313/notes/reference-based_face_super-resolution_using_the_sp/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Reference-Based Face Super-Resolution Using the Spatial Transformer",
  "name": "Reference-Based Face Super-Resolution Using the Spatial Transformer",
  "description": "Paper-reading notes: Reference-Based Face Super-Resolution Using the Spatial Transformer",
  "keywords": [
    
  ],
  "articleBody": "Source code: https://github.com/varun-jois/FSRST\nIntroduction Face super-resolution aims to reconstruct high-resolution (HR) facial images from low-resolution (LR) inputs, enhancing fine details. This task is challenging because it is ill-posed: many possible HR outputs can correspond to the same LR image.\nTo reduce ambiguity, Reference-Based Super-Resolution (RefSR) introduces external HR reference images that share similar content (e.g., same person’s other photos). The model can then use these reference textures and shapes to guide reconstruction.\nHowever, RefSR introduces two main challenges:\nAlignment problem – matching facial structures between LR input and HR reference images. Information aggregation problem – determining how much and which parts of each reference to use. Traditional alignment methods (like deformable convolutions) are powerful but unstable and hard to train. To overcome this, the authors propose FSRST, which uses a Spatial Transformer Network (STN) for stable alignment and a distance-based weighted aggregation for effective information fusion.\nMethod The proposed Face Super-Resolution using Spatial Transformer (FSRST) is an end-to-end model with four components:\nFeature Extractor:\nExtracts features from both the LR input and HR references using residual blocks. Reference images are converted to grayscale and reshaped (space-to-depth) to match the LR resolution.\nSpatial Transformer Alignment (STA):\nReplaces unstable deformable convolutions with a Spatial Transformer module. It predicts an affine transformation that aligns each reference’s features with those of the LR image. This alignment is differentiable and stable, ensuring good correspondence between LR and HR feature spaces.\nDistance-Based Weighted Aggregation (DWA):\nAfter alignment, the model computes the L2-distance between each aligned reference feature and the LR feature. A softmax weighting gives higher importance to more similar references, while irrelevant ones are ignored. This allows the model to dynamically use helpful references or fall back to single-image SR when references are poor.\nOutput Constructor:\nCombines aggregated and LR features and passes them through multiple residual blocks and sub-pixel convolution for upsampling. The model predicts a residual image, added to a bicubic-upsampled LR image to produce the final HR output.\nConclusion The FSRST model introduces a stable and efficient alternative to deformable alignment for reference-based face super-resolution.\nIts Spatial Transformer alignment provides consistent and accurate correspondence, while the distance-based aggregation flexibly handles multiple references.\nExperiments on DFD, CelebAMask-HQ, and VoxCeleb2 datasets show that FSRST outperforms previous methods like TTSR, C2-Matching, MRefSR, and HIME — achieving higher PSNR/SSIM with fewer parameters.\nAlthough the STN-based alignment is not fully convolutional (fixed input size), the method is lightweight, effective, and adaptable for real-time applications such as video conferencing.\nFuture work aims to extend this model to video super-resolution and make the alignment module fully convolutional.\n",
  "wordCount" : "428",
  "inLanguage": "en",
  "image": "http://localhost:1313/notes/reference-based_face_super-resolution_using_the_sp/image.png","datePublished": "2025-11-07T09:32:10Z",
  "dateModified": "2025-11-07T09:32:10Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/notes/reference-based_face_super-resolution_using_the_sp/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Home",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/selfile.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Home (Alt + H)">Home</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a class="tag-button" href="http://localhost:1313/notes/">
                    <span class="active">Notes</span>
                </a>
            </li>
            <li>
                <a class="tag-button" href="http://localhost:1313/thoughts/">
                    <span>Thoughts</span>
                </a>
            </li>
            <li>
                <a class="tag-button" href="http://localhost:1313/projects/">
                    <span>Projects</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      Reference-Based Face Super-Resolution Using the Spatial Transformer
    </h1>
    <div class="post-description">
      Paper-reading notes: Reference-Based Face Super-Resolution Using the Spatial Transformer
    </div>
    <div class="post-meta"><span title='2025-11-07 09:32:10 +0000 +0000'>November 7, 2025</span>&nbsp;·&nbsp;<span>428 words</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#method" aria-label="Method">Method</a></li></ul>
                    </ul>
                    
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Source code: <a href="https://github.com/varun-jois/FSRST">https://github.com/varun-jois/FSRST</a></p>
<h3 id="introduction"><strong>Introduction</strong><a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h3>
<p>Face super-resolution aims to reconstruct high-resolution (HR) facial images from low-resolution (LR) inputs, enhancing <strong>fine details</strong>. This task is challenging because it is <strong>ill-posed</strong>: many possible HR outputs can correspond to the same LR image.</p>
<p>To reduce ambiguity, <strong>Reference-Based Super-Resolution (RefSR)</strong> introduces external HR reference images that share similar content (e.g., same person’s other photos). The model can then use these reference textures and shapes to guide reconstruction.</p>
<p>However, RefSR introduces two main challenges:</p>
<ol>
<li><strong>Alignment problem</strong> – matching facial structures between LR input and HR reference images.</li>
<li><strong>Information aggregation problem</strong> – determining how much and which parts of each reference to use.</li>
</ol>
<p>Traditional alignment methods (like deformable convolutions) are powerful but unstable and hard to train. To overcome this, the authors propose <strong>FSRST</strong>, which uses a <strong>Spatial Transformer Network (STN)</strong> for stable alignment and a <strong>distance-based weighted aggregation</strong> for effective information fusion.</p>
<h3 id="method"><strong>Method</strong><a hidden class="anchor" aria-hidden="true" href="#method">#</a></h3>
<p>The proposed <strong>Face Super-Resolution using Spatial Transformer (FSRST)</strong> is an end-to-end model with four components:</p>
<p><img alt="image.png" loading="lazy" src="/notes/reference-based_face_super-resolution_using_the_sp/image.png"></p>
<ol>
<li>
<p><strong>Feature Extractor:</strong></p>
<p>Extracts features from both the LR input and HR references using residual blocks. Reference images are converted to grayscale and reshaped (space-to-depth) to match the LR resolution.</p>
</li>
<li>
<p><strong>Spatial Transformer Alignment (STA):</strong></p>
<p>Replaces unstable deformable convolutions with a <strong>Spatial Transformer module</strong>. It predicts an affine transformation that aligns each reference’s features with those of the LR image. This alignment is differentiable and stable, ensuring good correspondence between LR and HR feature spaces.</p>
<p><img alt="image.png" loading="lazy" src="/notes/reference-based_face_super-resolution_using_the_sp/image_1.png"></p>
</li>
<li>
<p><strong>Distance-Based Weighted Aggregation (DWA):</strong></p>
<p>After alignment, the model computes the L2-distance between each aligned reference feature and the LR feature. A <strong>softmax weighting</strong> gives higher importance to more similar references, while irrelevant ones are ignored. This allows the model to dynamically use helpful references or fall back to single-image SR when references are poor.</p>
</li>
<li>
<p><strong>Output Constructor:</strong></p>
<p>Combines aggregated and LR features and passes them through multiple residual blocks and sub-pixel convolution for upsampling. The model predicts a <strong>residual image</strong>, added to a bicubic-upsampled LR image to produce the final HR output.</p>
</li>
</ol>
<h1 id="conclusion"><strong>Conclusion</strong><a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<p>The FSRST model introduces a <strong>stable and efficient alternative</strong> to deformable alignment for reference-based face super-resolution.</p>
<p>Its <strong>Spatial Transformer alignment</strong> provides consistent and accurate correspondence, while the <strong>distance-based aggregation</strong> flexibly handles multiple references.</p>
<p>Experiments on <strong>DFD</strong>, <strong>CelebAMask-HQ</strong>, and <strong>VoxCeleb2</strong> datasets show that FSRST outperforms previous methods like TTSR, C2-Matching, MRefSR, and HIME — achieving higher PSNR/SSIM with fewer parameters.</p>
<p>Although the STN-based alignment is not fully convolutional (fixed input size), the method is lightweight, effective, and adaptable for real-time applications such as video conferencing.</p>
<p>Future work aims to extend this model to <strong>video super-resolution</strong> and make the alignment module fully convolutional.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
