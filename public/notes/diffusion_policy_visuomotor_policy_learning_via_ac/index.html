<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Diffusion Policy: Visuomotor Policy Learning via Action Diffusion | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: Diffusion Policy"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/diffusion_policy_visuomotor_policy_learning_via_ac/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/diffusion_policy_visuomotor_policy_learning_via_ac/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/diffusion_policy_visuomotor_policy_learning_via_ac/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"><meta property="og:description" content="Paper-reading notes: Diffusion Policy"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-12-28T15:32:18+00:00"><meta property="article:modified_time" content="2025-12-28T15:32:18+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/diffusion_policy_visuomotor_policy_learning_via_ac/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/diffusion_policy_visuomotor_policy_learning_via_ac/image_1.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/diffusion_policy_visuomotor_policy_learning_via_ac/image_2.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/diffusion_policy_visuomotor_policy_learning_via_ac/image.png"><meta name=twitter:title content="Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"><meta name=twitter:description content="Paper-reading notes: Diffusion Policy"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Diffusion Policy: Visuomotor Policy Learning via Action Diffusion","item":"https://my-blog-alpha-vert.vercel.app/notes/diffusion_policy_visuomotor_policy_learning_via_ac/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Diffusion Policy: Visuomotor Policy Learning via Action Diffusion","name":"Diffusion Policy: Visuomotor Policy Learning via Action Diffusion","description":"Paper-reading notes: Diffusion Policy","keywords":[],"articleBody":" A policy is any rule that maps observations/state to actions.\n1. Problems (what makes “BC = supervised learning” hard for robot actions) 1.1. Multimodal action distributions (one observation → many valid actions) In demonstrations, the same visual/state observation can reasonably lead to different next actions (e.g., go left vs right around an obstacle, different grasp styles, different sub-goal order). Explicit regressions (single Gaussian / MSE) tend to average modes and become “invalid” actions.\n1.2. High-dimensional output and temporal consistency Robot control is sequential. Predicting one step at a time often produces tiny or mode-switching (action at t chooses mode A, action at t+1 chooses mode B). But predicting a whole action sequence is high-dimensional and hard for many policy classes.\n1.3. Training instability of implicit / energy-based policies (IBC) Implicit policies (IBC) represent $p(a|o)\\propto e^{-E(o,a)}$ but training needs negative samples to estimate the intractable normalization $Z(o,\\theta)$, which can be inaccurate and causes instability and checkpoint sensitivity.\n1.4. Real-world deployment constraints Policies must be fast enough for closed-loop control; naïve diffusion conditioning on both state and action trajectories can be expensive.\n2. Method (how Diffusion Policy solves them) Core idea: policy = conditional denoising diffusion on action space\nTrain a model to predict the noise added to the expert action.\nInstead of outputting an action directly, the policy outputs a denoising direction field over actions and iteratively refines noise into an action sequence.\n2.1. DDPM sampling view: Start from Gaussian noise $x^K$. For $k=K\\to1$, update (x → x - predicted noise + noise):\n$$ x^{k-1}=\\alpha\\Big(x^k-\\gamma,\\varepsilon_\\theta(x^k,k)+\\mathcal N(0,\\sigma^2 I)\\Big) $$\nwhere $\\varepsilon_\\theta$ is a learned “noise predictor” (can be seen as a learned gradient field).\n2.2. Adapt DDPM to visuomotor policy: generate actions, conditioned on observations\nThe denoising update becomes:\n$$ A_t^{k-1}=\\alpha\\Big(A_t^k-\\gamma,\\varepsilon_\\theta(O_t, A_t^k, k)+\\mathcal N(0,\\sigma^2 I)\\Big) $$\nSo the network input is (observation features, noisy action sequence, diffusion step k) and output is predicted noise / denoising direction for that step.\n2.3. Training: standard diffusion noise-prediction MSE (stable)\nPick a clean expert action sequence $A_t^0$, sample a diffusion step $k$, add noise $\\varepsilon_k$, train (actual noise added to the expert actions = predicted noise):\n$$ L=\\mathrm{MSE}\\big(\\varepsilon_k,\\ \\varepsilon_\\theta(O_t,\\ A_t^0+\\varepsilon_k,\\ k)\\big) $$\nThis avoids EBM’s intractable $Z(o,\\theta)$ and negative sampling.\n2.4. Key technical contributions to make it work well on robots Closed-loop action sequences + receding horizon control\nAt time $t$, use last $T_o$ observations $O_t$ to predict a future horizon of actions $T_p$, execute only $T_a$ steps, then re-plan (receding horizon). Warm-start the next plan for smoothness.\nVisual conditioning for speed\nTreat vision as conditioning: compute visual features once and reuse them across diffusion iterations, enabling real-time inference.\nTime-series diffusion transformer\nA transformer-based denoiser improves tasks requiring high-frequency action changes / velocity control, reducing over-smoothing seen in CNN temporal models.\nThe expert trajectory contains: move → stop → move quickly → stop A CNN tends to produce: move → slow → slow → slow Real-time acceleration via DDIM\nUse DDIM(Denoising Diffusion Implicit Models) to reduce latency (reported ~0.1s on RTX 3080 for real-world). Train with many diffusion steps (e.g., 100), but infer with fewer (e.g., 10–16)\n3. Novelty what’s new vs prior policy representations?\n3.1. New policy representation: “diffusion on action space” They frame a visuomotor policy as a conditional denoising diffusion process over action sequences, not direct regression (explicit) and not energy-minimization with negatives (EBM).\n3.2. Handles multimodality naturally via stochastic sampling + iterative refinement Different random initializations $A_t^K$ and stochastic updates let the policy represent and sample multiple valid action modes, and action-sequence prediction helps it commit to one mode per rollout.\n3.3. Scales to high-dimensional outputs by predicting action sequences Diffusion models are known to work in high dimension; they exploit this to model whole trajectories, improving temporal consistency and robustness (including idle-action segments).\n3.4. Training stability advantage over IBC/EBM They explicitly explain stability: EBM needs $Z(o,\\theta)$ and negative sampling; diffusion learns the score $\\nabla_a \\log p(a|o)$ which does not depend on $Z$, so training/inference avoid that source of instability.\n3.5. System-level robotics contributions Receding-horizon execution, efficient visual conditioning(Extract observations once, then reuse them across all diffusion steps), and a time-series diffusion transformer are concrete robotics-driven changes that “unlock” diffusion for real-world visuomotor control.\n","wordCount":"693","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/diffusion_policy_visuomotor_policy_learning_via_ac/image.png","datePublished":"2025-12-28T15:32:18Z","dateModified":"2025-12-28T15:32:18Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/diffusion_policy_visuomotor_policy_learning_via_ac/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</h1><div class=post-description>Paper-reading notes: Diffusion Policy</div><div class=post-meta><span title='2025-12-28 15:32:18 +0000 +0000'>December 28, 2025</span>&nbsp;·&nbsp;<span>693 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-problems-what-makes-bc--supervised-learning-hard-for-robot-actions aria-label="1. Problems (what makes “BC = supervised learning” hard for robot actions)">1. Problems (what makes “BC = supervised learning” hard for robot actions)</a><ul><li><a href=#11-multimodal-action-distributions-one-observation--many-valid-actions aria-label="1.1. Multimodal action distributions (one observation → many valid actions)">1.1. Multimodal action distributions (one observation → many valid actions)</a></li><li><a href=#12-high-dimensional-output-and-temporal-consistency aria-label="1.2. High-dimensional output and temporal consistency">1.2. High-dimensional output and temporal consistency</a></li><li><a href=#13-training-instability-of-implicit--energy-based-policies-ibc aria-label="1.3. Training instability of implicit / energy-based policies (IBC)">1.3. Training instability of implicit / energy-based policies (IBC)</a></li><li><a href=#14-real-world-deployment-constraints aria-label="1.4. Real-world deployment constraints">1.4. Real-world deployment constraints</a></li></ul></li><li><a href=#2-method-how-diffusion-policy-solves-them aria-label="2. Method (how Diffusion Policy solves them)">2. Method (how Diffusion Policy solves them)</a><ul><li><a href=#21-ddpm-sampling-view aria-label="2.1. DDPM sampling view:">2.1. DDPM sampling view:</a></li><li><a href=#22-adapt-ddpm-to-visuomotor-policy aria-label="2.2. Adapt DDPM to visuomotor policy:">2.2. Adapt DDPM to visuomotor policy:</a></li><li><a href=#23-training aria-label="2.3. Training:">2.3. Training:</a></li><li><a href=#24-key-technical-contributions-to-make-it-work-well-on-robots aria-label="2.4. Key technical contributions to make it work well on robots">2.4. Key technical contributions to make it work well on robots</a></li></ul></li><li><a href=#3-novelty aria-label="3. Novelty">3. Novelty</a><ul><li><a href=#31-new-policy-representation-diffusion-on-action-space aria-label="3.1. New policy representation: “diffusion on action space”">3.1. New policy representation: “diffusion on action space”</a></li><li><a href=#32-handles-multimodality-naturally-via-stochastic-sampling--iterative-refinement aria-label="3.2. Handles multimodality naturally via stochastic sampling + iterative refinement">3.2. Handles multimodality naturally via stochastic sampling + iterative refinement</a></li><li><a href=#33-scales-to-high-dimensional-outputs-by-predicting-action-sequences aria-label="3.3. Scales to high-dimensional outputs by predicting action sequences">3.3. Scales to high-dimensional outputs by predicting action sequences</a></li><li><a href=#34-training-stability-advantage-over-ibcebm aria-label="3.4. Training stability advantage over IBC/EBM">3.4. Training stability advantage over IBC/EBM</a></li><li><a href=#35-system-level-robotics-contributions aria-label="3.5. System-level robotics contributions">3.5. System-level robotics contributions</a></li></ul></li></ul></div></details></div><div class=post-content><aside><p>A policy is any rule that maps observations/state to actions.</p></aside><h1 id=1-problems-what-makes-bc--supervised-learning-hard-for-robot-actions>1. Problems (what makes “BC = supervised learning” hard for robot actions)<a hidden class=anchor aria-hidden=true href=#1-problems-what-makes-bc--supervised-learning-hard-for-robot-actions>#</a></h1><h2 id=11-multimodal-action-distributions-one-observation--many-valid-actions><strong>1.1. Multimodal action distributions (one observation → many valid actions)</strong><a hidden class=anchor aria-hidden=true href=#11-multimodal-action-distributions-one-observation--many-valid-actions>#</a></h2><p>In demonstrations, the same visual/state observation can reasonably lead to different next actions (e.g., go left vs right around an obstacle, different grasp styles, different sub-goal order). Explicit regressions (single Gaussian / MSE) tend to <strong>average modes</strong> and become “invalid” actions.</p><h2 id=12-high-dimensional-output-and-temporal-consistency><strong>1.2. High-dimensional output and temporal consistency</strong><a hidden class=anchor aria-hidden=true href=#12-high-dimensional-output-and-temporal-consistency>#</a></h2><p>Robot control is sequential. Predicting <strong>one step</strong> at a time often produces tiny or mode-switching (action at t chooses mode A, action at t+1 chooses mode B). But predicting a <strong>whole action sequence</strong> is high-dimensional and hard for many policy classes.</p><h2 id=13-training-instability-of-implicit--energy-based-policies-ibc><strong>1.3. Training instability of implicit / energy-based policies (IBC)</strong><a hidden class=anchor aria-hidden=true href=#13-training-instability-of-implicit--energy-based-policies-ibc>#</a></h2><p>Implicit policies (IBC) represent $p(a|o)\propto e^{-E(o,a)}$ but training needs <strong>negative samples</strong> to estimate the intractable normalization $Z(o,\theta)$, which can be inaccurate and causes instability and checkpoint sensitivity.</p><p><img alt=image.png loading=lazy src=/notes/diffusion_policy_visuomotor_policy_learning_via_ac/image.png></p><h2 id=14-real-world-deployment-constraints><strong>1.4. Real-world deployment constraints</strong><a hidden class=anchor aria-hidden=true href=#14-real-world-deployment-constraints>#</a></h2><p>Policies must be <strong>fast enough</strong> for closed-loop control; naïve diffusion conditioning on both state and action trajectories can be expensive.</p><h1 id=2-method-how-diffusion-policy-solves-them>2. Method (how Diffusion Policy solves them)<a hidden class=anchor aria-hidden=true href=#2-method-how-diffusion-policy-solves-them>#</a></h1><aside><p>Core idea: policy = conditional denoising diffusion on action space</p><p>Train a model to predict the noise added to the expert action.</p></aside><p><img alt=image.png loading=lazy src=/notes/diffusion_policy_visuomotor_policy_learning_via_ac/image_1.png></p><p>Instead of outputting an action directly, the policy outputs a <strong>denoising direction field</strong> over actions and iteratively refines noise into an action sequence.</p><h2 id=21-ddpm-sampling-view><strong>2.1. DDPM sampling view:</strong><a hidden class=anchor aria-hidden=true href=#21-ddpm-sampling-view>#</a></h2><p>Start from Gaussian noise $x^K$. For $k=K\to1$, update (x → x - predicted noise + noise):</p><p>$$
x^{k-1}=\alpha\Big(x^k-\gamma,\varepsilon_\theta(x^k,k)+\mathcal N(0,\sigma^2 I)\Big)
$$</p><p>where $\varepsilon_\theta$ is a learned “noise predictor” (can be seen as a learned gradient field).</p><h2 id=22-adapt-ddpm-to-visuomotor-policy>2.2. Adapt DDPM to visuomotor policy:<a hidden class=anchor aria-hidden=true href=#22-adapt-ddpm-to-visuomotor-policy>#</a></h2><aside><p>generate <strong>actions</strong>, conditioned on <strong>observations</strong></p></aside><p>The denoising update becomes:</p><p>$$
A_t^{k-1}=\alpha\Big(A_t^k-\gamma,\varepsilon_\theta(O_t, A_t^k, k)+\mathcal N(0,\sigma^2 I)\Big)
$$</p><p>So the network input is <strong>(observation features, noisy action sequence, diffusion step k)</strong> and output is <strong>predicted noise / denoising direction</strong> for that step.</p><h2 id=23-training>2.3. Training:<a hidden class=anchor aria-hidden=true href=#23-training>#</a></h2><aside><p>standard diffusion noise-prediction MSE (stable)</p></aside><p>Pick a clean expert action sequence $A_t^0$, sample a diffusion step $k$, add noise $\varepsilon_k$, train (actual noise added to the expert actions = predicted noise):</p><p>$$
L=\mathrm{MSE}\big(\varepsilon_k,\ \varepsilon_\theta(O_t,\ A_t^0+\varepsilon_k,\ k)\big)
$$</p><p>This avoids EBM’s intractable $Z(o,\theta)$ and negative sampling.</p><h2 id=24-key-technical-contributions-to-make-it-work-well-on-robots>2.4. Key technical contributions to make it work well on robots<a hidden class=anchor aria-hidden=true href=#24-key-technical-contributions-to-make-it-work-well-on-robots>#</a></h2><ol><li><p><strong>Closed-loop action sequences + receding horizon control</strong></p><p>At time $t$, use last $T_o$ observations $O_t$ to predict a <strong>future horizon</strong> of actions $T_p$, execute only $T_a$ steps, then re-plan (receding horizon). Warm-start the next plan for smoothness.</p></li><li><p><strong>Visual conditioning for speed</strong></p><p>Treat vision as conditioning: compute visual features <strong>once</strong> and reuse them across diffusion iterations, enabling real-time inference.</p></li><li><p><strong>Time-series diffusion transformer</strong></p><p>A transformer-based denoiser improves tasks requiring <strong>high-frequency action changes / velocity control</strong>, reducing over-smoothing seen in CNN temporal models.</p><ul><li>The expert trajectory contains: move → stop → move quickly → stop</li><li>A CNN tends to produce: move → slow → slow → slow</li></ul></li><li><p><strong>Real-time acceleration via DDIM</strong></p><p>Use DDIM(Denoising Diffusion Implicit Models) to reduce latency (reported ~0.1s on RTX 3080 for real-world). Train with many diffusion steps (e.g., 100), but infer with fewer (e.g., 10–16)</p></li></ol><h1 id=3-novelty>3. Novelty<a hidden class=anchor aria-hidden=true href=#3-novelty>#</a></h1><aside><p>what’s new vs prior policy representations?</p></aside><h2 id=31-new-policy-representation-diffusion-on-action-space><strong>3.1. New policy representation: “diffusion on action space”</strong><a hidden class=anchor aria-hidden=true href=#31-new-policy-representation-diffusion-on-action-space>#</a></h2><p>They frame a visuomotor policy as a <strong>conditional denoising diffusion process over action sequences</strong>, not direct regression (explicit) and not energy-minimization with negatives (EBM).</p><p><img alt=image.png loading=lazy src=/notes/diffusion_policy_visuomotor_policy_learning_via_ac/image_2.png></p><h2 id=32-handles-multimodality-naturally-via-stochastic-sampling--iterative-refinement><strong>3.2. Handles multimodality naturally via stochastic sampling + iterative refinement</strong><a hidden class=anchor aria-hidden=true href=#32-handles-multimodality-naturally-via-stochastic-sampling--iterative-refinement>#</a></h2><p>Different random initializations $A_t^K$ and stochastic updates let the policy represent and sample <strong>multiple valid action modes</strong>, and action-sequence prediction helps it <strong>commit to one mode per rollout</strong>.</p><h2 id=33-scales-to-high-dimensional-outputs-by-predicting-action-sequences><strong>3.3. Scales to high-dimensional outputs by predicting action sequences</strong><a hidden class=anchor aria-hidden=true href=#33-scales-to-high-dimensional-outputs-by-predicting-action-sequences>#</a></h2><p>Diffusion models are known to work in high dimension; they exploit this to model <strong>whole trajectories</strong>, improving temporal consistency and robustness (including idle-action segments).</p><h2 id=34-training-stability-advantage-over-ibcebm><strong>3.4. Training stability advantage over IBC/EBM</strong><a hidden class=anchor aria-hidden=true href=#34-training-stability-advantage-over-ibcebm>#</a></h2><p>They explicitly explain stability: EBM needs $Z(o,\theta)$ and negative sampling; diffusion learns the <strong>score</strong> $\nabla_a \log p(a|o)$ which does <strong>not</strong> depend on $Z$, so training/inference avoid that source of instability.</p><h2 id=35-system-level-robotics-contributions><strong>3.5. System-level robotics contributions</strong><a hidden class=anchor aria-hidden=true href=#35-system-level-robotics-contributions>#</a></h2><p>Receding-horizon execution, efficient visual conditioning(Extract observations once, then reuse them across all diffusion steps), and a time-series diffusion transformer are concrete robotics-driven changes that “unlock” diffusion for real-world visuomotor control.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>