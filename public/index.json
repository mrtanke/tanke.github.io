[{"content":" A policy is any rule that maps observations/state to actions.\n1. Problems (what makes ‚ÄúBC = supervised learning‚Äù hard for robot actions) 1.1. Multimodal action distributions (one observation ‚Üí many valid actions) In demonstrations, the same visual/state observation can reasonably lead to different next actions (e.g., go left vs right around an obstacle, different grasp styles, different sub-goal order). Explicit regressions (single Gaussian / MSE) tend to average modes and become ‚Äúinvalid‚Äù actions.\n1.2. High-dimensional output and temporal consistency Robot control is sequential. Predicting one step at a time often produces tiny or mode-switching (action at t chooses mode A, action at t+1 chooses mode B). But predicting a whole action sequence is high-dimensional and hard for many policy classes.\n1.3. Training instability of implicit / energy-based policies (IBC) Implicit policies (IBC) represent $p(a|o)\\propto e^{-E(o,a)}$ but training needs negative samples to estimate the intractable normalization $Z(o,\\theta)$, which can be inaccurate and causes instability and checkpoint sensitivity.\n1.4. Real-world deployment constraints Policies must be fast enough for closed-loop control; na√Øve diffusion conditioning on both state and action trajectories can be expensive.\n2. Method (how Diffusion Policy solves them) Core idea: policy = conditional denoising diffusion on action space\nTrain a model to predict the noise added to the expert action.\nInstead of outputting an action directly, the policy outputs a denoising direction field over actions and iteratively refines noise into an action sequence.\n2.1. DDPM sampling view: Start from Gaussian noise $x^K$. For $k=K\\to1$, update (x ‚Üí x - predicted noise + noise):\n$$ x^{k-1}=\\alpha\\Big(x^k-\\gamma,\\varepsilon_\\theta(x^k,k)+\\mathcal N(0,\\sigma^2 I)\\Big) $$\nwhere $\\varepsilon_\\theta$ is a learned ‚Äúnoise predictor‚Äù (can be seen as a learned gradient field).\n2.2. Adapt DDPM to visuomotor policy: generate actions, conditioned on observations\nThe denoising update becomes:\n$$ A_t^{k-1}=\\alpha\\Big(A_t^k-\\gamma,\\varepsilon_\\theta(O_t, A_t^k, k)+\\mathcal N(0,\\sigma^2 I)\\Big) $$\nSo the network input is (observation features, noisy action sequence, diffusion step k) and output is predicted noise / denoising direction for that step.\n2.3. Training: standard diffusion noise-prediction MSE (stable)\nPick a clean expert action sequence $A_t^0$, sample a diffusion step $k$, add noise $\\varepsilon_k$, train (actual noise added to the expert actions = predicted noise):\n$$ L=\\mathrm{MSE}\\big(\\varepsilon_k,\\ \\varepsilon_\\theta(O_t,\\ A_t^0+\\varepsilon_k,\\ k)\\big) $$\nThis avoids EBM‚Äôs intractable $Z(o,\\theta)$ and negative sampling.\n2.4. Key technical contributions to make it work well on robots Closed-loop action sequences + receding horizon control\nAt time $t$, use last $T_o$ observations $O_t$ to predict a future horizon of actions $T_p$, execute only $T_a$ steps, then re-plan (receding horizon). Warm-start the next plan for smoothness.\nVisual conditioning for speed\nTreat vision as conditioning: compute visual features once and reuse them across diffusion iterations, enabling real-time inference.\nTime-series diffusion transformer\nA transformer-based denoiser improves tasks requiring high-frequency action changes / velocity control, reducing over-smoothing seen in CNN temporal models.\nThe expert trajectory contains: move ‚Üí stop ‚Üí move quickly ‚Üí stop A CNN tends to produce: move ‚Üí slow ‚Üí slow ‚Üí slow Real-time acceleration via DDIM\nUse DDIM(Denoising Diffusion Implicit Models) to reduce latency (reported ~0.1s on RTX 3080 for real-world). Train with many diffusion steps (e.g., 100), but infer with fewer (e.g., 10‚Äì16)\n3. Novelty what‚Äôs new vs prior policy representations?\n3.1. New policy representation: ‚Äúdiffusion on action space‚Äù They frame a visuomotor policy as a conditional denoising diffusion process over action sequences, not direct regression (explicit) and not energy-minimization with negatives (EBM).\n3.2. Handles multimodality naturally via stochastic sampling + iterative refinement Different random initializations $A_t^K$ and stochastic updates let the policy represent and sample multiple valid action modes, and action-sequence prediction helps it commit to one mode per rollout.\n3.3. Scales to high-dimensional outputs by predicting action sequences Diffusion models are known to work in high dimension; they exploit this to model whole trajectories, improving temporal consistency and robustness (including idle-action segments).\n3.4. Training stability advantage over IBC/EBM They explicitly explain stability: EBM needs $Z(o,\\theta)$ and negative sampling; diffusion learns the score $\\nabla_a \\log p(a|o)$ which does not depend on $Z$, so training/inference avoid that source of instability.\n3.5. System-level robotics contributions Receding-horizon execution, efficient visual conditioning(Extract observations once, then reuse them across all diffusion steps), and a time-series diffusion transformer are concrete robotics-driven changes that ‚Äúunlock‚Äù diffusion for real-world visuomotor control.\n","permalink":"/notes/diffusion_policy_visuomotor_policy_learning_via_ac/","summary":"\u003caside\u003e\n\u003cp\u003eA policy is any rule that maps observations/state to actions.\u003c/p\u003e\n\u003c/aside\u003e\n\u003ch1 id=\"1-problems-what-makes-bc--supervised-learning-hard-for-robot-actions\"\u003e1. Problems (what makes ‚ÄúBC = supervised learning‚Äù hard for robot actions)\u003c/h1\u003e\n\u003ch2 id=\"11-multimodal-action-distributions-one-observation--many-valid-actions\"\u003e\u003cstrong\u003e1.1. Multimodal action distributions (one observation ‚Üí many valid actions)\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eIn demonstrations, the same visual/state observation can reasonably lead to different next actions (e.g., go left vs right around an obstacle, different grasp styles, different sub-goal order). Explicit regressions (single Gaussian / MSE) tend to \u003cstrong\u003eaverage modes\u003c/strong\u003e and become ‚Äúinvalid‚Äù actions.\u003c/p\u003e","title":"Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"},{"content":"Problem Self-attention in Transformers is built on query‚Äìkey dot products, which are widely believed to be essential for modeling token interactions and long-range dependencies. However, it is unclear whether this content-based, pairwise similarity computation is truly necessary for good performance.\nThe paper questions three common assumptions:\nThat attention weights must be computed from token‚Äìtoken interactions (Q¬∑K) That attention must be instance-specific rather than globally learned That dot-product attention is the key reason for Transformer success In short, the problem is to understand how important dot-product self-attention really is, and whether simpler or alternative mechanisms can replace it without hurting performance .\nMethod The paper proposes Synthetic Attention, which removes query‚Äìkey dot products entirely and instead directly learns (or generates) the attention/alignment matrix.\nCore idea:\nInstead of computing attention weights via token similarity, synthesize them using parameterized functions.\nThe proposed SYNTHESIZER replaces standard self-attention with:\nDense Synthesizer:\nEach token independently predicts its attention weights using an MLP (no token‚Äìtoken interaction).\nRandom Synthesizer:\nAttention weights are global, randomly initialized matrices (trainable or fixed), shared across all inputs.\nFactorized Synthesizers:\nLow-rank versions to reduce parameters and improve efficiency.\nMixture Models:\nCombine synthetic attention with dot-product attention, showing they are complementary.\nThe model keeps the rest of the Transformer unchanged (values, feed-forward layers, multi-head structure) and is evaluated across machine translation, language modeling, text generation, and GLUE/SuperGLUE benchmarks.\nResults show that synthetic attention alone is often competitive, and combining it with dot-product attention can outperform standard Transformers .\n","permalink":"/notes/synthesizer_rethinking_self-attention_for_transfor/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eSelf-attention in Transformers is built on query‚Äìkey dot products\u003c/strong\u003e, which are widely believed to be essential for modeling token interactions and long-range dependencies. However, it is unclear \u003cstrong\u003ewhether this content-based, pairwise similarity computation is truly necessary\u003c/strong\u003e for good performance.\u003c/p\u003e\n\u003cp\u003eThe paper questions three common assumptions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThat attention weights must be computed from \u003cstrong\u003etoken‚Äìtoken interactions (Q¬∑K)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThat attention must be \u003cstrong\u003einstance-specific\u003c/strong\u003e rather than globally learned\u003c/li\u003e\n\u003cli\u003eThat dot-product attention is the key reason for Transformer success\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn short, the problem is to understand \u003cstrong\u003ehow important dot-product self-attention really is\u003c/strong\u003e, and whether simpler or alternative mechanisms can replace it without hurting performance .\u003c/p\u003e","title":"Synthesizer: Rethinking Self-Attention for Transformer Models"},{"content":"Problem Modern Transformers are powerful but fundamentally opaque. Even with mechanistic interpretability tools (attention analysis, circuit discovery), understanding what algorithm a trained Transformer implements still requires heavy manual effort and is often incomplete or misleading.\nPrevious work like RASP and Tracr shows that human-written programs can be compiled into Transformers, but not the reverse: we cannot take an arbitrary trained Transformer and reliably recover a faithful, human-readable program.\nCore problem:\nCan we train Transformers so that their learned computation is, by construction, directly convertible into a discrete, interpretable program‚Äîwithout post-hoc reverse engineering?\nThis paper targets intrinsic interpretability, not explanation after the fact.\nMethod The authors propose Transformer Programs:\na restricted Transformer architecture that is trained with gradients but constrained so it can be deterministically decompiled into a RASP-style program.\nThe method has three key ideas:\n1. Disentangled Residual Stream (Program Variables) The residual stream is split into named, orthogonal variables (e.g. tokens, positions, attn_1_output, ‚Ä¶). Each attention head reads specific variables (V) and writes a new variable to a fresh slot. This mirrors RASP‚Äôs shared tape, but implemented as structured residual slots. 2. Interpretable Attention = select + aggregate Each attention head is constrained to implement: a discrete predicate (which query attends to which key) followed by hard attention (attend to exactly one key). Attention predicates are parameterized as categorical choices, learned with Gumbel-Softmax, then discretized. This makes every head equivalent to a RASP select operator, with aggregation matching aggregate. 3. Discrete Optimization ‚Üí Program Extraction Training is done with continuous relaxations of discrete choices. After training, the model is fully discretized:\npredicates become if/else rules, attention becomes symbolic selection, MLPs become lookup tables. The entire model is then automatically converted into Python code, functionally identical to the Transformer and debuggable with standard tools.\nExtensions Numerical attention for counting (RASP selector_width) Constrained MLPs as finite lookup tables Learned word embeddings decomposed into categorical variables One-sentence takeaway Instead of explaining black-box Transformers, this paper trains Transformers that can be directly converted into programs, so interpretation becomes reading code‚Äînot guessing behavior.\n","permalink":"/notes/learning_transformer_programs/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eModern Transformers are powerful but \u003cstrong\u003efundamentally opaque\u003c/strong\u003e. Even with mechanistic interpretability tools (attention analysis, circuit discovery), understanding \u003cem\u003ewhat algorithm a trained Transformer implements\u003c/em\u003e still requires \u003cstrong\u003eheavy manual effort\u003c/strong\u003e and is often \u003cstrong\u003eincomplete or misleading\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003ePrevious work like \u003cstrong\u003eRASP\u003c/strong\u003e and \u003cstrong\u003eTracr\u003c/strong\u003e shows that \u003cstrong\u003ehuman-written programs can be compiled into Transformers\u003c/strong\u003e, but \u003cstrong\u003enot the reverse\u003c/strong\u003e: we cannot take an arbitrary trained Transformer and reliably recover a faithful, human-readable program.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCore problem:\u003c/strong\u003e\u003c/p\u003e","title":"Learning Transformer Programs"},{"content":"1. Problem Transformer models achieve strong performance but are extremely inefficient for long sequences and deep architectures. In practice, they suffer from three fundamental bottlenecks:\nActivation memory scales linearly with the number of layers, because activations must be stored for backpropagation; Feed-forward layers consume large memory, since the intermediate dimension $d_{ff}$ is much larger than the model dimension; Self-attention has quadratic time and memory complexity $O(L^2)$ in sequence length, making long sequences (e.g., 64K tokens) impractical even with memory-efficient implementations. As a result, large Transformers often cannot be trained or even fine-tuned on a single accelerator, limiting scalability and accessibility. The key question posed by the paper is whether this limitation is fundamental‚Äîor merely due to architectural inefficiency.\n2. Method Reformer addresses these inefficiencies by redesigning the Transformer with three complementary techniques:\n2.1. Locality-Sensitive Hashing (LSH) Attention Full dot-product attention is replaced with an approximate attention mechanism based on angular locality-sensitive hashing. Queries and keys are shared $(Q=K)$ and hashed so that only nearby tokens (in embedding space) attend to each other. This reduces attention complexity from $O(L^2)$ to $O(L \\log L)$ while preserving accuracy through multiple hashing rounds.\n2.2. Reversible Residual Layers Standard residual connections are replaced with reversible layers, allowing activations from earlier layers to be reconstructed during backpropagation instead of stored. This removes the linear dependence on the number of layers in activation memory.\n2.3. Chunked Feed-Forward Computation Feed-forward layers are computed in chunks over the sequence dimension, exploiting position-wise independence. This avoids storing large intermediate activations of size $d_{ff}$, further reducing peak memory usage.\nTakeaway Together, these changes produce the Reformer architecture, which matches standard Transformer performance while enabling efficient training on very long sequences and deep models with dramatically lower memory requirements.\n","permalink":"/notes/reformer_the_efficient_transformer/","summary":"\u003ch1 id=\"1-problem\"\u003e1. Problem\u003c/h1\u003e\n\u003cp\u003eTransformer models achieve strong performance but are \u003cstrong\u003eextremely inefficient for long sequences and deep architectures\u003c/strong\u003e. In practice, they suffer from three fundamental bottlenecks:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eActivation memory scales linearly with the number of layers\u003c/strong\u003e, because activations must be stored for backpropagation;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFeed-forward layers consume large memory\u003c/strong\u003e, since the intermediate dimension $d_{ff}$ is much larger than the model dimension;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-attention has quadratic time and memory complexity $O(L^2)$\u003c/strong\u003e in sequence length, making long sequences (e.g., 64K tokens) impractical even with memory-efficient implementations.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAs a result, large Transformers often \u003cstrong\u003ecannot be trained or even fine-tuned on a single accelerator\u003c/strong\u003e, limiting scalability and accessibility. The key question posed by the paper is whether this limitation is fundamental‚Äîor merely due to architectural inefficiency.\u003c/p\u003e","title":"Reformer: The Efficient Transformer"},{"content":"Github: https://github.com/openvla/openvla\n1. Keywords End-to-end VLA polic Fully open-source (data, code, weights) 2. Problem OpenVLA is proposed to overcome three major limitations in existing robotic learning systems:\nthe lack of end-to-end VLA modeling, insufficiently large and diverse robot datasets, the reliance on closed-source models and data. 3. Method 3.1. Overall goal OpenVLA is a vision‚Äìlanguage‚Äìaction (VLA) foundation model that learns an end-to-end robot control policy, mapping visual observations and natural-language instructions directly to robot actions in a closed loop.\n3.2. Model architecture OpenVLA uses a VLM backbone extended for action prediction:\nVisual encoder Fuses pretrained features from DINOv2 (geometry \u0026amp; structure) SigLIP (vision‚Äìlanguage alignment) Language model backbone Llama-2 (7B) Receives visual tokens + language instruction tokens Outputs action tokens instead of text This forms a single transformer handling vision, language, and action.\n3.3. Action representation (key design choice) Robot actions are continuous. They are discretized per dimension into 256 bins. Bin ranges are defined using the 1st‚Äì99th percentiles of training data ‚Üí ignores outliers, preserves precision.\nEach bin is mapped to a token in the LLM vocabulary, which is implemented by overwriting rarely used tokenizer tokens\nResult:\ncontinuous action ‚Üí discrete action tokens ‚Üí LLM prediction ‚Üí decoded back to discrete action value.\n3.4. Training 3.4.1. Training Objective Trained using standard next-token prediction Loss: cross-entropy Evaluated only on action tokens No custom control head or regression loss This allows reuse of pretrained LLM weights without architectural changes.\n3.4.2. Training data Trained on ~970k robot episodes Includes: diverse tasks multiple robots multiple environments Episodes are full trajectories (closed-loop interaction) 3.4.3. Inference / deployment At each timestep:\nObserve camera images Read language instruction Predict next action tokens Decode to continuous control Execute on robot Repeat (closed-loop control) One-sentence takeaway OpenVLA is an open, end-to-end vision‚Äìlanguage‚Äìaction transformer that discretizes robot actions into tokens, enabling a pretrained LLM to directly perform closed-loop robot control at scale.\n","permalink":"/notes/openvla_an_open-source_vision-language-action_mode/","summary":"\u003cp\u003eGithub: \u003ca href=\"https://github.com/openvla/openvla\"\u003ehttps://github.com/openvla/openvla\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"1-keywords\"\u003e1. Keywords\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eEnd-to-end VLA polic\u003c/li\u003e\n\u003cli\u003eFully open-source (data, code, weights)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"2-problem\"\u003e2. Problem\u003c/h1\u003e\n\u003cp\u003eOpenVLA is proposed to overcome three major limitations in existing robotic learning systems:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe lack of end-to-end VLA modeling,\u003c/li\u003e\n\u003cli\u003einsufficiently large and diverse robot datasets,\u003c/li\u003e\n\u003cli\u003ethe reliance on closed-source models and data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"3-method\"\u003e3. Method\u003c/h1\u003e\n\u003ch2 id=\"31-overall-goal\"\u003e3.1. Overall goal\u003c/h2\u003e\n\u003cp\u003eOpenVLA is a \u003cstrong\u003evision‚Äìlanguage‚Äìaction (VLA) foundation model\u003c/strong\u003e that learns an \u003cstrong\u003eend-to-end robot control policy\u003c/strong\u003e, mapping visual observations and natural-language instructions \u003cstrong\u003edirectly to robot actions\u003c/strong\u003e in a closed loop.\u003c/p\u003e","title":"OpenVLA: An Open-Source Vision-Language-Action Model"},{"content":"‚ÅâÔ∏è Why Bayesian Optimization Finally Beat Random Search A Beginner-Friendly Review of the NeurIPS 2020 Black-Box Optimization Challenge.\nHyperparameter tuning sounds boring ‚Äî but it quietly determines the final performance of almost every machine learning model.\nYet most researchers still rely on manual tuning, grid search, or random search.\nIn 2020, a large international competition decided to answer a simple but important question:\nIs Bayesian Optimization really better than Random Search for hyperparameter tuning?\nThe results?\nA decisive yes ‚Äî and even more interesting insights about how top teams achieved massive improvements.\nThis article gives you a simple, intuitive overview of the entire paper and competition.\nüåü 1. Background: Why This Competition Matters Hyperparameter tuning is a black-box optimization problem:\nYou adjust parameters ‚Üí train a model ‚Üí observe the score ‚Üí repeat.\nYou don‚Äôt know the shape of the loss surface or its derivatives.\nThe ML community has long believed Bayesian Optimization (BO) should outperform random search ‚Äî but large-scale, ML-focused benchmarks were missing.\nThe NeurIPS 2020 challenge filled this gap:\n65 teams participated Hyperparameter tuning tasks came from real scikit-learn models and real datasets All evaluations were done in a secure Docker environment Final scores were based on hidden test problems to prevent overfitting The goal was simple:\nFind the most effective black-box optimizer for ML hyperparameters.\nüîß 2. How the Competition Worked The organizers built a ‚Äúdataset of optimization tasks‚Äù:\nDifferent ML models Different datasets Different evaluation metrics This created dozens of unique problems such as:\nTune GBDT on MNIST (accuracy) Tune logistic regression (log loss) Tune MLP on Boston housing (RMSE) A summary of the different model, loss, and data set combinations that made up the different phases.\nParticipants submitted an optimizer, not hyperparameters.\nTheir optimizer could:\nSuggest() k hyperparameter candidates Observe() the returned scores from the benchmark Each submission had:\n16 rounds Batch size = 8 evaluations per round Total = 128 evaluations per problem 640 seconds runtime limit per problem All practice problems were public, but feedback and final problems were completely hidden.\nThis design ensured:\nFairness No leaking of datasets No manual tuning on test problems üìä 3. How Scores Were Calculated (Super Simple Version) Scoring used the Bayesmark system:\nRandom Search average ‚Üí normalized score = 1 Best possible performance ‚Üí normalized score = 0 Then scores were transformed to a final leaderboard value:\nü•á Score = 100 √ó (1 ‚Äì normalized_mean_performance)\nSo:\n100 = always finds best hyperparameters 0 = no better than random search This created a clean, unitless, intuitive 0‚Äì100 scale.\nüèÜ 4. What Actually Worked? Key Insights from the Top Teams Insight 1: Bayesian Optimization dominates Out of 65 teams:\n61 beat random search Almost all top submissions used surrogate models + acquisition functions The best solutions achieved 100√ó sample efficiency vs random search Insight 2: Trust-region BO (TuRBO) is incredibly strong TuRBO (a local BO method) was the strongest baseline and appeared in 6 of the top 10 solutions.\nThis suggests:\nIn hyperparameter tuning, the landscape is often locally structured, so local models work well.\nInsight 3: Ensembles win ‚Äî even simple ones Every top-10 team used some form of ensemble.\nThis was the biggest surprise.\nExamples:\nNVIDIA combined TuRBO + Scikit-Optimize (50). Duxiaoman combined TuRBO + pySOT. AutoML.org used a more complex combination with differential evolution in later rounds. These ensembles consistently outperformed their components, especially avoiding failure cases where one method gets stuck.\nInsight 4: Handling categorical/discrete integer variables matters Most BO literature focuses on continuous parameters, but ML models often include:\nnumber of layers max_depth activation choices categorical losses Some teams modified TuRBO or used bandit-style strategies to better treat these.\nThis gave additional performance boosts.\nInsight 5: Meta-learning \u0026amp; Warm Starting can skyrocket performance During the feedback phase, teams noticed patterns:\n‚ÄúSimilar models like similar hyperparameters.‚Äù Some teams used meta-learning:\nUse good hyperparameters from similar past problems Warm-start the optimizer near plausible good regions When parameter names were revealed in a controlled ‚Äúwarm start experiment,‚Äù\nAutoML.org jumped to 1st place with huge gains.\nüéØ 5. Main Takeaways for Practitioners 1. Always prefer BO over random search The competition provided the clearest proof so far.\nEven simple BO implementations gave orders of magnitude better results.\n2. If you don‚Äôt know what to use ‚Üí start with TuRBO It performed well out-of-the-box across all tasks.\n3. Ensembling is a cheat code Even a basic 50/50 ensemble of two optimizers can dramatically improve stability and performance.\n4. Don‚Äôt ignore categorical parameters A small adjustment to treat them properly can make your optimizer more robust.\n5. Warm-start when you can If you repeatedly solve similar ML tasks, reuse previous experience.\nüîÆ 6. Future Directions The authors highlight several exciting extensions:\nmulti-fidelity optimization (early stopping, partial data) asynchronous parallel BO adding constraints or multi-objective settings giving partial model information to optimize smarter üßæ 7. Conclusion The NeurIPS 2020 Black-Box Optimization Challenge delivered a clear message:\nBayesian Optimization is not only better than random search ‚Äî it‚Äôs much better.\nWith simple ensembles and trust-region methods, teams achieved more than 100√ó speedups in sample efficiency.\nThis competition set a new benchmark and provided practical insights that anyone doing ML hyperparameter tuning can benefit from.\n","permalink":"/notes/bayesian_optimization_is_superior_to_random_search/","summary":"\u003ch1 id=\"-why-bayesian-optimization-finally-beat-random-search\"\u003e‚ÅâÔ∏è Why Bayesian Optimization Finally Beat Random Search\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eA Beginner-Friendly Review of the NeurIPS 2020 Black-Box Optimization Challenge.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eHyperparameter tuning sounds boring ‚Äî but it quietly determines the final performance of almost every machine learning model.\u003c/p\u003e\n\u003cp\u003eYet most researchers still rely on manual tuning, grid search, or random search.\u003c/p\u003e\n\u003cp\u003eIn 2020, a large international competition decided to answer a simple but important question:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIs Bayesian Optimization really better than Random Search for hyperparameter tuning?\u003c/p\u003e","title":"Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning"},{"content":"‚Åâ¬†Why Is Hyperparameter Tuning So Difficult? Bengio‚Äôs Classic Paper Tells You: Stop Using Grid Search! In machine learning, there‚Äôs a saying:\n‚ÄúTraining the model is easy. Tuning hyperparameters is hell.‚Äù\nLearning rate, number of layers, regularization strength, hidden units‚Ä¶\nEverything seems important, but tuning them feels like opening blind boxes.\nToday we look at a classic paper from Yoshua Bengio‚Äôs team:\n‚ÄúRandom Search for Hyper-Parameter Optimization.‚Äù\nIts key message is surprisingly simple:\nRandom Search is far more efficient than Grid Search.\nIt‚Äôs cheaper, faster, and works especially well in high-dimensional spaces.\nLet‚Äôs walk through the core ideas of this paper in a beginner-friendly way.\n‚ùì1. Why Is Traditional Grid Search Inefficient? The goal of hyperparameter optimization is simple:\nFind a set of hyperparameters Œª that gives the best generalization performance.\nThe usual routine:\nTry many Œª combinations Train models Evaluate on validation data Pick the best But here‚Äôs the problem: Grid Search does this:\nPick several values for each hyperparameter Try every combination As the number of hyperparameters grows, the number of combinations grows exponentially.\nüìå The paper notes:\nGrid Search suffers badly from the curse of dimensionality.\nEven worse:\n‚ùó Most hyperparameters don‚Äôt matter very much..\nAs shown later in the paper:\nFor a given dataset, only 1‚Äì3 hyperparameters significantly affect performance Many others barely change anything But Grid Search still allocates equal effort across all dimensions In simple words:\nGrid Search wastes massive compute exploring unimportant directions.\nü§ì¬†2. Key Conclusion of the Paper: Random Search Is Better The authors make a bold claim:\nRandom Search is more efficient and often finds better models.\nAnd the reasoning is intuitive:\n2.1. Random Search focuses more effectively on ‚Äúimportant dimensions‚Äù If only 1‚Äì2 hyperparameters matter:\nGrid Search: spreads effort unnecessarily across all dimensions Random Search: each sample covers all dimensions, increasing the chance of hitting useful values The following figure illustrates:\nIn a 2D function where only x matters, Grid Search tests only a few distinct x-values, Random Search can test many more unique x-values with the same budget. 2.2. Random samples are independent (i.i.d.) This gives several engineering advantages:\nYou can stop early You can add more trials anytime Trials naturally run in parallel, asynchronously Failed runs don‚Äôt affect anything It‚Äôs simple, robust, and flexible.\nüî¨ 3. Experiments Show: Random Search Finds Better Models Faster The paper reproduces classic deep learning experiments on datasets like:\nMNIST variants rectangles convex plus experiments with DBNs involving 32 hyperparameters üî• The results are striking: ‚úî With the same compute budget: Random Search needs only:\n8 trials to match Grid Search‚Äôs performance at 100 trials 32 trials to outperform Grid Search ‚úî In high-dimensional DBN tuning: Random Search performs:\nBetter on 1 dataset Comparable on 4 datasets Slightly worse on 3 datasets In short:\nRandom Search is cheaper, faster, and still high-quality.\nüß† 4. A Hidden Highlight: How to Fairly Evaluate the ‚ÄúBest Model‚Äù from Hyperparameter Tuning? Because the validation set is finite:\nThe hyperparameter with the best validation score might not truly be the best.\nIt may just have been ‚Äúlucky‚Äù on this particular validation split.\nThe paper proposes:\n‚úî Don‚Äôt report the test performance of only the single best Œª. Instead:\n‚úî Report a weighted average of test performances across all Œª candidates. The weight comes from:\nThe probability that each Œª would be the winner\n(considering validation noise and variance)\nThis gives a more robust, unbiased, and fair estimate of generalization.\nüìà 5. Why You Probably Should Stop Using Grid Search The conclusion states clearly:\nGrid Search wastes computation Random Search is more efficient in high-dimensional spaces Random trials are easy to parallelize Random Search should be the standard baseline for evaluating new tuning algorithms The implicit message:\nUnless you have only 1‚Äì2 hyperparameters, avoid Grid Search.\n‚≠ê 6. Practical Takeaways: Start Using Random Search for Hyperparameter Tuning! One-sentence summary of the paper:\nGrid Search is inefficient; Random Search is faster and finds better results.\nWhy Random Search works better:\nReal models have low effective dimensionality. Grid Search wastes effort in irrelevant dimensions. Random Search offers wider, more meaningful exploration. It‚Äôs parallel-friendly and simple to implement. A practical tip:\nIf you have more than 2 hyperparameters, Grid Search is almost always a bad idea.\nüìå 7. Practical Advice for ML Engineers \u0026amp; Students ‚úî Prefer Random Search over Grid Search (even for neural networks and classical ML)\n‚úî If budget permits Upgrade to Bayesian Optimization\n‚úî When tuning many hyperparameters (\u0026gt;20) Use:\nRandom Search + Early Stopping\nIt often delivers surprisingly strong results.\nüèÅ Final Golden Sentence Random Search isn‚Äôt ‚Äútrying things randomly‚Äù; it‚Äôs statistically one of the smartest ways to tune models.\n","permalink":"/notes/random_search_for_hyper-parameter_optimization/","summary":"\u003ch1 id=\"why-is-hyperparameter-tuning-so-difficult\"\u003e‚Åâ¬†Why Is Hyperparameter Tuning So Difficult?\u003c/h1\u003e\n\u003ch3 id=\"bengios-classic-paper-tells-you-stop-using-grid-search\"\u003eBengio‚Äôs Classic Paper Tells You: \u003cstrong\u003eStop Using Grid Search!\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eIn machine learning, there‚Äôs a saying:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e‚ÄúTraining the model is easy. Tuning hyperparameters is hell.‚Äù\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLearning rate, number of layers, regularization strength, hidden units‚Ä¶\u003c/p\u003e\n\u003cp\u003eEverything seems important, but tuning them feels like opening blind boxes.\u003c/p\u003e\n\u003cp\u003eToday we look at a classic paper from Yoshua Bengio‚Äôs team:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e‚ÄúRandom Search for Hyper-Parameter Optimization.‚Äù\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIts key message is surprisingly simple:\u003c/p\u003e","title":"Random Search for Hyper-Parameter Optimization"},{"content":"ALTA is a new programming language and a compiler that can map ALTA programs to Transformer weights.\nIt can help clearly analyze what algorithms Transformers can represent, why sometimes fail to learn them, and how to design models that generalize compositionally.\n1. Proposed Framework 1.1. Overview An ALTA program specification includes three key components:\na set of variables, a set of attention heads, a ‚ÄúMLP function‚Äù To mirror Transformer computation.\nAn ALTA Program Example - parity\nA Transformer can implement this parity algorithm because:\nResidual stream = variables stored inside the hidden vector\n(‚Äústore parity and done as part of the hidden state‚Äù)\nAttention = lookup from other tokens\n(‚Äúgive me the value from the previous token‚Äù)\nFFN/MLP = local computation\n(‚Äúcompute new parity from old parity‚Äù)\nThe ALTA framework includes:\nan interpreter symbolically executes a program, a compiler compiles programs to Transformer weights. Symbolic representation:\nan ALTA program: P input: a sequence of integers X output: a sequence of integers Y Interpreter ‚Üí func I: P x X ‚Üí Y\nCompiler ‚Üí func C: Œ∏ = C(P)\nT(x, Œ∏) ‚âà I(P, x)\nT: a transformer encoder 1.2. Variables There are three kinds of variables ALTA supports:\nCategorical variables Value is from a small finite set (e.g., {0,1}, {A,B,C}) Represented as one-hot vectors inside the Transformer Numerical variables Real-number values Represented as a single neuron (one scalar dimension) Set variables Values are sets of integers (e.g., {1, 4, 7}) Represented as multi-hot vectors Attention-head outputs as variables Some variables come from attention heads These can take a null / undefined value (e.g., when a head attends to no token) One residual block\nThe residual stream is the single vector per token that all sublayers read and write.\n1.3. Execution Attention affects inter-token variables:\nparity_left, done_left Any variable whose value comes from other tokens MLP affects intra-token variables:\nparity, done Other algorithmic state variables ALTA represents each token‚Äôs state with symbolic variables, which correspond to segments of the Transformer‚Äôs residual vector.\nFor layer k, token i:\nz^k_{\u0026lt;i, idx\u0026gt;} z^k_{\u0026lt;i, idx_left\u0026gt;} z^k_{\u0026lt;i, parity\u0026gt;} z^k_{\u0026lt;i, done\u0026gt;} \u0026hellip;\nInitialization: Input and position-dependent variables are encoded directly into the initial residual stream using embeddings.\nresidual[i] = [ idx_onehot(0..2) | idx_left_onehot(0..2) | parity | done ]\nPer-layer execution Self-attention:\nselect(query, key) builds a binary attention pattern based on variable equality. aggregate collects the corresponding value variables. This produces inter-token variables like parity_left. MLP:\nDefined by symbolic transition rules. Compiler turns these into a fixed 4-layer ReLU network that updates variables (e.g., parity, done). Layer1 ‚Üí ReLU ‚Üí Layer2 ‚Üí ReLU ‚Üí Layer3 ‚Üí ReLU ‚Üí Layer4 ‚Üí ReLU\nThe first 2 layers are only responsible for converting numerical/set variables into one-hot bucket representations.; The final 2 layers are based on the set of transition rules. Residual updates: Attention and MLP outputs are added to the residual stream, as in standard Transformers.\nHalting \u0026amp; output: Execution stops when a halting condition is met; the specified output variable is read from the final residual stream.\n2. Expressibility and Learnability 2.1. Expressibility ALTA shows constructively that Transformers can exactly implement algorithms like parity, addition, and SCAN.\n2.2. Learnability Even if an algorithm is expressible by a Transformer, training on input‚Äìoutput pairs may fail to learn it.\nALTA provides two tools to analyze and improve learnability:\nTrace supervision: Use ALTA‚Äôs intermediate states as supervision to help training follow the intended algorithm. Minimality analysis: A program must be minimal on the training set for the compiled Transformer weights to be a stable solution; non-minimal programs lead to unstable or incorrect learning. 3. ALTA Summary Six sentences to summarize ALTA:\nALTA is a formal programming language that describes the computation inside a transformer in a human-interpretable way. It converts opaque neural operations into clear algorithmic steps. ALTA includes a compiler that translates an ALTA program into actual transformer weights (embeddings, attention, FFN). ALTA programs use explicit symbolic variables (e.g., idx, parity, done, parity_left) to track interpretable token-level states throughout the computation. Attention in ALTA implements inter-token communication, enabling a token to read variables from other positions (e.g., computing left-side context like parity_left). FFNs implement intra-token updates, modifying the variables of each token based only on its own state (e.g., updating parity or flags). Because attention moves information only one step per layer, n tokens often require O(n)(n-1) layers for full information propagation. ","permalink":"/notes/alta_compiler-based_analysis_of_transformers/","summary":"\u003cp\u003eALTA is a new programming language and a compiler that can map ALTA programs to Transformer weights.\u003c/p\u003e\n\u003cp\u003eIt can help clearly analyze what algorithms Transformers can represent, why sometimes fail to learn them, and how to design models that generalize compositionally.\u003c/p\u003e\n\u003ch1 id=\"1-proposed-framework\"\u003e1. Proposed Framework\u003c/h1\u003e\n\u003ch2 id=\"11-overview\"\u003e1.1. Overview\u003c/h2\u003e\n\u003caside\u003e\n\u003cp\u003eAn ALTA program specification includes three key components:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea set of \u003cstrong\u003evariables\u003c/strong\u003e,\u003c/li\u003e\n\u003cli\u003ea set of \u003cstrong\u003eattention heads\u003c/strong\u003e,\u003c/li\u003e\n\u003cli\u003ea ‚Äú\u003cstrong\u003eMLP\u003c/strong\u003e function‚Äù\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo mirror Transformer computation.\u003c/p\u003e","title":"ALTA: Compiler-Based Analysis of Transformers"},{"content":"Laboratory for Interpretability\nTracr translates RASP programs to transformer weights in six steps:\nSplit the RASP program into small steps (a). Figure out what each step can output (a). Label each step as MLP or Attention (b). Arrange them into Transformer layers (c). Insert no-op blocks to fill empty spots (c). Generate real Transformer weights that implement each step. ","permalink":"/notes/tracr_compiled_transformers_as_a_laboratory_for_in/","summary":"\u003cp\u003eLaboratory for Interpretability\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTracr\u003c/strong\u003e translates RASP programs to transformer weights in six steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eSplit\u003c/strong\u003e the RASP program into small steps (a).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFigure out\u003c/strong\u003e what each step can output (a).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLabel\u003c/strong\u003e each step as MLP or Attention (b).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArrange\u003c/strong\u003e them into Transformer layers (c).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInsert\u003c/strong\u003e no-op blocks to fill empty spots (c).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGenerate\u003c/strong\u003e real Transformer weights that implement each step.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"/notes/tracr_compiled_transformers_as_a_laboratory_for_in/2c340e9a-8266-4e2c-9683-00b701af7b7b.png\"\u003e\u003c/p\u003e","title":"Tracr: Compiled Transformers as a Laboratory for Interpretability"},{"content":"1. What is RASP? A small, symbolic language that models how Transformers compute.\nDescribes sequence operations using simple functions instead of neural weights. 2. Core Components 2.1 s-ops (sequence operators) Functions that take a sequence and return another sequence of the same length.\nExamples:\ntokens ‚Üí original input\nindices ‚Üí [0,1,2,\u0026hellip;]\nlength ‚Üí broadcast length\nelementwise ops: +, ==, %, conditionals\n‚Üí These mimic MLP/FFN behavior.\n2.2 select (symbolic attention) select(q, k, predicate)\nProduces an n√ón boolean matrix ‚Üí attention pattern. Tells which positions ‚Äúattend‚Äù to which. 2.3 aggregate (value combination) aggregate(selector, values)\nFor each position: gather values from selected positions and average them. Symbolic version of attention-value combination. 2.4 selector_width Counts how many positions were selected for each token. Used for counting / histogram tasks. 3. What RASP Can Express RASP programs can represent many Transformer-computable tasks:\nHistogram Double histogram Sequence reversal Matching parentheses Dyck languages Filtering, counting, boolean logic over sequences Shows that Transformers can perform structured, compositional operations.\n4. How RASP Models Transformers Elementwise s-ops ‚Üí Transformer MLP select + aggregate ‚Üí Self-attention No loops ‚Üí fixed-depth computation, like real Transformers Models information flow limits: only attention can move information across tokens. 5. Examples Reverse sequence flip = select(indices, length - indices - 1, ==) reverse = aggregate(flip, tokens)\nHistogram same = select(tokens, tokens, ==) hist = selector_width(same)\n6. Compilation RASP programs can be compiled into real Transformer architectures:\nnumber of layers number of heads attention patterns Enables empirical testing of symbolic algorithms.\n7. Key Insights RASP provides a clean, understandable model of Transformer computation. It highlights what Transformers can and cannot compute. It acts like pseudocode for attention-based algorithms.\n","permalink":"/notes/thinking_like_transformers/","summary":"\u003ch2 id=\"1-what-is-rasp\"\u003e\u003cstrong\u003e1. What is RASP?\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eA small, symbolic language that models how Transformers compute.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribes \u003cstrong\u003esequence operations\u003c/strong\u003e using simple functions instead of neural weights.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"2-core-components\"\u003e\u003cstrong\u003e2. Core Components\u003c/strong\u003e\u003c/h2\u003e\n\u003ch3 id=\"21-s-ops-sequence-operators\"\u003e\u003cstrong\u003e2.1 s-ops (sequence operators)\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eFunctions that take a sequence and return another sequence of the same length.\u003c/p\u003e\n\u003cp\u003eExamples:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003etokens\u003c/code\u003e ‚Üí original input\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eindices\u003c/code\u003e ‚Üí [0,1,2,\u0026hellip;]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003elength\u003c/code\u003e ‚Üí broadcast length\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eelementwise ops: +, ==, %, conditionals\u003c/p\u003e\n\u003cp\u003e‚Üí These mimic \u003cstrong\u003eMLP/FFN\u003c/strong\u003e behavior.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"22-select-symbolic-attention\"\u003e\u003cstrong\u003e2.2 select (symbolic attention)\u003c/strong\u003e\u003c/h3\u003e\n\u003caside\u003e\n\u003cp\u003eselect(q, k, predicate)\u003c/p\u003e","title":"Thinking Like Transformers"},{"content":"1. Introduction Modern sequence models (especially Transformers) achieve strong performance due to their ability to learn from long contexts at scale. However, Transformers suffer from quadratic complexity and linearly growing memory (KV cache), which limits long-context modeling. To overcome this, recent research develops efficient recurrent alternatives that compress information into fixed-size memory, focusing on:\nLearning rules (from Hebbian ‚Üí Delta ‚Üí new variants) Forget gates (from LSTM ‚Üí Mamba ‚Üí Titan gates) Memory architectures (vector memory in RetNet, LRU; deep memory in Titans and TTT) These advances raise a central question:\nWhat is the unified design framework behind all these sequence models, and how can we extend it?\nThe authors reinterpret Transformers, Titans, and modern linear RNNs as associative memory systems, guided by a new concept called attentional bias‚Äîthe internal objective that determines how models learn mappings between keys and values. Surprisingly, they observe that almost all existing models use only two types of attentional bias: dot-product similarity or ‚Ñì‚ÇÇ regression.\nBased on this insight, they reinterpret forgetting mechanisms as retention ‚Ñì‚ÇÇ-regularization, then introduce Miras, a general design framework defined by four choices:\nAttentional bias (memory objective) Retention gate Memory architecture Memory learning algorithm (optimizer) Using Miras, they create three new sequence models‚ÄîMoneta, Yaad, and Memora‚Äîthat incorporate new attentional biases and robust forgetting mechanisms. Experiments show that these models outperform current architectures in language modeling, reasoning, and memory-intensive tasks.\n2. Method 2.1. Associative Memory and Attentional Bias Associative memory maps keys (K) to values (V). The mapping is learned by optimizing an objective $\\mathcal{L}$ called attentional bias.\nFormally, memory $\\mathcal{M}$ is learned by:\n$$ \\mathcal{M}^* = \\arg\\min_{\\mathcal{M}} \\mathcal{L}(\\mathcal{M}(K); V). $$\nRemark 1 When memory is parameterized by a matrix $W$, we optimize $W$, not $\\mathcal{M}$. We may also add regularization $R(W)$ to retain past memory. Remark 2 Learning keys‚Äìvalues is a meta-learning problem: inner-loop optimizes memory; outer-loop optimizes the rest of the network. Remark 3 Forgetting is not explicit erasing; rather, the model may fail to retrieve past memory. Therefore they use the term ‚ÄúRetention Gate‚Äù, not ‚ÄúForget Gate‚Äù. Remark 4 Most modern sequence models optimize the associative memory objective (attentional bias) via gradient descent. The theory applies beyond GD, any optimization method can be used. 2.1.1. Learning to Memorize and Retain (Optimization View) Memory is updated by gradient descent:\n$$ W_t = W_{t-1} - \\eta_t \\nabla \\ell(W_{t-1}; k_t, v_t), $$\nwhere $\\ell$ is the attentional bias applied to the latest pair.\n2.1.2. Viewpoint 1: Online Regression and Follow-The-Regularized-Leader Gradient descent can be interpreted as minimizing a sequence of losses:\n$$ \\ell(W; k_1, v_1), \\ell(W; k_2, v_2), \\ldots $$\nEquivalent formulation:\n$$ W_t = \\arg\\min_W \\sum_{i=1}^t \\langle W - W_{t-1}, \\nabla \\ell(W_{t-1}; k_i, v_i) \\rangle + \\frac{1}{2\\eta_t}|W|^2.\n$$\nThe first term measures how well memory fits new data;\nthe second term is a regularizer that stabilizes memory size.\nGeneral FTRL form:\n$$ W_t = \\arg\\min_{W \\in \\mathcal{W}} \\left(\\sum_{i=1}^t \\tilde{\\ell}_i(W; k_i, v_i)\\right) + \\frac{1}{\\eta_t} R_t(W).\n$$\nHere:\n$\\tilde{\\ell}_i$ = approximated attentional bias $R_t(W)$ = memory stability regularizer 2.1.3. Viewpoint 2: Learning the Latest Token While Retaining Previous Memory Another interpretation decomposes memory update into: Learning new info + Retaining old memory.\nEquivalent update:\n$$ W_t = \\arg\\min_W \\Big( \\langle W - W_{t-1}, \\nabla \\ell(W_{t-1}; k_t, v_t)\\rangle + \\frac{1}{2\\eta_t} |W - W_{t-1}|^2 \\Big).\n$$\nThe form generalizes to:\n$$ W_t = \\arg\\min_{W \\in \\mathcal{W}} \\Big( \\tilde{\\ell}_t(W; k_t, v_t) + \\text{Ret}_t(W, W{t-1}) \\Big). $$\nAttentional Bias: $\\tilde{\\ell}_t(W; k_t, v_t)$ ‚Üí learns new key‚Äìvalue mapping. Retention: $\\text{Ret}_t(W, W{t-1})$ ‚Üí encourages memory to stay close to its previous state. Retention has local and global components:\n$$ \\text{Ret}t(W, W{t-1}) = \\frac{1}{\\eta_t} D_t(W, W_{t-1}) + \\frac{1}{\\alpha_t} G_t(W).\n$$\n$D_t$: local retention ‚Üí prevents forgetting $G_t$: global retention ‚Üí controls memory magnitude 2.1.4. Connection Between the Two Viewpoints Both viewpoints describe the same process using online optimization concepts. The two formulations are equivalent under mild assumptions.\nThe FTRL viewpoint emphasizes loss over time + regularization. The Learning‚ÄìRetaining viewpoint emphasizes new learning + memory retention. 2.2. MIRAS MIRAS says every sequence model is defined by 4 choices:\nMemory Structure\nWhat the memory looks like vector, matrix, MLP.\nAttentional Bias\nThe loss used to learn key‚Üívalue mapping like dot-product, $\\ell_2$, $\\ell_p$, Huber, KL. ‚Üí loss function\nRetention Gate\nControls how much old memory is kept. Like: $|W - W_{t-1}|^2$, KL divergence, elastic net, etc.\nMemory Algorithm\nHow memory is updated (GD, momentum, Newton, etc.). ‚Üí optimizer\nAttentional Bias = write new info.\nRetention Gate = keep old info.\nMemory Learning Algorithm = the formula that mixes them into the final memory update.\nAll existing models fit this form.\nExamples:\nHebbian RNNs (RetNet, LA) $$ M_t = \\alpha M_{t-1} + v_t k_t^\\top $$\nDelta rule models (DeltaNet, RWKV)\nThey optimize MSE: $|M(k_t) - v_t|^2$.\nTitans / TTT\nUse deep memory + gradient descent with retention.\n2.3. Architecture Backbone and Fast Training Architectural Backbone for Miras‚Äôs Variants: Moneta, Yaad, and Memora Replace the attention block with a MIRAS block inside a Llama-style model. Use modern components: SwiGLU MLPs, RoPE, RMSNorm, depthwise conv, and ‚Ñì‚ÇÇ-normed q/k. Channel-wise Parameters Parameters like $\\eta_t, \\delta_t, \\alpha_t$ are learned per channel. To reduce cost, apply low-rank projections. Hybrid Models MIRAS layers can be combined with Sliding Window Attention (SWA). Parallel Training Recurrence is broken using chunking: split the sequence into chunks and compute gradients per chunk. This makes training fast and parallelizable. Core recurrence idea Inside a chunk, replace:\n$$ M_t = \\alpha_t M_{t-1} - \\eta_t \\nabla \\ell $$\nwith a parallel form:\n$$ M_t = \\beta_t M_0 - \\sum_{i=1}^t \\frac{\\beta_t}{\\beta_i}\\eta_i\\nabla\\ell(M_0;k_i,v_i) $$\nso no step-by-step recurrence is needed.\n3. Comparison ","permalink":"/notes/its_all_connected_a_journey_through_test-time_mem/","summary":"\u003ch1 id=\"1-introduction\"\u003e1. Introduction\u003c/h1\u003e\n\u003cp\u003eModern sequence models (especially Transformers) achieve strong performance due to their ability to learn from long contexts at scale. However, Transformers suffer from \u003cstrong\u003equadratic complexity\u003c/strong\u003e and \u003cstrong\u003elinearly growing memory (KV cache)\u003c/strong\u003e, which limits long-context modeling. To overcome this, recent research develops \u003cstrong\u003eefficient recurrent alternatives\u003c/strong\u003e that compress information into \u003cstrong\u003efixed-size memory\u003c/strong\u003e, focusing on:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eLearning rules\u003c/strong\u003e (from Hebbian ‚Üí Delta ‚Üí new variants)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eForget gates\u003c/strong\u003e (from LSTM ‚Üí Mamba ‚Üí Titan gates)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMemory architectures\u003c/strong\u003e (vector memory in RetNet, LRU; deep memory in Titans and TTT)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThese advances raise a central question:\u003c/p\u003e","title":"It‚Äôs All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization"},{"content":"1. Method 1.1. Replace self-attention with a 2D Fourier Transform FNet removes the entire self-attention sublayer from Transformer encoders. Instead, each layer performs a 2D Discrete Fourier Transform (DFT) on the input:\n$$ y = \\Re\\left(F_{\\text{seq}}(F_h(x))\\right) $$\n$F_{\\text{seq}}$: 1D DFT along the sequence length $F_h$: 1D DFT along the hidden dimension The discrete Fourier Transform (DFT) is defined by the formula: Thus, no Q/K/V, no dot-products, and no softmax are computed.\n1.2. Simple Transformer-style architecture Each encoder block contains:\nFourier mixing sublayer (parameter-free) Feed-forward network (FFN) Residual + LayerNorm (same as BERT) The model uses the same word/type/position embeddings as BERT, but position embeddings are technically unnecessary because the Fourier basis already encodes position.\nFNet architecture with N encoder blocks.\nFNet includes position embeddings only to make experiments directly comparable with BERT. The authors wanted to keep everything the same except replacing self-attention with Fourier Transform.\n1.3. Computational efficiency The FFT has complexity: $O(n \\log n d)$ because of FFT (Fast Fourier Transform).\nCompared to self-attention: $O(n^2 d)$\nThis yields major speedups:\n80% faster training on GPUs, 70% faster on TPUs, Much faster for long sequences (LRA benchmark). 2. Novelty 2.1. First model to fully replace attention with Fourier mixing Previous works used Fourier features to approximate attention (e.g., Performer).\nFNet is the first to:\nRemove self-attention entirely and use a fixed, unparameterized Fourier Transform as the token-mixing mechanism.\nThe mixing weights come purely from:\n$$ e^{-2\\pi i nk / N} $$\nand not from learned Q¬∑K projections.\n2.2. Demonstrates that structured linear mixing can rival attention A surprising empirical finding:\nFNet reaches 92‚Äì97% of BERT‚Äôs accuracy on GLUE Despite having zero learned parameters in its mixing layer This suggests:\nAttention is not always the main source of performance;\nhigh-quality token mixing + FFN may be sufficient for many NLP tasks.\n2.3. Superior long-sequence scalability On the Long-Range Arena (LRA) benchmark:\nFNet matches the accuracy of the strongest models But is faster and more memory-efficient than Performer, Linformer, and other efficient Transformers This shows a new path:\nInstead of approximating attention, one can replace it with a simpler mathematical transform.\n2.4. Extremely good small-model efficiency For smaller models, FNet and Linear mixing form the Pareto frontier for speed‚Äìaccuracy (Fig. 2).\nBecause Fourier mixing is parameter-free:\nSmaller memory footprint High stability during training Better deployment potential on edge devices 3. Why is DFT a mixing operation like attention? Because:\n$$ X_k = \\sum_{n=0}^{N-1} w_{kn} \\cdot x_n $$\nWhere:\n$$ w_{kn} = \\cos(2\\pi nk / N) $$\nThis is EXACTLY what attention does:\n$$ y_i = \\sum_j W_{ij} \\cdot x_j $$\nThe only difference:\nMechanism Weights ( W ) Attention Learned from Q¬∑K Fourier / DFT Fixed sine/cosine patterns Both compute:\nOutput token = weighted sum of all input tokens\nTherefore both are global mixing layers.\n4. Summary Table of Time Complexities ","permalink":"/notes/fnet_mixing_tokens_with_fourier_transforms/","summary":"\u003ch1 id=\"1-method\"\u003e\u003cstrong\u003e1. Method\u003c/strong\u003e\u003c/h1\u003e\n\u003ch3 id=\"11-replace-self-attention-with-a-2d-fourier-transform\"\u003e\u003cstrong\u003e1.1. Replace self-attention with a 2D Fourier Transform\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eFNet removes the entire self-attention sublayer from \u003cstrong\u003eTransformer encoders\u003c/strong\u003e. Instead, each layer performs a \u003cstrong\u003e2D Discrete Fourier Transform (DFT)\u003c/strong\u003e on the input:\u003c/p\u003e\n\u003cp\u003e$$\ny = \\Re\\left(F_{\\text{seq}}(F_h(x))\\right)\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$F_{\\text{seq}}$: 1D DFT along the \u003cstrong\u003esequence length\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e$F_h$: 1D DFT along the \u003cstrong\u003ehidden dimension\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003ediscrete Fourier Transform (DFT)\u003c/strong\u003e is defined by the formula:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"/notes/fnet_mixing_tokens_with_fourier_transforms/image.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThus, \u003cstrong\u003eno Q/K/V\u003c/strong\u003e, \u003cstrong\u003eno dot-products\u003c/strong\u003e, and \u003cstrong\u003eno softmax\u003c/strong\u003e are computed.\u003c/p\u003e","title":"FNet: Mixing Tokens with Fourier Transforms"},{"content":"1. Method 1.1. Key Observation The self-attention matrix is low-rank. Both empirical spectra and theory show that most information is concentrated in a few singular values.\n$$ P = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right) $$\n1.2. Main Idea If $P$ is low-rank, then instead of computing an $n\\times n$ matrix, we can approximate it using a factorization of size $n \\times k$) where $k \\ll n$.\n1.3. Linear Projections Linformer introduces two projection matrices $E, F \\in R^{n \\times k}$ to reduce the sequence length of keys and values:\n$$ K\u0026rsquo; = EK,\\qquad V\u0026rsquo; = FV. $$\n1.4. Linear Self-Attention Attention is computed as\n$$ \\text{Attention}(Q, K\u0026rsquo;, V\u0026rsquo;) = \\text{softmax}\\left(\\frac{Q {K\u0026rsquo;}^\\top}{\\sqrt{d}}\\right)V\u0026rsquo;. $$\nThis reduces complexity from $O(n^2)$ to $O(nk)$.\n1.5. Practical Choices k is small and independent of sequence length. Projections can be shared across heads and layers. 1.6. Empirical Results Linformer achieves similar or better accuracy than standard Transformers while being much faster and using much less memory.\n2. Novelty First to show self-attention is inherently low-rank.\nThis gives a theoretical reason why quadratic attention is unnecessary.\nIntroduces a simple and general low-rank factorization of attention.\nInstead of sparsity or hashing, Linformer directly compresses the sequence dimension.\nAchieves true linear complexity (O(n)) in both time and memory.\nProjection dimension does not grow with sequence length.\nThis enables extremely long-sequence training on standard hardware.\nCompatible with standard Transformer design.\nNo special sparsity patterns, no complex algorithms. Easy to drop in.\n","permalink":"/notes/linformer_self-attention_with_linear_complexity/","summary":"\u003ch1 id=\"1-method\"\u003e1. Method\u003c/h1\u003e\n\u003ch2 id=\"11-key-observation\"\u003e\u003cstrong\u003e1.1. Key Observation\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThe self-attention matrix is \u003cstrong\u003elow-rank\u003c/strong\u003e. Both empirical spectra and theory show that most information is concentrated in a few singular values.\u003c/p\u003e\n\u003cp\u003e$$\nP = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)\n$$\u003c/p\u003e\n\u003ch2 id=\"12-main-idea\"\u003e\u003cstrong\u003e1.2. Main Idea\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eIf $P$ is low-rank, then instead of computing an $n\\times n$ matrix, we can approximate it using a factorization of size $n \\times k$) where $k \\ll n$.\u003c/p\u003e\n\u003ch2 id=\"13-linear-projections\"\u003e\u003cstrong\u003e1.3. Linear Projections\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eLinformer introduces two projection matrices $E, F \\in R^{n \\times k}$ to reduce the sequence length of \u003cstrong\u003ekeys\u003c/strong\u003e and \u003cstrong\u003evalues\u003c/strong\u003e:\u003c/p\u003e","title":"Linformer: Self-Attention with Linear Complexity"},{"content":"1. Introduction Transformers are widely used in many areas, but their softmax attention requires quadratic time and memory, making them expensive for long sequences. Because of this limitation, many prior works propose ‚Äúefficient attention‚Äù by adding structural assumptions.\nCommon ideas include:\nlimiting attention to local neighbors, enforcing sparsity, using pooling or clustering (e.g., k-means), hashing similar tokens together, sliding windows, or using low-rank approximations of the attention matrix. These methods reduce cost but depend heavily on hand-designed priors such as sparsity patterns or low-rank structure. They often lack theoretical guarantees, and some approaches still fail to capture long-range interactions or introduce approximation bias.\nPerformers provide a new solution: they approximate full softmax attention accurately while using only linear time and memory. The key technique, FAVOR+ (Fast Attention Via positive Orthogonal Random features), approximates softmax and other kernels using random features with strong theoretical guarantees (unbiased or nearly-unbiased estimation and low variance).\nBecause FAVOR+ works for many kernelizable attention mechanisms, Performers make it feasible to compare different attention kernels on large-scale tasks, something that was previously too expensive with traditional quadratic attention.\nFinally, experiments show that Performers achieve competitive performance on tasks from pixel prediction to text and protein modeling, while remaining fully compatible with standard Transformer architectures.\n2. Method Performers reformulate softmax attention as a kernel function and then approximating this kernel using random feature maps so that attention can be computed without constructing the quadratic matrix.\nThe key observation is that the softmax kernel $\\exp(q^\\top k)$ can be written as an expectation over random features. Performer introduces Positive Random Features (PRF) that approximate $\\exp(q^\\top k)$ using mappings of the form\n$$ \\phi(x) = h(x) (\\exp(\\omega_1^\\top x),\\dots, \\exp(\\omega_m^\\top x)), $$\nwhere $h(x)$ is a stabilizing factor and $\\omega_i$ are sampled from an isotropic Gaussian distribution. Unlike classical Fourier features (cos), PRFs always produce positive, non-oscillatory values, yielding an unbiased approximation to the softmax kernel that remains stable even when dot products are small or negative. This allows the softmax kernel to be approximated by\n$$ \\exp(q^\\top k) \\approx \\phi(q)^\\top \\phi(k), $$\nso the attention can be computed as\n$$ \\widetilde{\\text{Att}}(Q,K,V) = D^{-1}\\big(Q\u0026rsquo;((K\u0026rsquo;)^\\top V)\\big), $$\nwhich requires only O(Lrd) operations and never forms an $L \\times L$ matrix.\nTo further reduce variance, Performer augments PRFs with Orthogonal Random Features (ORF). Here, the random vectors $\\omega_1,\\dots,\\omega_m$ are orthogonalized (e.g., by Gram‚ÄìSchmidt), significantly tightening concentration bounds and producing exponentially smaller variance compared to independently sampled features.\nThe combined PRF + ORF mechanism forms FAVOR+ (Fast Attention Via Orthogonal Random features), achieving high accuracy with relatively few random features $r \\ll L$. This enables Performers to match or surpass the accuracy of softmax attention while using linear time and memory.\n3. Novelty Performers introduce FAVOR+, a new linear-time attention mechanism that accurately approximates softmax using positive orthogonal random features.\nThis replaces the quadratic $(L^2)$ attention matrix with a linear $O(Lrd)$ computation while preserving accuracy.\nThe method is unbiased, low-variance, and fully compatible with standard Transformers, enabling fast and memory-efficient training on long sequences.\n","permalink":"/notes/rethinking_attention_with_performers/","summary":"\u003ch1 id=\"1-introduction\"\u003e1. Introduction\u003c/h1\u003e\n\u003cp\u003eTransformers are widely used in many areas, but their \u003cstrong\u003esoftmax attention\u003c/strong\u003e requires \u003cstrong\u003equadratic time and memory\u003c/strong\u003e, making them expensive for long sequences. Because of this limitation, many prior works propose ‚Äúefficient attention‚Äù by adding \u003cstrong\u003estructural assumptions\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eCommon ideas include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elimiting attention to local neighbors,\u003c/li\u003e\n\u003cli\u003eenforcing sparsity,\u003c/li\u003e\n\u003cli\u003eusing pooling or clustering (e.g., k-means),\u003c/li\u003e\n\u003cli\u003ehashing similar tokens together,\u003c/li\u003e\n\u003cli\u003esliding windows,\u003c/li\u003e\n\u003cli\u003eor using low-rank approximations of the attention matrix.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese methods reduce cost but depend heavily on \u003cstrong\u003ehand-designed priors\u003c/strong\u003e such as sparsity patterns or low-rank structure. They often lack theoretical guarantees, and some approaches still fail to capture long-range interactions or introduce approximation bias.\u003c/p\u003e","title":"Rethinking Attention with Performers"},{"content":"1. Motivation Traditional theoretical results on transformer ‚ÄúTuring-completeness‚Äù are not actually about language models. They:\nstudy language recognition, not probabilistic generation, and often add extra symbols to simulate a Turing machine. But a real LM = a probability distribution over strings of a fixed alphabet.\nThis paper provides a theory of expressivity for actual language models, especially when augmented with chain-of-thought (CoT) steps.\n2. Core Idea: Treat LMs as Probabilistic Models of Computation Instead of saying ‚Äúthis LM recognizes this language,‚Äù the paper asks: What distributions over strings can an LM represent? (probabilistic Turing-machine viewpoint)\nThis shift allows a proper comparison between:\nLMs without CoT LMs that generate extra intermediate steps (CoT) 3. Key Concept Introduced: Regular Reducibility CoT inserts extra reasoning tokens. To compare LMs with and without CoT, we need a way to ignore these internal steps in the output.\nRegular reducibility: LM (A) is reducible to LM (B) if:\nYou can convert samples from (A) into samples from (B) using only a finite-state transducer (very simple machine). Example: deleting or rewriting CoT reasoning steps.\n4. Main Results (1) CoT increases the power of RNN language models Constant-precision RNN LM (no CoT)\n‚Üí equivalent to deterministic probabilistic finite-state automata (PFSAs)\n‚Üí very weak\nConstant-precision RNN LM with CoT\n‚Üí equivalent to non-deterministic PFSAs\n‚Üí strictly more expressive\nSo CoT makes even weak RNN LMs more powerful.\n(2) Turing-complete RNNs can be viewed as doing implicit CoT Previous ‚ÄúTuring-complete RNN‚Äù constructions use complicated hidden-state motions.\nThe authors reinterpret these as implicit chain-of-thought reasoning inside the hidden state.\n(3) The Big Result: CoT makes LMs probabilistic-Turing-complete With enough numeric precision:\nLinear-precision RNN LMs + CoT, and Logarithmic-precision transformer decoder LMs + CoT, can simulate any probabilistic Turing machine.\nThus CoT raises LM generative capacity to the full power of probabilistic computation. This is the probabilistic analogue of Turing-completeness ‚Äî but now for true LMs, not for recognizers.\n5. Implications CoT adds computational steps, increasing representational power.\nThis gives a theoretical explanation for the empirical success of CoT prompting.\nLM expressivity depends heavily on precision and intermediate computation.\nTransformers without CoT (limited depth, finite precision) behave like small probabilistic automata (very weak).\nTransformers with CoT behave like full probabilistic Turing machines (maximally expressive).\n6. Takeaway Chain-of-thought turns a limited LM into a model as expressive as a probabilistic Turing machine, by giving it extra internal computation steps that fundamentally increase its generative power.\n7. Turing machine A simple abstract computer with an infinite tape, a read-write head, and a finite set of states. It follows a table of rules that tell it how to read/write symbols and move on the tape.\nIt can express any algorithm, which makes it the foundation of computability theory and the ‚Äúmaximal‚Äù model of expressivity.\nVideo source: https://www.bilibili.com/video/BV1br4y1N762/?spm_id_from=333.337.search-card.all.click\u0026amp;vd_source=650390d4a2decee4b694a632313a3cca\n","permalink":"/notes/on_the_representational_capacity_of_neural_languag/","summary":"\u003ch2 id=\"1-motivation\"\u003e\u003cstrong\u003e1. Motivation\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eTraditional theoretical results on transformer ‚ÄúTuring-completeness‚Äù are not actually about \u003cstrong\u003elanguage models\u003c/strong\u003e. They:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003estudy \u003cstrong\u003elanguage recognition\u003c/strong\u003e, not \u003cstrong\u003eprobabilistic generation\u003c/strong\u003e,\u003c/li\u003e\n\u003cli\u003eand often add \u003cstrong\u003eextra symbols\u003c/strong\u003e to simulate a Turing machine.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBut a real LM = a probability distribution over strings of a fixed alphabet.\u003c/p\u003e\n\u003cp\u003eThis paper provides a theory of expressivity \u003cstrong\u003efor actual language models\u003c/strong\u003e, especially when augmented with \u003cstrong\u003echain-of-thought (CoT)\u003c/strong\u003e steps.\u003c/p\u003e\n\u003ch2 id=\"2-core-idea-treat-lms-as-probabilistic-models-of-computation\"\u003e\u003cstrong\u003e2. Core Idea: Treat LMs as Probabilistic Models of Computation\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eInstead of saying ‚Äúthis LM recognizes this language,‚Äù the paper asks: What \u003cstrong\u003edistributions\u003c/strong\u003e over strings can an LM represent? (probabilistic Turing-machine viewpoint)\u003c/p\u003e","title":"On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning"},{"content":"1. Core Question Transformers work extremely well, but what can they theoretically express or compute? (Not about training, but about the architecture‚Äôs raw capability.)\nExpressivity is studied from two angles:\nApproximation theory: Can transformers approximate certain functions? Formal language theory (the focus): Can transformers recognize/generate certain formal languages? 2. Formal Background 2.1. Chomsky hierarchy Regular ‚Üí finite automaton Context-free ‚Üí pushdown automaton Context-sensitive ‚Üí bounded-tape Turing machine Recursively enumerable ‚Üí full Turing machine 2.2. Circuit hierarchy Transformers behave like constant-depth parallel circuits, so we compare with:\nAC‚Å∞ (very weak ‚Äî no PARITY) TC‚Å∞ (MAJORITY allowed) NC¬π (log-depth circuits, stronger) 2.3. DLOGTIME-uniform Means: the circuit for length n must be generable by a very simple, O(log n)-time algorithm.\nEquivalent to: no cheating, one systematic architecture, just like a real transformer.\n3. Key Positive Results Depending on assumptions:\nWith hard attention, special positional encodings, or extra intermediate steps\n‚Üí transformers can simulate arbitrary algorithms (Turing-complete).\nWith scratchpads / chain-of-thought\n‚Üí decoder-only LMs can match probabilistic Turing machines in expressivity.\nWith sinusoidal or arbitrary positional encodings\n‚Üí they can handle some context-free patterns (e.g., limited Dyck languages).\n4. Key Negative Results Under realistic constraints:\nconstant depth finite precision softmax attention no scratchpad / CoT transformers collapse to very weak circuit classes (AC‚Å∞ or TC‚Å∞).\nConsequences:\nThey cannot express PARITY on unbounded input length. They have trouble with even simple forms of counting or nesting (e.g., Dyck-k). They cannot recognize certain languages beyond TC‚Å∞. Interpretation:\nA shallow, finite-precision transformer cannot compute global, unbounded structure.\n5. Ramifications (Why this matters) Theory explains why LLMs struggle with long-range algorithmic tasks. Theory explains why chain-of-thought reliably boosts reasoning: extra generated steps dramatically increase theoretical power. It clarifies how sensitive expressivity is to assumptions like precision, depth, attention type, and positional encoding. 6. Takeaway A finite-precision transformer with fixed depth is as weak as a small parallel circuit (TC‚Å∞), but with hard attention or chain-of-thought steps, its expressivity can rise all the way to Turing-complete.\n","permalink":"/notes/what_formal_languages_can_transformers_express_a_s/","summary":"\u003ch1 id=\"1-core-question\"\u003e\u003cstrong\u003e1. Core Question\u003c/strong\u003e\u003c/h1\u003e\n\u003cp\u003eTransformers work extremely well, but \u003cstrong\u003ewhat can they theoretically express or compute?\u003c/strong\u003e (Not about training, but about the architecture‚Äôs raw capability.)\u003c/p\u003e\n\u003cp\u003eExpressivity is studied from two angles:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eApproximation theory:\u003c/strong\u003e Can transformers approximate certain functions?\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFormal language theory (the focus):\u003c/strong\u003e Can transformers recognize/generate certain \u003cstrong\u003eformal languages\u003c/strong\u003e?\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"2-formal-background\"\u003e\u003cstrong\u003e2. Formal Background\u003c/strong\u003e\u003c/h1\u003e\n\u003ch3 id=\"21-chomsky-hierarchy\"\u003e\u003cstrong\u003e2.1. Chomsky hierarchy\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRegular\u003c/strong\u003e ‚Üí finite automaton\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eContext-free\u003c/strong\u003e ‚Üí pushdown automaton\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eContext-sensitive\u003c/strong\u003e ‚Üí bounded-tape Turing machine\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRecursively enumerable\u003c/strong\u003e ‚Üí full Turing machine\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"22-circuit-hierarchy\"\u003e\u003cstrong\u003e2.2. Circuit hierarchy\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eTransformers behave like constant-depth parallel circuits, so we compare with:\u003c/p\u003e","title":"What Formal Languages Can Transformers Express? A Survey"},{"content":" Titans = learn to memorize token at test time,\nATLAS = learn to memorize context at test time, with more capacity, better objectives, and stronger optimizers.\n1. Introduction ATLAS addresses a fundamental limitation of modern recurrent memory models (e.g., Titans): they update memory token by token, using only the current key‚Äìvalue pair, which limits:\nMemory capacity ‚Äì only O(d) independent pairs can be stored. Memory quality ‚Äì memory captures individual tokens rather than the structure of a context span. Memory optimization ‚Äì update rules (e.g., gradient descent + momentum) are simple and often sub-optimal. Scalability ‚Äì sequential updates reduce parallelization. To overcome these issues, ATLAS proposes a more expressive and scalable framework in which:\nMemory is a deep MLP updated during inference (like Titans), but the update rule optimizes memory w.r.t. a whole context window, using richer feature mappings and a stronger internal optimizer. This allows the model to memorize context, not just individual tokens, while retaining RNN-style linear complexity.\n2. Method The ATLAS framework has three core components:\n2.1. High-capacity memory via polynomial feature mapping Instead of using raw keys $k$, ATLAS applies a feature map $\\phi(k)$:\nPolynomial kernels increase effective dimensionality from $d ‚Üí O(d^p)$. Exponential kernels approximate the exponential $q^T k$ of Transformers. This dramatically increases memory capacity without increasing the number of memory parameters.\n2.2. Context-aware memory update (Omega Rule) Unlike Titans (which update with only $(k_t, v_t)$),\nATLAS updates memory using a sliding window of the last c tokens:\n$$ M_t = \\arg\\min_M \\sum_{i=t-c+1}^{t} Y_i^{(t)} | M(\\phi(k_i)) - v_i |^2 $$\nFeatures:\nMulti-token optimization: memory learns from a context span, not a single token. Learned gates $Y_i^t$: controls how much each token contributes. Generalizes classical rules: $c=1$ ‚Üí Delta rule / Titans $c=\\infty$ ‚Üí global optimization like attention This is the heart of ATLAS: memory performs small-batch gradient descent at test time on a local context.\n2.3. Stronger internal optimizer (Muon) ATLAS replaces Titans‚Äô first-order memory update (gradient descent + momentum):\n$$ M_t = \\alpha_t M_{t-1} + S_t,\\qquad $$\n$$ S_t = \\gamma_t S_{t-1} - \\eta_t \\nabla \\ell(M_{t-1}; k_t, v_t) $$\nwith a second-order, quasi-Newton style update using the Muon optimizer.\nMuon approximates the Newton update:\n$$ M_t = M_{t-1} - H_t^{-1} \\nabla \\ell_t, $$\nand replaces $H_t^{-1}$ with a cheap matrix-free approximation using the Newton‚ÄìSchulz iteration:\n$$ H_t^{-1} \\approx \\text{NS}(G_t), $$\nwhere $G_t$ is an approximate curvature / preconditioner matrix.\nThus, the ATLAS memory update using Muon becomes:\n$$ M_t = \\alpha_t M_{t-1} - \\eta_t \\text{NS}(G_t) , \\left( \\sum_{i=t-c+1}^{t} Y_i^{(t)} \\nabla \\ell(M_{t-1}; k_i, v_i) \\right), $$\nwhere:\n$\\alpha_t$ = learned forget gate $\\eta_t$ = learned step size $Y_i^{(t)}$ = per-token contribution gate $\\text{NS}(G_t)$ = Muon second-order curvature approximation ATLAS still computes the same gradient (the gap between predicted value and true value). Muon simply reshapes this gradient using an approximate second-order update, so the memory moves in a smarter direction, not just the steepest direction.\nTraining \u0026amp; parallelization To make Omega Rule scale to long sequences, ATLAS:\nsplits the sequence into parallel chunks, applies recurrent updates within a chunk, and applies parallelizable gradient accumulation across chunks. Thus ATLAS preserves the GPU/TPU-friendly nature of Titans while significantly improving memory quality.\n3. Novelty Attention replacement with linear complexity\nATLAS Layer substitutes the Transformer attention block with a read‚Äìwrite memory module that enables long-context reasoning at O(n) cost.\nContext-based memory updates\nATLAS replaces token-level updates (Titans) with the Omega Rule, optimizing memory over a window of past tokens rather than only the current one.\nDeep neural memory with high capacity\nThe memory is a deep MLP whose parameters are updated at inference.\nPolynomial/exponential feature maps expand keys/queries and give super-linear memory capacity.\nSecond-order test-time learning\nMemory updates use the Muon optimizer, a quasi-Newton method, providing more stable and expressive learning than Titan‚Äôs first-order gradient descent.\n","permalink":"/notes/atlas_learning_to_optimally_memorize_the_context_a/","summary":"\u003caside\u003e\n\u003cp\u003eTitans = learn to memorize token at test time,\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eATLAS\u003c/strong\u003e = learn to memorize context at test time, with more capacity, better objectives, and stronger optimizers.\u003c/p\u003e\n\u003c/aside\u003e\n\u003ch1 id=\"1-introduction\"\u003e\u003cstrong\u003e1. Introduction\u003c/strong\u003e\u003c/h1\u003e\n\u003cp\u003eATLAS addresses a fundamental limitation of modern recurrent memory models (e.g., Titans): they update memory \u003cstrong\u003etoken by token\u003c/strong\u003e, using only the current key‚Äìvalue pair, which limits:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eMemory capacity\u003c/strong\u003e ‚Äì only O(d) independent pairs can be stored.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMemory quality\u003c/strong\u003e ‚Äì memory captures individual tokens rather than the structure of a context span.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMemory optimization\u003c/strong\u003e ‚Äì update rules (e.g., gradient descent + momentum) are simple and often sub-optimal.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScalability\u003c/strong\u003e ‚Äì sequential updates reduce parallelization.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTo overcome these issues, ATLAS proposes a more expressive and scalable framework in which:\u003c/p\u003e","title":"ATLAS: Learning to Optimally Memorize the Context at Test Time"},{"content":"Introduction Theorem proving is hard for ML because there are not enough human-written proofs in formal languages. Geometry is especially difficult because translating human geometric proofs into machine-verifiable form is very hard.\nCurrent geometry provers rely mainly on symbolic rules and hand-crafted heuristics, not learning. The authors solve the data shortage by generating 100 million synthetic theorems and proofs using symbolic engines.\nThey introduce dependency difference, which helps create nearly 10 million steps of auxiliary constructions. Auxiliary construction means adding new helper points; this creates infinite branching and is the hardest part of geometry proofs. The synthetic data lets the model learn auxiliary constructions without human demonstrations.\nThey train a language model on all synthetic proofs and fine-tune it to focus on auxiliary constructions. The symbolic engines handle all normal deduction steps, while the neural model generates new proof terms.\nThe resulting system, AlphaGeometry, proves geometry problems and produces human-readable proofs, performing close to an IMO gold medalist level.\nMethod Generate synthetic geometry data Random simple premises ‚Üí symbolic engine ‚Üí 100M synthetic theorems + proofs. Symbolic Engine is built manually: Deductive Database (DD) ‚Üí Databse of deductive rules (symbolic rules) Algebraic Reasoning (AR) ‚Üí Gaussian method (algebraic solver) Extract auxiliary constructions: Use dependency difference to find extra helper points (~10M). Dependency difference works like this:\nGenerate random geometry premises. Use symbolic engine to derive all proofs. For each conclusion, use traceback to find the minimal premises needed. Compare minimal premises vs. original premises. The difference gives extra points ‚Äî interpreted as auxiliary constructions. These extra points become training examples for the language model. This teaches the model to generate helper points fully automatically, without human supervision.\nCompare the conclusion with its minimal required premises. Any extra points used along the way are treated as auxiliary constructions (~10M examples). These teach the model how to add helpful new points.\nTrain a Transformer model on serialized proofs. Feed it the serialized form of each synthetic theorem:\n.\nThe model learns how proofs and helper constructions look.\nNeuro-symbolic proof search: The symbolic engine tries to prove the input problem using known rules. If it cannot finish, the language model proposes a new helper point. Add the point ‚Üí try symbolic engine again ‚Üí repeat until solved. Think of AlphaGeometry as a student solving a geometry problem:\nStep 1: Try to solve it with what you already know\nThis is the Symbolic Engine.\nStep 2: If you\u0026rsquo;re stuck, add a new helper point\nThis is the Language Model.\nThen repeat. This cycle continues until the solution appears.\nNovelty Generate all training data synthetically\n‚Üí No human proofs needed.\nIntroduce ‚Äúdependency difference‚Äù\n‚Üí Automatically learn auxiliary constructions (helper points).\nNeuro-symbolic loop\nSymbolic engine deduces Neural model adds new constructions when stuck. These three ideas together let AlphaGeometry reach near IMO gold-medalist level in geometry.\nNeural = intuition and exploration. Uses a neural network to propose new ideas when rules are not enough. Symbolic = logic and verification. Uses exact deduction rules to prune the search and ensure correctness. Neuro-symbolic = combine neural creativity + symbolic rigor to solve problems that require both exploration and precise reasoning.\n","permalink":"/notes/solving_olympiad_geometry_without_human_demonstrat/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eTheorem proving is hard for ML\u003c/strong\u003e because there are not enough human-written proofs in formal languages. \u003cstrong\u003eGeometry is especially difficult\u003c/strong\u003e because translating human geometric proofs into machine-verifiable form is very hard.\u003c/p\u003e\n\u003cp\u003eCurrent geometry provers rely mainly on \u003cstrong\u003esymbolic rules and hand-crafted heuristics\u003c/strong\u003e, not learning. The authors solve the data shortage by generating \u003cstrong\u003e100 million synthetic theorems and proofs\u003c/strong\u003e using symbolic engines.\u003c/p\u003e\n\u003cp\u003eThey introduce \u003cstrong\u003edependency difference\u003c/strong\u003e, which helps create nearly \u003cstrong\u003e10 million steps of auxiliary constructions\u003c/strong\u003e. \u003cstrong\u003eAuxiliary construction\u003c/strong\u003e means adding new helper points; this creates infinite branching and is the hardest part of geometry proofs. The synthetic data lets the model \u003cstrong\u003elearn auxiliary constructions without human demonstrations\u003c/strong\u003e.\u003c/p\u003e","title":"Solving olympiad geometry without human demonstrations"},{"content":"1. Introduction 1.1. Informal math LLMs have improved, but face clear limits Most current AI4Math models use NLP-style ‚Äúinformal reasoning‚Äù (datasets, CoT, self-consistency).\nThey perform well on benchmarks like GSM8K and MATH, but this approach struggles to scale to advanced or research-level mathematics due to:\nscarcity of high-quality data difficulty evaluating long reasoning chains hallucinated or invalid reasoning 1.2. Scaling training alone is not enough Simply making models bigger and training on more data cannot solve these limitations.\nRecently, systems like OpenAI o1 try to scale inference-time reasoning (search + verification), but their effectiveness on advanced math remains uncertain.\nInference-time reasoning is the process where a model performs extended, multi-step thinking during answer generation / inference, often involving search(generate several answers ‚Üí select one), self-correction, and verification‚Äîwithout retraining the model.\n1.3. Formal mathematical reasoning is a crucial complementary path Formal methods use proof assistants (Lean, Coq, Isabelle) that:\nenforce strict, verifiable logic provide reliable feedback reduce hallucination support synthetic data generation 1.4. Recent successes show the promise of formal methods Neuro-symbolic systems like AlphaProof and AlphaGeometry achieve breakthrough results by combining:\nsymbolic formal systems (proof assiatant)‚Üí PA neural reasoning models These demonstrate that formal reasoning can scale to high-level mathematics.\n1.5. The field is at an inflection point AI-based formal mathematical reasoning is rapidly growing and has major potential for:\nadvancing pure mathematics improving software/hardware verification building reliable reasoning systems The paper argues that formal reasoning should complement informal LLM approaches to push AI4Math forward.\n2. Method 2.1. Informal Reasoning LLM NuminaMath: a math LLM for informal reasoning:\nMath pretraining Finetuning on step-by-step solutions Tool-integrated reasoning 2.2. Formal Mathematical Reasoning 2.2.1. Formal Proof Assistant Think of Lean like a strict programming language for math:\nProof Tree = how a single theorem is proved internally by Lean nodes are goals, edges are tactics Lean File = the code written by humans to prove the theorem Project = a whole project full of code 2.2.2. Neuro-symbolic Theorem Prover 3. Open Challenges and Future Directions 3.1. Data 3.2. Algorithms 3.3. Tools for Assisting Human Mathematicians 4. Milestones and Success Measures ","permalink":"/notes/formal_mathematical_reasoning_a_new_frontier_in_ai/","summary":"\u003ch1 id=\"1-introduction\"\u003e1. Introduction\u003c/h1\u003e\n\u003ch3 id=\"11-informal-math-llms-have-improved-but-face-clear-limits\"\u003e\u003cstrong\u003e1.1. Informal math LLMs have improved, but face clear limits\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eMost current AI4Math models use NLP-style ‚Äúinformal reasoning‚Äù (datasets, CoT, self-consistency).\u003c/p\u003e\n\u003cp\u003eThey perform well on benchmarks like GSM8K and MATH, but this approach struggles to scale to \u003cstrong\u003eadvanced or research-level mathematics\u003c/strong\u003e due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003escarcity of high-quality data\u003c/li\u003e\n\u003cli\u003edifficulty evaluating long reasoning chains\u003c/li\u003e\n\u003cli\u003ehallucinated or invalid reasoning\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"12-scaling-training-alone-is-not-enough\"\u003e\u003cstrong\u003e1.2. Scaling training alone is not enough\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eSimply making models bigger and training on more data cannot solve these limitations.\u003c/p\u003e","title":"Formal Mathematical Reasoning A New Frontier in AI"},{"content":"1. Introduction Transformers provide accurate short-term memory through attention but suffer from quadratic cost and fixed context window limits. Linear Transformers and modern linear RNNs improve efficiency but must compress all history into fixed-size states, which causes memory overflow and poor long-range recall.v\nHuman memory has separate short-term and long-term systems; existing architectures usually miss long-term memory that can adapt at test time.\nKey question: How to design a neural long-term memory that can learn, forget, and recall information over extremely long contexts efficiently?\nTitans introduce:\nA neural long-term memory (LMM) that updates weights at test time. Titans architectures that integrate LMM with attention and persistent memory. 2. Method 2.1 Neural Long-term Memory (LMM) LMM treats learning as memorizing past tokens into its parameters during test time.\nUses a surprise metric: the gradient of the associative memory loss with respect to the input, larger gradient ‚Üí more ‚Äúsurprising‚Äù token ‚Üí more memorable.\nThe memory update dynamics are:\nMemory stores Associative key‚Äìvalue pairs using a loss**:** $$ \\ell = |M_{t-1}(k_t) - v_t|_2^2 $$\nwhere $k_t = x_t W_K$ and $v_t = x_t W_V$.\nSurprise gradient: $$ g_t = \\nabla_{M_{t-1}}\\ell $$\nSurprise momentum: $$ S_t = \\eta_t S_{t-1} - \\theta_t g_t $$\nCombined using a data-dependent decay $\\eta_t$ and learning rate $\\theta_t$ Forget gate: $\\alpha_t \\in [0,1]$ $\\alpha_t \\to 1$ ‚áí forget history $\\alpha_t \\to 0$ ‚áí retain history Memory update: $$ M_t = (1 - \\alpha_t) M_{t-1} + S_t $$\nRetrieval: $$ y_t = M_t(q_t) $$\n2.2 Parallelizable Training LMM training is equivalent to mini-batch gradient descent with momentum + weight decay.\nThe authors show this can be reformulated into operations using matmuls + associative scan, enabling fast, hardware-friendly parallel training.\n2.3 Persistent Memory A small set of fixed, learnable vectors prepended to the sequence.\nPurpose:\nStore task-level knowledge (not input-dependent). Counteract attention bias toward early tokens. Equivalent to data-independent attention keys/values (as shown by FFN‚Üísoftmax reinterpretation). 2.4 Titans Architectures (Three Variants) All Titans have three components:\nShort-term memory = attention (sliding window attention) Long-term memory = LMM Persistent memory = learned prefix Variants:\nMAC ‚Äî Memory as Context: Retrieve memory ‚Üí concatenate with persistent memory ‚Üí feed into attention ‚Üí Best long-context performance.\nMAG ‚Äî Memory as Gate: Combine Sliding Window Attention output and memory output via gating. MAL ‚Äî Memory as Layer: Sequential: LMM ‚Üí Sliding Window Attention. Simpler but weaker performance. 3. Novelty 3.1. Memory Structure Titans introduce a long-term memory (LMM) that can learn and store information across millions of tokens. This memory is a deep, learnable module, not just a matrix or KV-cache. Three designs (MAL) show flexible ways to combine long-term memory with short-term attention. 3.2. Memory Update LMM learns during inference, using a simple idea: more surprising tokens are written more strongly. Updates use momentum (past surprise + current surprise) for stability. A forget gate decides how much old memory to remove to avoid overflow. 3.3. Memory Retrieval LMM learns a key ‚Üí value mapping, acting like a smart, compressing KV-cache. The model retrieves long-term information when needed and mixes it with short-term attention (SWA). 4. More Details 4.1. What is M? M is a learnable function (an MLP) that stores long-term information.\n$$ M : R^d \\rightarrow R^d $$\nIt takes a vector (key or query) and outputs another vector (a ‚Äúmemory value‚Äù).\nYou can think of M as a neural dictionary:\ninput = address output = content Except this dictionary learns at test time, and compresses many past tokens into a fixed-sized neural network.\nM is a single neural function used for both retrieving (q‚Üímemory output) and storing (k‚Üív); it learns key‚Äìvalue associations during update and returns long-term recall results during retrieval.\n4.2. What does M do during RETRIEVAL? Retrieval input: query\n$$ q_t = x_t W_Q $$\nMemory returns:\n$$ y^{(LMM)}_t = M(q_t) $$\nMeaning:\n‚ÄúGiven this query, what long-term knowledge have we stored that matches it?‚Äù\nSo during retrieval:\nM behaves as a lookup function q = the question M(q) = the answer from long-term memory Here, all the knowledge is compressed inside the MLP weights.\n4.3. What does M do during UPDATE? Update input: key\n$$ k_t = x_t W_K,\\qquad v_t = x_t W_V $$\nMemory tries to learn the mapping:\n$$ M(k_t) \\approx v_t $$\nError:\n$$ \\ell = | M(k_t) - v_t |^2 $$\nGradient:\n$$ g_t = \\nabla_{M} \\ell $$\nMemory update:\n$$ M \\leftarrow M - \\theta g_t $$\nMeaning:\n‚ÄúGiven this key, the correct value should be v. Update yourself so you can remember this in the future.‚Äù\nSo during update:\nM behaves as a learnable associative memory k = the address v = the content M learns: k ‚Üí v This is exactly like writing to a KV-cache ‚Äî except the cache is a learnable neural network that can compress, forget, and generalize.\n4.4. What role does M play? Role 1: long-term storage M learns from (k,v) pairs:\n$$ M(k) \\approx v $$\nThis is how it stores information.\nRole 2: long-term retrieval M responds to queries:\n$$ M(q) = \\text{long-term memory output} $$\nThis is how it retrieves information.\nWhy both? Because:\nKeys write into memory Queries read from memory Both must use the same space so they match Keys are used to WRITE because they represent stable addresses for storing information (like a KV-cache).\nQueries are used to READ because they represent the current question the model is asking the memory.\nTitans follow the same logic as attention: K = address, V = content, Q = question.\n","permalink":"/notes/titans_learning_to_memorize_at_test_time/","summary":"\u003ch1 id=\"1-introduction\"\u003e\u003cstrong\u003e1. Introduction\u003c/strong\u003e\u003c/h1\u003e\n\u003cp\u003eTransformers provide \u003cstrong\u003eaccurate short-term memory\u003c/strong\u003e through attention but suffer from \u003cstrong\u003equadratic cost\u003c/strong\u003e and \u003cstrong\u003efixed context window\u003c/strong\u003e limits. Linear Transformers and modern linear RNNs improve efficiency but must \u003cstrong\u003ecompress all history into fixed-size states\u003c/strong\u003e, which causes \u003cstrong\u003ememory overflow and poor long-range recall\u003c/strong\u003e.v\u003c/p\u003e\n\u003cp\u003eHuman memory has \u003cstrong\u003eseparate short-term and long-term systems\u003c/strong\u003e; existing architectures usually miss long-term memory that can adapt at test time.\u003c/p\u003e\n\u003cp\u003eKey question: \u003cem\u003eHow to design a neural long-term memory that can learn, forget, and recall information over extremely long contexts efficiently?\u003c/em\u003e\u003c/p\u003e","title":"Titans: Learning to Memorize at Test Time"},{"content":"Introduction Positional information is very important. Transformers need positional information; Old methods add it directly to token representations, making them incompatible with linear self-attention. Motivation for RoPE:\nSelf-attention is often seen as position-agnostic, which is undesirable. Attention mechanism cannot understand word order by itself. It does not know the position of tokens. Need a method that: Encodes both absolute and relative positions Works with linear self-attention Has good behavior for long-range dependency modeling ‚Üí Previous methods fail at least one of these.\nMethod Goal: Encode relative position inside attention Transformers use self-attention, which computes:\n$$ q_m^\\top k_n $$\nRight now, this inner product does not include position information unless we manually add it.\nThe authors want:\n$$ \\langle f_q(x_m, m),\\ f_k(x_n, n)\\rangle = g(x_m, x_n, m - n) $$\nMeaning: the attention score should depend on the relative position (m ‚àí n), not the absolute positions m and n.\nRotary Position Embedding (RoPE) RoPE in 2D Query encoding:\n$$ f_q(x_m, m) = (W_q x_m)e^{im\\theta} $$\nKey encoding:\n$$ f_k(x_n, n) = (W_k x_n)e^{in\\theta} $$\nHere, multiplying by $e^{i m\\theta}$ rotates the vector by an angle proportional to position m.\nAttention after rotation:\n$$ g(x_m, x_n, m-n) = \\text{Re}\\left[ (W_q x_m)(W_k x_n)^* e^{i(m-n)\\theta} \\right] $$\nKey idea: Rotation angles subtract ‚Üí giving relative position (m - n).\nReal 2D rotation matrix form Complex rotation can be written as a real 2√ó2 matrix:\nMeaning: A simple 2D rotation based on position index (m).\nGeneral RoPE for d-dimensional vectors Split $x\\in R^d$ into d/2 pairs. For each pair, apply a 2D rotation with its own frequency $\\theta_i$:\nFrequencies:\n$$ \\theta_i = 10000^{-2(i-1)/d} $$\n(same spectrum as sinusoidal PE)\nRelative position emerges in attention Applying RoPE inside attention:\n$$ q_m^\\top k_n = x_m^\\top W_q^\\top R^d_{\\Theta, n-m} W_k x_n $$\nWhere:\n$$ R^d_{\\Theta, n-m} = (R^d_{\\Theta,m})^\\top R^d_{\\Theta,n} $$\nMeaning: The effective rotation depends only on relative distance (n - m).\nRePE with linear attention Novelty Encode position by rotation, not by adding vectors. Relative position appears automatically through angle differences. Compatible with linear attention (unlike additive methods). No extra parameters; simple and stable. ","permalink":"/notes/roformer_enhanced_transformer_with_rotary_position/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003ePositional information is very important. Transformers need positional information;\u003c/li\u003e\n\u003cli\u003eOld methods add it directly to token representations, making them incompatible with \u003cstrong\u003elinear self-attention\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMotivation for \u003cstrong\u003eRoPE\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSelf-attention is often seen as \u003cstrong\u003eposition-agnostic\u003c/strong\u003e, which is undesirable.\n\u003cul\u003e\n\u003cli\u003eAttention mechanism cannot understand word order by itself. It does not know the position of tokens.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNeed a method that:\n\u003cul\u003e\n\u003cli\u003eEncodes both \u003cstrong\u003eabsolute\u003c/strong\u003e and \u003cstrong\u003erelative\u003c/strong\u003e positions\u003c/li\u003e\n\u003cli\u003eWorks with \u003cstrong\u003elinear self-attention\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eHas good behavior for \u003cstrong\u003elong-range dependency modeling\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e‚Üí\u003c/strong\u003e Previous methods fail at least one of these.\u003c/p\u003e","title":"Roformer: Enhanced Transformer With Rotary Position Embedding"},{"content":"Introduction AlphaGo AlphaGo combines CNN with MCTS to play Go at a superhuman level.\nIt first trains two types of networks (policy network \u0026amp; value network \u0026amp; rollout policy), using the current game board combined with several handcrafted Go features as input:\nThen integrates the networks into MCTS to enhance the basic tree search.\nAlphaGo Zero AlphaGo Zero combines a deep residual network ResNet with MCTS. Unlike AlphaGo, where MCTS is separate from training, AlphaGo Zero integrates MCTS directly inside the training loop.\nIt trains a single unified network (two heads, produce policy and value outputs) directly from raw board positions, without using any human games or handcrafted Go features.\nTrain the model to minimize the gap between p and œÄ | v and z.\nHow to choose the next move? ‚Üí Policy as prior probability in MCTS. The actual move is always selected based on the MCTS-improved policy œÄ, produced after many MCTS simulations.\nMethod AlphaZero generalizes AlphaGo Zero to Go, Chess, and Shogi using one unified algorithm.\nIt removes all handcrafted knowledge and relies only on the basic game rules. A single neural network $f_\\theta(s)$ outputs both the policy p and value v from the raw board. Training MCTS uses this network to produce an improved policy $\\pi$, which selects moves and supervises training. Self-play generates $(s, \\pi, z)$ data, and training minimizes value error $(z - v)^2$ and policy cross-entropy $-\\pi^\\top \\log p$.\nThe same network is updated continuously, with no best-player selection stage. Hyperparameters are reused across games, and the board is encoded only by simple rule-based planes with no extra features.\nNovelty Compared with AlphaGo Zero:\nAlphaZero works for multiple games, not only Go. Predicts expected outcome (handles draws), not just win/loss. Removes all symmetry augmentation and board transformations. Uses one continuously updated network, no best-player selection. Reuses one set of hyperparameters across all games. Best-player selection ‚Üí In AlphaGo Zero:\nTrain a new network for one iteration. Play it against the previous best network for several round games. If it wins more than 55%, it becomes the new best network. Otherwise, it is discarded. Only the best network is used for future self-play training. ","permalink":"/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003ch2 id=\"alphago\"\u003eAlphaGo\u003c/h2\u003e\n\u003cp\u003eAlphaGo combines \u003cstrong\u003eCNN\u003c/strong\u003e with \u003cstrong\u003eMCTS\u003c/strong\u003e to play Go at a superhuman level.\u003c/p\u003e\n\u003cp\u003eIt first trains two types of networks (\u003cstrong\u003epolicy network\u003c/strong\u003e \u0026amp; \u003cstrong\u003evalue network \u0026amp; rollout policy\u003c/strong\u003e), using the current game board combined with several handcrafted Go features  as input:\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/image.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThen integrates the networks into MCTS to enhance the basic tree search.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/image_1.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"alphago-zero\"\u003eAlphaGo Zero\u003c/h2\u003e\n\u003cp\u003eAlphaGo Zero combines a \u003cstrong\u003edeep residual network ResNet\u003c/strong\u003e with \u003cstrong\u003eMCTS\u003c/strong\u003e. Unlike AlphaGo, where MCTS is separate from training, AlphaGo Zero integrates MCTS directly inside the training loop.\u003c/p\u003e","title":"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"},{"content":"Introduction AlphaGo AlphaGo was the first AI to beat top human Go players. AlphaGo combines CNN with MCTS to play Go at a superhuman level.\nIt first trains two types of networks (policy network \u0026amp; value network \u0026amp; rollout policy), using the current game board combined with several handcrafted Go features as input:\nThen integrates the networks into MCTS to enhance the basic tree search.\nAlphaGo Zero removes all human input.\nIt learns only from self-play and starts from random play. Its input is just the raw board (white_stones), with no handcrafted features. It uses one unified neural network that outputs both policy and value. Its search is simpler (no rollouts) and relies entirely on the neural network. Method AlphaGo Zero Network The network takes the raw board state and outputs:\np: a distribution over moves v: probability of winning It is made from deep residual blocks with batch norm and ReLU.\nTraining the Network In each training position, MCTS is run using the current network.\nMCTS produces an improved move distribution œÄ through simulation like AlphaGo.\nMCTS can therefore be viewed as a policy improvement operator. A new move is selected from œÄ, and the self-play game continues.\nAfter each game, the terminal result gives a final value z.\nTraining uses three terms to update the network:\n(z ‚àí v)¬≤ ‚Üí match value prediction to the outcome ‚àíœÄ·µÄ log p ‚Üí match network policy to the MCTS policy L2 regularization Continuous Self-Improvement AlphaGo Zero repeatedly plays self-play games, updates the network, and replaces the old version when the new model wins enough evaluation games.\nOver many iterations, the system improves rapidly and surpasses earlier AlphaGo versions.\nNovelty It‚Äôs trained solely by self-play RL, starting from ran¬≠dom play, without any supervision or use of human data. It uses only the black and white stones from the board as input features. It uses a single neural network, rather than separate policy and value networks. It uses a simpler tree search that relies upon this single neural network to evaluate positions and sample moves. ","permalink":"/notes/mastering_the_game_of_go_without_human_knowledge/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003ch2 id=\"alphago\"\u003eAlphaGo\u003c/h2\u003e\n\u003cp\u003eAlphaGo was the first AI to beat top human Go players. AlphaGo combines \u003cstrong\u003eCNN\u003c/strong\u003e with \u003cstrong\u003eMCTS\u003c/strong\u003e to play Go at a superhuman level.\u003c/p\u003e\n\u003cp\u003eIt first trains two types of networks (\u003cstrong\u003epolicy network\u003c/strong\u003e \u0026amp; \u003cstrong\u003evalue network \u0026amp; rollout policy\u003c/strong\u003e), using the current game board combined with several handcrafted Go features  as \u003cstrong\u003einput\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"/notes/mastering_the_game_of_go_without_human_knowledge/image.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThen integrates the networks into MCTS to enhance the basic tree search.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"/notes/mastering_the_game_of_go_without_human_knowledge/image_1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAlphaGo Zero\u003c/strong\u003e removes all human input.\u003c/p\u003e","title":"Mastering the game of Go without human knowledge"},{"content":"Introduction CNNs treat inputs as general images with no special structure. But LF is not a random stack of images.\nTo be more specific, CNN expects the disparity of all pixels are the same. But in light field, objects with different depth exist in the same images! So a standard CNN kernel cannot learn mixed spatial + angular + disparity correlations incorrectly. This is why 2D CNNs fail on LF directly. In short, A fixed convolution window cannot follow disparity.\nThere are many existing methods aiming to solve this problem.\nCurrent Questions The current CNN structures for LF image processing:\nNeighbor-view combination ‚Üí Since only 2 or 4 adjacent views were used to SR a specific view, this method cannot achieve a high reconstruction quality due to the discard of rich angular information. Convolution on EPIs ‚Üí since an EPI is a 2D slice of a 4D LF, performing conv on EPIs can only incorporate angular information from the same horizontal or vertical views and cannot incorporate the spatial context prior. Multi-stream structure ‚Üí it discards the views outside the four angular directions, resulting in under-exploitation of the rich angular information in an LF. Spatial-angular alternate convolution ‚Üí it processes spatial and angular features separately. The model never learns deep joint spatial-angular representations. The key difference between existing schemes and disentangling mechanism is, disentangling mechanism can fully use the information from all angular views and incorporate the LF structure prior.\nWith this mechanism, high-dimensional LF data can be disentangled into different low-dimensional subspaces. Consequently, the difficulty for learning deep CNNs is reduced and several LF image processing tasks can be benefited.\nTake the 4D LF and separate into:\nSpatial component (x,y) Angular component (u,v) Disparity component (slopes in EPI) LF structural priors = the known geometric and angular relationships that exist naturally in light-field images.\nNow the CNN can learn:\nDisparity changes ‚Üí from angular subspace Texture \u0026amp; edges ‚Üí from spatial subspace Parallax = correlation between spatial \u0026amp; angular subspaces LF 4D consistency = combining both subspaces Method LF Representation SAIs: many 2D views arranged by angle. 3D stack: SAIs stacked to show angular variation. EPI: a 2D slice that reveals disparity as straight lines. SAI array: full grid of all SAIs. MacPI: one 2D image where each spatial location stores angular samples. Compared to SAI and EPI, the authors adopt the MacPI representation because it evenly mixes the spatial and angular information in the light field. This makes it easier to extract and combine spatial and angular features using convolution operations.\nLF Feature Disentanglement The 4D light field contains spatial (H, W) and angular (U, V) information. To make learning easier, the model uses three feature extractors, each handling one 2D subspace.\nAngular feature extractor, Spatial feature extractor, Vertical EPI feature extractor, Horizontal EPI feature extractor. Spatial Feature Extractor SFE extracts spatial texture inside the same view. It uses a 3√ó3 dilated convolution to avoid mixing different views. Output size stays the same as the input.\nAngular Feature Extractor AFE extracts angular information inside each macro-pixel. It uses an A√óA convolution with stride A, so each angular block becomes one spatial pixel. Output size becomes H √ó W.\nEPI Feature Extractors SFE and AFE miss the spatial‚Äìangular relationship. EPIs show this relationship as line patterns.\nEFE-H processes horizontal EPIs (V‚ÄìW). EFE-V processes vertical EPIs (U‚ÄìH). They learn disparity and spatial‚Äìangular correlation.\nHow it solve current questions This design keeps all angular views, instead of discarding neighbors or directions, so the network can fully use the complete angular information.\nIt also processes spatial, angular, and EPI cues simultaneously, rather than alternating between them, which preserves both spatial context and angular geometry.\nThe network becomes easier to train and can learn deeper, more expressive LF representations, leading to better reconstruction quality for all LF image processing tasks.\nDistgSSR Disentangling Mechanism for Spatial SR.\nDistgSSR applies the disentangling mechanism to do LF spatial SR.\nIt converts LR SAIs to MacPI, extracts spatial, angular, and EPI features, and uses a residual-in-residual structure for better SR quality. Only the Y channel is SR; Cb/Cr are bicubic-upscaled.\nThe 3 channels for image with YCbCr color space RGB:\nY = brightness Cb = blue color difference Cr = red color difference Most of the detail, texture, and edges are stored in the Y channel.\nColor (Cb, Cr) changes slowly and contains low-frequency information.\nDistg-Block Each Distg-Block has four parallel branches:\nSpatial branch: two SFEs to keep spatial details. Angular branch: AFE + 1√ó1 conv + 2D pixel shuffle for angular-to-spatial upsampling. Two EPI branches: EFE-H/EFE-V + 1√ó1 conv + 1D pixel shuffle to handle disparity through EPI lines. All branches are fused by 1√ó1 conv + SFE, with a skip connection for local residual learning.\nPixSF-1D:\nFor EFE-H, the angular variation is only 1D (the V direction) along a horizontal EPI (V‚ÄìW). Convolution flattens these V angular samples into the horizontal (W) direction, compressing the angular information into the channel space in a 1D form.\nPixSF-1D then:\nun-flattens this 1D angular information. rearranges it back into the correct spatial structure. but only along one direction (horizontal). So PixSF-1D re-expands 1D angular EPIs into spatial feature maps along a single axis.\nSpatial Upsampling After all groups, MacPI features are reshaped back to SAIs.\nA 1√ó1 conv increases channels, a PixSF-2D upsamples the resolution, and a final 1√ó1 conv to reduce channels to 1, so the output is the HR Y-SAIs.\nCb and Cr from the input are bicubic-upsampled to the same HR size, channels (Y, Cb, Cr) are combined and convert to RGB version. That gives the final HR LF image.\nExperiments Achieve high spatial reconstruction quality and angular consistency.\nDistgASR Disentangling Mechanism for Angular SR.\nDistgASR applies the disentangling mechanism to angular super-resolution.\nIt takes a sparse angular SAI array and reconstructs a dense angular array. The input SAI array is first converted to a MacPI for spatial, angular, and EPI feature extraction.\nDistg-Block for ASR A Distg-Block separately processes spatial, angular, and EPI features. Then the features are fused.\nFor ASR, angular information is more important because we must create many new views. Also, disparity between sparse views is large, so EPI features are helpful.\nTherefore, the angular and EPI branches output C channels to keep more useful information.\nAngular Upsampling Angular upsampling must handle non-integer upsampling factors (for example 2√ó2 ‚Üí 7√ó7).\nPixel shuffle cannot handle non-integer factors directly.\nSo the network uses a downsample‚Äìupsample strategy.\nSteps‚Üí\nAFE produces a downsampled angular feature. A 1√ó1 conv expands channels. A 2D pixel shuffle performs angular upsampling. Finally, a 1√ó1 conv + SFE recover the output SAI array. This allows clean and flexible angular SR, even when the upsampling factor is non-integer.\nDistgDISP Disentangling Mechanism For Disparity Estimation.\nDistgDisp applies the disentangling mechanism to light-field disparity estimation.\nThe network input is a MacPI with 9√ó9 angular views. The network performs:\nfeature extraction ‚Üí cost volume construction ‚Üí cost aggregation ‚Üí disparity regression.\nSpatial Res-Block forFeature Extraction A Spatial Res-Block is used to extract spatial features. It uses SFE + BN + LeakyReLU + SFE + BN. A residual skip adds the input back to the block output. This helps model spatial context and smooth texture areas.\nDS-AFEs for CostVolume Construction Cost volume needs features from different disparities. Existing LF methods use ‚Äúshift-and-concat‚Äù, which is slow. DistgDisp replaces this with DS-AFEs, which directly convolve pixels that match a specific disparity.\nDS-AFE works like this:\nFor each view (u, v), the offset of its corresponding pixel depends on disparity d. When converted into MacPI, pixels of the same disparity form a square pattern. A properly designed convolution (padding) can extract all pixels with disparity d in one pass. Different disparity values use different dilation and padding.\nThe author use 9 disparity levels (‚àí4 to +4). For each disparity, a cost volume is produced. All cost volumes are concatenated into a 5D tensor B √ó 9 √ó C √ó H √ó W.\nCost Aggregation and Disparity Regression Eight 3D convolutions (3√ó3√ó3 kernel) aggregate the cost volumes. This produces a final 3D tensor of size D √ó H √ó W. A softmax is applied along the disparity axis.\nThe predicted disparity is:\n$$ \\hat{d} = \\sum_{d \\in D} d \\cdot softmax(F_{final}) $$\nThis gives the final disparity map.\n","permalink":"/notes/disentangling_light_fields_for_super-resolution_an/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eCNNs treat inputs as \u003cstrong\u003egeneral images\u003c/strong\u003e with no special structure. But LF is \u003cstrong\u003enot a random stack of images.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo be more specific, CNN expects the disparity of all pixels are the same. But in light field, objects with different depth exist in the same images! So a standard CNN kernel cannot learn mixed spatial + angular + disparity correlations incorrectly. This is \u003cstrong\u003ewhy 2D CNNs fail\u003c/strong\u003e on LF directly. In short, A fixed convolution window cannot follow disparity.\u003c/p\u003e","title":"Disentangling Light Fields for Super-Resolution and Disparity Estimation"},{"content":" Each recurrence step is like a different attention head, but instead of running fully in parallel (multi-head), Hyena stacks them sequentially (multi-step).\nReview Transformer Attention in a Transformer takes a sequence of tokens and, for each token, creates a new representation by mixing information from all tokens.\nEach token first computes its own query, while every token also has a key and a value. For a token i, attention computes similarity scores between its query $Q_i$ and all keys $K_j$, applies softmax to turn these scores into weights, and then forms the output $y_i$ as a weighted sum of all values $V_j$.\n1. Introduction Traditional attention is expressive but computationally expensive.\nHyena asks:\nCan we reproduce the essential capabilities of attention: global mixing + data control ‚Äîwithout building a full N√óN matrix?\nThe answer is yes, using two cheap primitives:\nLong implicit convolutions (for global token mixing) Element-wise gates (for input-dependent weighting) Attention action Hyena replacement Mixed all tokens according to distance \u0026amp; structure (global mixing) Toeplitz convolution Weighted tokens based on QK / input (data control) Gating (diagonal matrix) Hyena stacks these primitives in a recurrence to approximate the expressiveness of attention at lower cost.\n2. Method The paper presents Hyena, an attention-free building block that replaces Transformers‚Äô attention mechanism using a recurrence of gating and implicitly-parameterized long convolutions.\nFactorizing Attention Into Cheap Components: Self-attention does:\n$$ y = softmax(QK^\\top) V $$\nHyena replaces the giant attention matrix with a product of Toeplitz (convolution) and diagonal (gating) matrices:\n$$ H(u) = D_x^N S_h^N \\cdots D_x^2 S_h^2 D_x^1 S_h^1 $$\nWhere:\nS‚Çï‚Åø ‚Äî Toeplitz matrices implementing long convolution D‚Çì‚Åø ‚Äî diagonal matrices implementing input-controlled gating Hyena forward pass: z1 = v # \u0026ldquo;value\u0026rdquo; z(n+1) = x(n) ‚äô (h(n) ‚àó z(n)) # conv ‚Üí gate y = z(N+1)\nRepeating (conv ‚ûú gate) N times gives Hyena deep expressive power, similar to attention heads.\n3. Novelty A. Implicit Long Convolutions Hyena creates very long filters using a small FFN with positional encoding, instead of storing huge kernels. These filters capture long-range dependencies and are applied efficiently with FFT in O(N log N).\n$$ h^n(t) = Window(t) \\cdot FFN(PosEnc(t)) $$\nB. Data-Controlled Gating Each step computes a gating vector from the input (via a linear layer). This makes the mixing input-dependent, similar to how QK·µÄ gives dynamic weights in attention.\n$$ v = W_v u \\ x^n = Linear_n(u) \\ = W_n u $$\nC. Recurrence Depth as Multi-Head Analogue Hyena stacks many (convolution ‚Üí gate) blocks. Each block learns a different interaction pattern, like an attention head, but sequential and far cheaper.\n$$ S_h¬π ‚Üí D_x¬π ‚Üí S_h¬≤ ‚Üí D_x¬≤ ‚Üí ‚Ä¶ ‚Üí S_h·¥∫ ‚Üí D_x·¥∫ $$\nD. Fast FFT-Based Computation All long convolutions are executed using FFT ‚Üí multiply ‚Üí inverse FFT, avoiding large matrices and enabling efficient processing of very long sequences.\n$$ h^n * z^n $$\nE. Structured Matrix Factorization View Hyena effectively approximates an attention matrix by breaking it into many Toeplitz (convolution) and diagonal (gating) factors. This yields an expressive, attention-like operator with lower cost.\n$$ A(q,k) \\approx D_q S_\\psi D_k S_\\varphi $$\n","permalink":"/notes/hyena_hierarchy_towards_larger_convolutional_langu/","summary":"\u003caside\u003e\n\u003cp\u003eEach recurrence step is like a different attention head, but instead of running fully in parallel (multi-head), Hyena stacks them \u003cstrong\u003esequentially (multi-step)\u003c/strong\u003e.\u003c/p\u003e\n\u003c/aside\u003e\n\u003ch1 id=\"review-transformer\"\u003eReview Transformer\u003c/h1\u003e\n\u003cp\u003eAttention in a Transformer takes a sequence of tokens and, for each token, creates a new representation by mixing information from \u003cstrong\u003eall\u003c/strong\u003e tokens.\u003c/p\u003e\n\u003cp\u003eEach token first computes its own query, while every token also has a key and a value. For a token i, attention computes similarity scores between its query $Q_i$ and all keys $K_j$, applies softmax to turn these scores into weights, and then forms the output $y_i$ as a weighted sum of all values $V_j$.\u003c/p\u003e","title":"Hyena Hierarchy: Towards Larger Convolutional Language Models"},{"content":"blog: https://blog.csdn.net/v_JULY_v/article/details/134923301\nS4 video: https://www.youtube.com/watch?v=luCBXCErkCs\nIntroduction Foundation models are large pretrained models used for many tasks like text, image, and audio. Most of them are built on the Transformer, which uses attention to connect information between tokens. However, attention is slow and limited to a fixed window, and its cost grows very fast with sequence length. Many improved versions exist, but none perform as well as Transformers at scale.\nStructured State Space Models (SSMs) are efficient sequence models inspired by RNNs and control theory. They can model long sequences with linear time complexity and work well on continuous data like audio or vision. But they still perform poorly on discrete data such as text because they cannot focus on relevant information.\nThe paper proposes Selective SSMs that let model parameters depend on the input. This helps the model decide what to keep or forget based on the content, similar to attention. The authors also design a new hardware-efficient algorithm to make it run fast and truly linear in sequence length.\nThe resulting Mamba architecture combines these selective SSMs into a simple, recurrent model. Mamba trains and runs much faster than Transformers, handles very long contexts, and needs less memory.\nExperiments show Mamba performs as well as or better than Transformers across language, audio, and genomics tasks. It reaches Transformer-level accuracy with only half the size and runs up to 5√ó faster. Mamba proves that efficient, attention-free models can match Transformer quality while scaling to very long sequences.\nMethod 1. Problems Transformers are powerful but slow and memory-heavy (quadratic cost). Existing efficient models (like SSMs, RNNs) are not content-aware ‚Äî can‚Äôt decide what to remember or ignore. Recurrent models are often hardware-inefficient on GPUs. 2. Architecture Mamba uses a new Selective State Space Model (SSM). Adds a selection mechanism so parameters depend on the input (content-aware). Each layer combines one Conv (local info) and one SSM (long memory) into a simple, unified block. 3. Noval Selective mechanism: Traditional SSMs use fixed matrices A, B, C Mamba updates them input-dependent: A, B, C, Œî ‚Üí adaptive reasoning. Hardware-aware selective scan: optimized GPU recurrence using fast SRAM. Practical simplified architecture: Removes heavy math from S4 (HiPPO kernel, complex eigenvalues) Uses simple parameterization + gating Makes SSMs trainable and stable in large models Result:\nMamba keeps linear-time efficiency, adds attention-like adaptability, and achieves Transformer-level or better performance up to 5√ó faster.\n","permalink":"/notes/mamba_linear-time_sequence_modeling_with_selective/","summary":"\u003cp\u003eblog: \u003ca href=\"https://blog.csdn.net/v_JULY_v/article/details/134923301\"\u003ehttps://blog.csdn.net/v_JULY_v/article/details/134923301\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eS4 video: \u003ca href=\"https://www.youtube.com/watch?v=luCBXCErkCs\"\u003ehttps://www.youtube.com/watch?v=luCBXCErkCs\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eFoundation models are large pretrained models used for many tasks like text, image, and audio. Most of them are built on the Transformer, which uses attention to connect information between tokens. However, attention is slow and limited to a fixed window, and its cost grows very fast with sequence length. Many improved versions exist, but none perform as well as Transformers at scale.\u003c/p\u003e\n\u003cp\u003eStructured State Space Models (SSMs) are efficient sequence models inspired by RNNs and control theory. They can model long sequences with \u003cstrong\u003elinear time complexity\u003c/strong\u003e and work well on continuous data like audio or vision. But they still perform poorly on discrete data such as text because they cannot focus on relevant information.\u003c/p\u003e","title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces"},{"content":" Three methods of Light Field Super-Resolution (LFSR):\nLight field spatial super-resolution (LFSSR) Light field angular super-resolution (LFASR) Light field spatial and angular super-resolution (LFSASR) Experiments for each method.\n1. Light field spatial super-resolution (LFSSR) Improve spatial resolution of each sub-aperture image (SAI).\n1.1. CNN-based method Use convolutions to learn spatial / angular correlations and fuse high-frequency details.\nResidual CNNs: Learn directional features and fuse sub-pixel details (e.g., resLF). Feature alignment: Optical-flow-based alignment, deformable conv alignment. 4D / separable CNNs: Use 4D conv or spatial‚Äìangular separable conv to jointly extract features. View fusion models: ‚ÄúAll-to-One‚Äù, multi-view complementary information fusion. Attention-based CNNs: Channel/view attention, angular deformable alignment. resLF\nseperable Conv model\n‚ÄúAll-to-One‚Äù model\nview+channel attention model\n1.2. Transformer-based method Use attention to model long-range spatial-angular dependency.\nSpatial‚Äìangular transformer: Self-attention along EPI lines to capture parallax geometry. Volume and cross-view transformers: Model correlations across many viewpoints. Multi-scale angular transformer: Robust to disparity variations. EPI attention\nvolume transformer and cross-view transformer\nLF-DET\n2. Light field angular super-resolution (LFASR) Increase number of viewpoints (more SAIs) while preserving geometry.\n2.1. Depth-dependent method Estimate depth/disparity ‚Üí warp existing views ‚Üí blend new views.\nOptical-flow and superpixel-based warping Layered depth representations EPI-based geometry modeling Depth-guided warping with occlusion reasoning 2.2. Depth-independent method Avoid explicit depth; rely on signal priors or learning-based angular patterns.\nCNN-based angular detail restoration (on EPIs) Angular attention models to reconstruct views 3. Light field spatial-angular super-resolution (LFSASR) Simultaneously increase both spatial and angular resolution ‚Üí full 4D reconstruction.\n3.1. Deep learning-based method Jointly model 4D light field structure (geometry + appearance).\n4D CNN encoder‚Äìdecoder Pseudo-4D convolution combining EPI + spatial-angular blocks Disentangled models: separate spatial and angular subspaces EPI-based networks (CNN + LSTM) preserving geometry Self-supervised or domain-generalized models for wild light fields 3D encoder\nEPI-based networks\nExperiments LFSSR CNN-based: Convolutions, 4D/sep-conv, optical-flow alignment, attention fusion. Transformer-based: Global spatial‚Äìangular modeling, EPI attention, multi-scale. LFASR Depth-dependent: Estimate depth ‚Üí warp ‚Üí blend new views. Depth-independent: Fourier/shearlet priors, CNN restoration on EPI, angular attention. LFSASR Deep learning-based: 4D CNNs, disentangled spatial‚Äìangular modeling, EPI networks, high-order residual networks. ","permalink":"/notes/a_survey_for_light_field_super-resolution/","summary":"\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eThree methods of Light Field Super-Resolution (LFSR):\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLight field \u003cstrong\u003espatial\u003c/strong\u003e super-resolution (LFSSR)\u003c/li\u003e\n\u003cli\u003eLight field \u003cstrong\u003eangular\u003c/strong\u003e super-resolution (LFASR)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLight field spatial and angular super-resolution (LFSASR)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eExperiments\u003c/strong\u003e for each method.\u003c/p\u003e\n\u003c/aside\u003e\n\u003ch1 id=\"1-light-field-spatial-super-resolution-lfssr\"\u003e1. Light field spatial super-resolution (LFSSR)\u003c/h1\u003e\n\u003cp\u003eImprove \u003cstrong\u003espatial resolution\u003c/strong\u003e of each sub-aperture image (\u003cstrong\u003eSAI\u003c/strong\u003e).\u003c/p\u003e\n\u003ch2 id=\"11-cnn-based-method\"\u003e1.1. CNN-based method\u003c/h2\u003e\n\u003cp\u003eUse convolutions to learn spatial / angular correlations and fuse high-frequency details.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResidual CNNs\u003c/strong\u003e: Learn directional features and fuse sub-pixel details (e.g., resLF).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFeature alignment\u003c/strong\u003e: Optical-flow-based alignment, deformable conv alignment.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e4D / separable CNNs\u003c/strong\u003e: Use 4D conv or spatial‚Äìangular separable conv to jointly extract features.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eView fusion models\u003c/strong\u003e: ‚ÄúAll-to-One‚Äù, multi-view complementary information fusion.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAttention-based CNNs\u003c/strong\u003e: Channel/view attention, angular deformable alignment.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"resLF\" loading=\"lazy\" src=\"/notes/a_survey_for_light_field_super-resolution/image.png\"\u003e\u003c/p\u003e","title":"A survey for light field super-resolution"},{"content":"Introduction Most models (RNNs, CNNs, Transformers) cannot handle very long sequences well. They either forget, see too little context, or are too slow. This makes long-range dependency tasks hard to solve.\nState Space Models (SSMs) can solve this issue by remembering long information. But the old version (LSSL) used too much time and memory, so it was not practical.\nNew solution S4 fixes this by changing how the SSM is built. It splits the main matrix into simple parts and computes in a faster, more stable way. Now it runs much faster and uses much less memory.\nBackground: State Spaces State Space Models (SSMs) describe how input changes hidden states and produces output. They learn parameters A, B, C, D automatically. Usually, D is ignored because it‚Äôs just a shortcut.\n$$ x\u0026rsquo;(t) = A x(t) + B u(t) \\\\ y(t) = C x(t) + D u(t) $$\nThe basic SSM forgets or explodes on long sequences. HiPPO fixes this by giving a special matrix for A that helps the model remember long-term information.\nWe mainly focus on optimizing A because A is the bottleneck ‚Äî it controls how the model remembers information over time. B and C are much simpler ‚Äî they only handle input and output scaling.\n$$ A_{nk} = \\begin{cases} -(2n + 1)^{1/2}(2k + 1)^{1/2}, \u0026amp; \\text{if } n \u0026gt; k \\\\ -(n + 1), \u0026amp; \\text{if } n = k \\\\ 0, \u0026amp; \\text{if } n \u0026lt; k \\end{cases} $$\nReal data comes in steps, so the SSM is turned into a discrete version that works like an RNN. To train faster, the model is rewritten as a convolution, which processes all inputs in parallel. The convolution weights are called the SSM kernel (K). (More details move to More Details)\nMethod: Structured State Spaces (S4) S4 is a new way to make State Space Models (SSMs) fast and stable. The technical results focus on developing the S4 parameterization and showing how to efficiently compute all views of the SSM. It connects three forms of SSMs ‚Äî continuous, recurrent, and convolutional ‚Äî in one model.\nThe old method was slow because it multiplied a big matrix many times. S4 fixes this by rewriting the matrix as Normal + Low-Rank, which is easier to compute. This design makes S4 very efficient:\nRecurrent form: O(N) time per step Convolution form: O(N + L) total time Each S4 layer has trainable parts (Œõ, P, Q, B, C) and works like a CNN or Transformer layer. Stacking layers with activations makes it powerful for long-sequence data.\n$$ A = V \\Lambda V^{} - P Q^{\\top} = V \\left( \\Lambda - (V^{}P)(V^{}Q)^{} \\right) V^{*} $$\nConclusion S4 is based on the State Space Model (SSM), which works like an RNN ‚Äî it keeps a hidden state and updates it over time.\nThe problem with the old version is that the state update matrix A is hard to compute and unstable for long sequences. S4 solves this by rewriting A into a new, easier form:\n$$ A = V \\Lambda V^{*} - P Q^{\\top} $$\nThis form (called Normal + Low-Rank) makes A stable, efficient, and fast to compute. As a result, S4 keeps the good memory ability of RNNs but runs much faster and handles much longer sequences.\nMore Details Step 1: The original state space equations Continuous-time form:\n$$ x\u0026rsquo;(t) = A x(t) + B u(t) $$\n$$ y(t) = C x(t) $$\nDiscrete-time version (after discretization):\n$$ x_k = \\bar{A} x_{k-1} + \\bar{B} u_k $$\n$$ y_k = \\bar{C} x_k $$\nHere:\n$x_k$: memory or hidden state at step $k$ $u_k$: input at step $k$ $y_k$: output at step $k$ $\\bar{A}, \\bar{B}, \\bar{C}$: system parameters This is a recurrent process ‚Äî to get $x_k$, we need $x_{k-1}$. So it must be done step-by-step, like an RNN ‚Üí no parallelization.\nStep 2: Expand the recurrence We can ‚Äúunroll‚Äù the recurrence to express $x_k$ directly in terms of all previous inputs:\n$$ x_k = \\bar{A}^k x_0 + \\sum_{i=0}^{k-1} \\bar{A}^i \\bar{B} u_{k-i} $$\nIf we ignore the initial state $(x_0 = 0)$, then:\n$$ x_k = \\sum_{i=0}^{k-1} \\bar{A}^i \\bar{B} u_{k-i} $$\nNow substitute this into $y_k = \\bar{C} x_k$:\n$$ y_k = \\bar{C} \\sum_{i=0}^{k-1} \\bar{A}^i \\bar{B} u_{k-i} $$\nRewriting it:\n$$ y_k = \\sum_{i=0}^{k-1} (\\bar{C}\\bar{A}^i\\bar{B}) u_{k-i} $$\nStep 3: Define the kernel Let‚Äôs define a kernel $\\bar{K}_i = \\bar{C}\\bar{A}^i\\bar{B}$.\nThen the output becomes:\n$$ y_k = \\sum_{i=0}^{k-1} \\bar{K_i} u_{k-i} $$\nThat‚Äôs exactly a 1D convolution:\n$$ y = \\bar{K} * u $$\nSo instead of computing step-by-step updates of $x_k$, we can compute all outputs at once using convolution.\nStep 4: Parallelization using FFT There is a mathematical truth from signal processing:\n‚Üí A convolution in time is equal to a multiplication in frequency.\nA convolution in time domain can be computed efficiently in frequency domain (via FFT):\n$$ F(y) = F(\\bar{K} * u) = F(\\bar{K}) \\odot F(u) $$\n(where $\\odot$ means elementwise multiplication.)\nThen invert it back:\n$$ y = F^{-1}(F(\\bar{K}) \\odot F(u)) $$\nUsing FFT, this whole computation is O(L log L) instead of O(L¬≤), and it can be done for all time steps in parallel ‚Äî no need to wait for $x_{k-1}$.\nStep 5: Why S4 can do this efficiently In normal SSMs, computing each $\\bar{A}^i\\bar{B}$ is expensive ‚Üí O(N¬≤L). S4 makes it efficient by reparameterizing A as:\n$$ A = V \\Lambda V^{*} - P Q^{\\top} $$\nwhich allows fast and stable computation of $\\bar{K}$ using frequency-space math (Cauchy kernel + Woodbury identity). That‚Äôs how S4 converts a recurrent model into a parallelizable convolutional model while keeping the same meaning.\n","permalink":"/notes/efficiently_modeling_long_sequences_with_structure/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eMost models (RNNs, CNNs, Transformers) cannot handle \u003cstrong\u003every long sequences\u003c/strong\u003e well. They either forget, see too little context, or are too slow. This makes long-range dependency tasks hard to solve.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eState Space Models (SSMs)\u003c/strong\u003e can solve this issue by remembering long information. But the old version (LSSL) used too much \u003cstrong\u003etime and memory\u003c/strong\u003e, so it was not practical.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"/notes/efficiently_modeling_long_sequences_with_structure/image.png\"\u003e\u003c/p\u003e\n\u003cp\u003eNew solution \u003cstrong\u003eS4\u003c/strong\u003e fixes this by changing how the SSM is built. It splits the main matrix into simple parts and computes in a faster, more stable way. Now it runs \u003cstrong\u003emuch faster\u003c/strong\u003e and uses \u003cstrong\u003emuch less memory\u003c/strong\u003e.\u003c/p\u003e","title":"Efficiently Modeling Long Sequences with Structured State Spaces"},{"content":"Introduction Transformer was introduced for its strong performance and efficient parallel training, but its inference cost remains high. This creates an ‚Äúimpossible triangle‚Äù, where the three dimentions can not be balanced simultaneously. Retentive networks (RetNet) make it possible!\nMethod For an L-layer retention network, the author stacks multi-scale retention (MSR) and feed-forward network(FFN) to build the model.\nFormally, the input sequence $x_i$ is transformed to vectors by a word embedding layer. Use the packed embeddings $X^0 = [x_1, ¬∑ ¬∑ ¬∑ , x_{|x|}] ‚àà R^{|x|√ód_{model}}$ as the input and compute the model output $X^L$:\n$$ Y^l = MSR(LN(X^l)) +X^l $$\n$$ X^{l+1} = FFN(LN(Y^l)) +Y^l $$\nwhere LN(¬∑) is LayerNorm. The FFN part is computed as $FFN(X) = gelu(XW_1)W_2$, where W1, W2 are parameter matrices.\nRetention is a new way for a model to remember previous tokens ‚Äî similar to attention, but faster. It combines parallel and recurrent forms:\nDuring training, it runs in parallel (like attention, efficient on GPUs). During inference, it runs recurrently, step by step, like an RNN. The core idea:\nInstead of learning complex attention weights with softmax, Retention uses a simple exponential decay to remember past information which have already combined the weighted values from past tokens. Form How it computes Best for Speed Memory use Key idea Parallel All tokens at once Training ‚ö° Fast üíæ High Full GPU parallelism Recurrent One token at a time Inference üê¢ Slower üß† Very low Step-by-step memory Chunkwise In chunks (hybrid) Long sequences ‚öñÔ∏è Balanced ‚öñÔ∏è Medium Parallel + Recurrent combo 1. Parallel representation (for training) $$ \\text{Retention}(X) = (QK^T \\odot D)V $$\nThis version computes all tokens at once using GPU-friendly matrix operations.\nIt‚Äôs equivalent to the unrolled version of the recurrence:\n$$ s_n = \\sum_{m=1}^{n} Œ≥^{n-m} K_m^T V_m $$\nIt‚Äôs called parallel because every token‚Äôs output $o_n$ can be computed in parallel.\n‚úÖ Best for training ‚Äî fast on GPUs, easy for backpropagation\n‚ùå Needs more memory (because you must store all tokens)\n2. Recurrent representation (for inference) $$ s_n = Œ≥ s_{n-1} + K_n^T V_n, \\quad o_n = Q_n s_n $$\nThis computes one token at a time, keeping a single running memory $s_n$. It doesn‚Äôt need to access all past tokens ‚Äî only the previous memory. So it‚Äôs much more memory-efficient. ‚úÖ Best for inference / streaming ‚Äî efficient step-by-step processing\n‚ùå Slower for training because you can‚Äôt parallelize easily\n3. Chunkwise recurrent representation (for long sequences) $$ \\text{Parallel inside each chunk}, \\quad \\text{Recurrent between chunks} $$\nFor long sequences (e.g., thousands of tokens), full parallel mode is too big for GPU memory. So RetNet splits the input into chunks (e.g., 512 tokens). Inside each chunk ‚Üí compute in parallel (fast). Between chunks ‚Üí pass memory recurrently (keep context). ‚úÖ Best trade-off ‚Äî combines GPU efficiency with long-context capability\n‚ùå Slightly more complex to implement\n","permalink":"/notes/retentive_network_a_successor_to_transformer_for_l/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eTransformer was introduced for its strong performance and efficient parallel training, but its inference cost remains high. This creates an ‚Äúimpossible triangle‚Äù, where the three dimentions can not be balanced simultaneously. \u003cstrong\u003eRetentive networks (RetNet)\u003c/strong\u003e make it possible!\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image.png\" loading=\"lazy\" src=\"/notes/retentive_network_a_successor_to_transformer_for_l/image.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"method\"\u003eMethod\u003c/h1\u003e\n\u003cp\u003eFor an L-layer retention network, the author stacks \u003cstrong\u003emulti-scale retention (MSR)\u003c/strong\u003e and \u003cstrong\u003efeed-forward network(FFN)\u003c/strong\u003e to build the model.\u003c/p\u003e\n\u003cp\u003eFormally, the input sequence $x_i$ is transformed to vectors by a word embedding layer. Use the packed embeddings $X^0 = [x_1, ¬∑ ¬∑ ¬∑ , x_{|x|}] ‚àà R^{|x|√ód_{model}}$ as the input and compute the model output $X^L$:\u003c/p\u003e","title":"Retentive Network: A Successor to Transformer for Large Language Models"},{"content":"Introduction An LF records both intensity information and directional information of all light rays. The currently popular representation for four-dimensional(4D) LF is the two-plane parametrization.\nSince the resolution of the device sensor is fixed, there is a trade-off between spatial and angular resolution. As a result, LF spatial super-resolution (LF-SSR) which aims to reconstruct high-resolution (HR) LF from its LR counterpart has attracted lots of attention.\nTo achieve high-quality LF-SSR performance, the key is to make full use of angular information which is not available in SISR.\nFor the CNNs, the local nature of convolutional filters raises a fundamental limitation in accessing global dependency. For the transformer, it provides global context modeling by self-attention mechanism, but the global interaction comes at a quadratic computational complexity cost. In addition, vision transformers are widely used to process 2D images rather than the complex 4D LFs. More efforts are needed to exploit an effective way to implement fully attentive features by transformers.\nThe paper proposes a new transformer-based network called LF-DET for light field spatial super-resolution. It first extracts local information from each sub-aperture image using convolution layers, then applies a spatial‚Äìangular separable transformer to model global context along both spatial and angular dimensions. The sub-sampling spatial modeling reduces computation cost, while the multi-scale angular modeling handles different disparity ranges effectively. Finally, hierarchical features from multiple transformer encoders are fused for high-quality reconstruction. LF-DET provides a flexible balance between model size, accuracy, and efficiency, and experiments show it achieves superior performance over existing methods.\nMethod Spatial‚ÄìAngular Separable Transformer Encoder A standard transformer handles 1D token sequences, but a light field (LF) image is 4D $H \\times W \\times U \\times V$. Flattening it into 1D causes high computational and memory costs. The vanilla transformer cannot efficiently process such large data. To solve this, a spatial‚Äìangular separable transformer encoder is designed. It includes two main parts: sub-sampling spatial modeling and multi-scale angular modeling.\nA. Sub-Sampling Spatial Modeling High-resolution SAIs lead to heavy computation and memory use. To handle this, a sub-sampling convolution is used before the multi-head self-attention (MSA) to reduce token number.\nGiven LF features $F_a \\in R^{C \\times H \\times W}$, we reshape them to embeddings $x_a \\in R^{(HW) \\times C}$.\nThe queries, keys, and values are calculated as:\n$$ Q_a = x_a W_Q, \\quad K_a = x_{a_{sub}} W_K, \\quad V_a = x_{a_{sub}} W_V $$\nwhere $W_Q, W_K, W_V \\in R^{C \\times C}$.\nThe self-attention for the (p)-th subspace is:\n$$ SA_p(Q_{a,p}, K_{a,p}, V_{a,p}) = softmax \\left( \\frac{Q_{a,p}(K_{a,p})^T}{\\sqrt{C/P}} \\right)V_{a,p} $$\nAfter concatenating all subspaces, the result is:\n$$ \\tilde{x}_a = [SA_1, SA_2, \\ldots, SA_P]W_P + x_a $$\nwhere $W_P \\in R^{C \\times C}$. This sub-sampling strategy reduces computation by a ratio of $S^2$.\nSince attention ignores token order, a 3√ó3 convolution is added in FFN to insert positional encoding and local information:\n$$ x_a = MLP(GELU(DWConv(MLP(\\tilde{x}_a)))) + \\tilde{x}_a $$\nwhere DWConv is depth-wise convolution and GELU is the activation. This introduces spatial locality to the transformer.\nFinally, $x_a \\in R^{(HW) \\times C}$ represents the spatial transformer output, and $K$ such transformers are cascaded to enhance global spatial features.\nB. Multi-Scale Angular Modeling To model angular information, angular transformers process macro-pixels. Since objects with different disparities are misaligned across views, the network analyzes different macro-pixel scales.\nFor a downsampling factor $\\alpha$, the valid disparity range in a region of $M \\times M$ macro-pixels is $[-\\alpha M, \\alpha M]$. Larger $M$ captures larger disparities but increases cost and mismatch, so three scales (M=1,2,3) are used. Each $M \\times M$ macro-pixel group is reshaped into sequences $x_s^M \\in R^{(M^2UV) \\times C}$.\nThe attention mechanism is similar to the spatial transformer:\n$$ Q_s^M = x_s^M W_Q, \\quad K_s^M = x_s^M W_K, \\quad V_s^M = x_s^M W_V $$\n$$ SA_p(Q_{s,p}^M, K_{s,p}^M, V_{s,p}^M) = softmax \\left( \\frac{Q_{s,p}^M (K_{s,p}^M)^T}{\\sqrt{C/P}} \\right)V_{s,p}^M $$\n$$ \\tilde{x}_s^M = [SA_1, SA_2, \\ldots, SA_P]W_P + x_s^M $$\nAfter FFN with DWConv, the final angular feature is:\n$$ x_s^M = MLP(GELU(DWConv(MLP(\\tilde{x}_s^M)))) + \\tilde{x}s^M $$\nEach scale outputs a feature map $F_{MacPI}^M \\in R^{C \\times H_U \\times W_V}$ focusing on a specific disparity range.\nC. Spatial Attention Fusion The three angular feature maps are concatenated and passed through a 1√ó1 convolution and softmax to compute attention weights:\n$$ W = softmax(H_{1\\times1}[F_{MacPI}^1, F_{MacPI}^2, F_{MacPI}^3]) $$\nThe weights are split and applied to each feature map, producing the fused output:\n$$ \\overline{F}{MacPI} = \\sum{M=1}^3 W^M F_{MacPI}^M $$\nThis adaptive fusion enables the model to handle both small and large disparity regions effectively.\nNetwork Architecture LF-DET is a transformer-based network for light field super-resolution. The input light field $L^{LR} \\in R^{U \\times V \\times H \\times W}$ is converted from RGB to YCbCr, and only the Y channel is used. The goal is to reconstruct a high-resolution output $L^{HR} \\in R^{U \\times V \\times \\alpha H \\times \\alpha W}$, where $\\alpha$ is the upsampling factor. The Cb and Cr channels are upsampled by bicubic interpolation.\nThe network has an encoder‚Äìdecoder structure. The encoder extracts local and global features, and the decoder fuses them through a Hierarchical Feature Aggregation (HFA) module. The final upsampling block generates the high-resolution light field.\nA. Local Feature Extraction Local context is important for super-resolution. The vanilla transformer cannot model local dependencies well. To fix this, LF-DET uses several 3√ó3 convolution layers with Leaky ReLU activations.\nEach SAI is processed by one convolution and three stacked convolutions, followed by a residual connection. This produces the initial local features $F_a^0 \\in R^{C \\times H \\times W}$. The same convolution weights are shared across all views.\nB. Global Feature Extraction Global spatial and angular information is modeled using spatial‚Äìangular separable transformer encoders. Each encoder first performs sub-sampling spatial modeling, then reshapes the data to the MacPI pattern for angular modeling.\nThe process for the (n)-th encoder is:\n$$ F_a^n = R_{LF_2}(H_{MAM}^n(R_{LF_1}(H_{SSM}^n(F_a^{n-1})))) $$\nThese encoders are stacked sequentially to extract deep and global correlations across views.\nC. Hierarchical Feature Aggregation \u0026amp; Upsampling The HFA module merges features from all encoder layers. Shallow and deep features are fused by element-wise addition and refined by three 3√ó3 convolution layers.\nThe overall fusion is:\n$$ \\overline{F_a} = [H_{FE}^1 \\sum_{i=1}^N F_a^i, H_{FE}^2 \\sum_{i=2}^N F_a^i, \\ldots, H_{FE}^N \\sum_{i=N}^N F_a^i] $$\nThen, the fused feature $\\overline{F}_a$ goes through an upsampling block with two convolutions, a pixel-shuffle layer, and Leaky ReLU activation. The pixel-shuffle enlarges the resolution from $H \\times W$ to $\\alpha H \\times \\alpha W$, and the final 3√ó3 convolution produces the output Y-channel image.\n","permalink":"/notes/exploiting_spatial_and_angular_correlations_with_d/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eAn LF records both \u003cstrong\u003eintensity information\u003c/strong\u003e and \u003cstrong\u003edirectional information of all light rays\u003c/strong\u003e. The currently popular representation for \u003cstrong\u003efour-dimensional(4D)\u003c/strong\u003e LF is the two-plane parametrization.\u003c/p\u003e\n\u003cp\u003eSince the resolution of the device sensor is fixed, there is a trade-off between spatial and angular resolution. As a result, \u003cstrong\u003eLF spatial super-resolution (LF-SSR)\u003c/strong\u003e which aims to reconstruct high-resolution (HR) LF from its LR counterpart has attracted lots of attention.\u003c/p\u003e\n\u003cp\u003eTo achieve high-quality LF-SSR performance, the key is to make full use of \u003cstrong\u003eangular information\u003c/strong\u003e which is not available in SISR.\u003c/p\u003e","title":"Exploiting Spatial and Angular Correlations With Deep Efficient Transformers for Light Field Image Super-Resolution"},{"content":"Introduction Large Transformer models achieve strong generalization and reasoning ability, but their huge size requires massive memory and computation. This makes them hard to train and deploy outside large data centers. To address this, researchers look for efficient architectures that save parameters or computation. Two main directions are parameter efficiency, which reduces or shares model weights, and adaptive computation, which uses more compute only when necessary.\nA common way to improve parameter efficiency is layer tying, where the same set of weights is reused across multiple layers. For adaptive computation, methods like early exiting let the model stop processing simple tokens earlier. However, most existing models treat these two goals separately. A unified design that combines both efficiency types has been missing. Recursive Transformers reuse one set of layers multiple times, naturally achieving weight sharing. Yet, previous recursive models often fix the number of recursions for all tokens. This wastes computation because every token receives the same processing depth, regardless of difficulty. Attempts at dynamic recursion have also faced training and efficiency challenges.\nTo solve these issues, the Mixture-of-Recursions (MoR) framework was proposed. It introduces a router that learns how many recursive steps each token needs. Easy tokens stop early, while complex ones go through more recursions. This provides true token-level adaptive depth. MoR also includes key‚Äìvalue (KV) caching, storing results from previous recursions to reduce memory use and improve throughput.\nOne Transformer layer = Self-Attention + Feed-Forward + Normalization + Residual connections.\nConceptually, MoR provides a model to ‚Äúthink‚Äù recursively inside its latent space during decoding of each token. It adjusts its reasoning depth per token instead of using a fixed number of layers. In this way, MoR unifies parameter efficiency and adaptive computation in one efficient Transformer architecture.\nThrough this design, MoR achieves three goals at once:\nWeight sharing reduces model parameters. Dynamic routing saves computation by skipping redundant steps. KV caching lowers memory traffic and speeds up inference. Methods Mixture-of-Recursions (MoR)‚Äîa framework that dynamically adjusts recursion step for each token during pretraining and inference.\nThe core of MoR lies in two components:\na routing mechanism that assigns token-specific recursion steps to adaptively concentrate computation on more challenging tokens; a KV caching strategy that defines how KV pairs are stored and selectively utilized for attention at each recursive step. 1. Routing Stategies: Expert-choice vs. Token-choice Expert-choice routing each recursion ‚Äîselect‚Üí top-k tokens\nIn expert-choice routing, each recursion depth is treated as an expert. At every recursion step $r$, only the top-k tokens (with the highest scores) are selected to pass through that expert‚Äôs recursion block.\nThe router computes a scalar routing score for each token:\n$$ g_t^r = G(\\theta_r^\\top H_t^r) $$\nwhere $\\mathcal{G}$ is an activation function (e.g., sigmoid or tanh).\nThen, tokens with scores above the $\\beta$-percentile threshold $P_\\beta(G^r)$ are selected to proceed:\n$$ H_t^{r+1} = \\begin{cases} g_t^r f(H_t^r, \\Phi\u0026rsquo;) + Ht^r, \u0026amp; \\text{if } g_t^r \u0026gt; P_\\beta(G^r) \\\\ H_t^r, \u0026amp; otherwise \\end{cases} $$\n$r$: recursion steps $t$: token This selective routing makes each recursion act like a different ‚Äúexpert,‚Äù and tokens move deeper only when needed. A mechanism called hierarchical filtering ensures that tokens selected at one step can still be re-evaluated at later steps, supporting early-exit-like behavior while training from scratch.\nToken-choice routing each token ‚Äîselect‚Üí recursion depth\nIn token-choice routing, each token independently chooses which recursion expert it wants to follow for all subsequent steps. Unlike expert-choice, routing happens once per token, not at every recursion step.\nGiven the hidden state $\\mathcal{H}_t^1$ at the first layer, the router computes routing scores for all experts $N$:\n$$ g_t^j = \\mathcal{G}(\\theta_r^\\top \\mathcal{H}_t^1), \\quad j \\in {1, \\dots, N_r} $$\nEach token selects its expert $i$ with the highest score ($i$ = the recursion depth of the token go through):\n$$ i = \\arg\\max_j g_t^j $$\nand applies that recursion block $i$ times.\nThe hidden state is updated recursively as:\n$$ H_t^{r+1} = \\begin{cases} g_t^r f(H_t^r, \\Phi\u0026rsquo;) + H_t^1, \u0026amp; \\text{if } r = i, last~recursion \\\\ g_t^r f(H_t^r, \\Phi\u0026rsquo;), \u0026amp; otherwise \\end{cases} $$\nThis approach avoids information leakage and makes each token commit to a fixed recursion path, though it may cause load imbalance among experts.\nFor Expert-choice:\nHierarchical filtering keeps recursion steps causally ordered in data flow ‚Äî each step selects tokens only from the previous one.\nHowever, during parallelized training, all recursion steps are computed at once, so gradients from deeper steps can flow backward and influence earlier routers.\nThis introduces training-time causality violation ‚Äî even though the hierarchical filtering itself maintains structural causality.\n2. KV Caching Strategies: Recursion-wise Caching vs. Recursive sharing Dynamic-depth models face challenges with KV cache consistency during autoregressive decoding. When a token exits early, its keys and values from deeper recursion steps are missing, which causes incomplete context for later tokens. Previous methods tried to reuse or recompute these entries but introduced extra complexity. To address this, MoR proposes two efficient strategies: recursion-wise KV caching and recursive KV sharing.\nIn recursion-wise KV caching, only the tokens selected for a specific recursion step store their key‚Äìvalue pairs at that level. The cache size at each depth depends on how many tokens are routed there. Attention computation is restricted to these locally cached tokens. This makes computation more localized, saving memory and reducing input/output operations.\nIn recursive KV sharing, all tokens share the KV pairs produced at the first recursion block. These cached pairs are reused by all later recursion steps, so each step still has access to the full sequence context. Even though fewer tokens may continue deeper, their keys and values still represent the whole sequence, preventing missing-context problems.\nRecursion-wise caching reduces KV memory and IO usage roughly by a factor of $(N_r+1)/(2N_r)$ across the model and decreases attention FLOPs, making both training and inference more efficient. Recursive sharing saves even more memory by globally reusing context, though it provides less FLOP reduction and still faces IO bottlenecks during decoding. Overall, both strategies improve efficiency but trade off between local memory savings (recursion-wise) and global context reuse (recursive sharing).\nMore Details Parameter-sharing strategies in Recursive Transformers.\nSo an equation such as\n$$ f(h_t^{\\ell}; \\Phi\u0026rsquo;_{\\ell \\bmod (L/N_r)}) $$\nmeans:\ntake the token representation $h_t^{\\ell}$, apply a layer function $f(\\cdot)$ using parameters $\\Phi\u0026rsquo;$, and reuse weights cyclically according to the current recursion index $\\ell$. Results MoR outperforms baselines with fewer parameters under equal train compute. MoR outperforms baselines with less compute at equal data. MoR performance varies with routing and caching strategies. Conclusion Mixture-of-Recursions (MoR) is a Transformer model that combines parameter sharing, adaptive recursion depth, and efficient KV caching. It uses routers to decide how many recursive steps each token needs and stores KV pairs only for active tokens. This design cuts unnecessary computation and memory use while keeping strong performance. Experiments show that MoR achieves lower perplexity, higher few-shot accuracy, and faster inference than standard Transformers or earlier recursive models.\nFuture Work Future work will focus on improving reasoning ability. Since MoR already adapts depth per token, it can be trained to adjust recursion depth based on reasoning difficulty, helping the model handle chain-of-thought tasks better.\nMoR can also be scaled to larger models. Future versions will train models with over 3B parameters and may use depth-specific LoRA, expert modules, or expert parallelism to improve performance without slowing inference. Reusing pre-trained LLMs could further reduce training cost.\nAnother goal is to make MoR more flexible during inference. The current router outputs are too sharp, making it hard to change capacity or top-k values after training. New routing methods are needed for dynamic control.\nMoR can also benefit from sparsity methods like pruning and quantization to skip unnecessary computation, improving efficiency further.\nFinally, MoR‚Äôs adaptive recursion is not limited to text. It can be extended to vision, speech, and multimodal models. Adjusting depth for different tokens or segments could make processing long videos or audio more efficient in both memory and speed.\n","permalink":"/notes/mixture-of-recursions_learning_dynamic_recursive_d/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eLarge Transformer models achieve strong generalization and reasoning ability, but their huge size requires massive memory and computation. This makes them hard to train and deploy outside large data centers. To address this, researchers look for \u003cstrong\u003eefficient architectures\u003c/strong\u003e that save parameters or computation. Two main directions are \u003cstrong\u003eparameter efficiency\u003c/strong\u003e, which reduces or shares model weights, and \u003cstrong\u003eadaptive computation\u003c/strong\u003e, which uses more compute only when necessary.\u003c/p\u003e\n\u003cp\u003eA common way to improve parameter efficiency is \u003cstrong\u003elayer tying\u003c/strong\u003e, where the same set of weights is reused across multiple layers. For adaptive computation, methods like \u003cstrong\u003eearly exiting\u003c/strong\u003e let the model stop processing simple tokens earlier. However, most existing models treat these two goals separately. A unified design that combines both efficiency types has been missing. \u003cstrong\u003eRecursive Transformers\u003c/strong\u003e reuse \u003cstrong\u003eone set of layers\u003c/strong\u003e multiple times, naturally achieving weight sharing. Yet, previous recursive models often fix the number of recursions for all tokens. This wastes computation because every token receives the same processing depth, regardless of difficulty. Attempts at dynamic recursion have also faced training and efficiency challenges.\u003c/p\u003e","title":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation"},{"content":"Source code: https://github.com/varun-jois/FSRST\nIntroduction Face super-resolution aims to reconstruct high-resolution (HR) facial images from low-resolution (LR) inputs, enhancing fine details. This task is challenging because it is ill-posed: many possible HR outputs can correspond to the same LR image.\nTo reduce ambiguity, Reference-Based Super-Resolution (RefSR) introduces external HR reference images that share similar content (e.g., same person‚Äôs other photos). The model can then use these reference textures and shapes to guide reconstruction.\nHowever, RefSR introduces two main challenges:\nAlignment problem ‚Äì matching facial structures between LR input and HR reference images. Information aggregation problem ‚Äì determining how much and which parts of each reference to use. Traditional alignment methods (like deformable convolutions) are powerful but unstable and hard to train. To overcome this, the authors propose FSRST, which uses a Spatial Transformer Network (STN) for stable alignment and a distance-based weighted aggregation for effective information fusion.\nMethod The proposed Face Super-Resolution using Spatial Transformer (FSRST) is an end-to-end model with four components:\nFeature Extractor:\nExtracts features from both the LR input and HR references using residual blocks. Reference images are converted to grayscale and reshaped (space-to-depth) to match the LR resolution.\nSpatial Transformer Alignment (STA):\nReplaces unstable deformable convolutions with a Spatial Transformer module. It predicts an affine transformation that aligns each reference‚Äôs features with those of the LR image. This alignment is differentiable and stable, ensuring good correspondence between LR and HR feature spaces.\nDistance-Based Weighted Aggregation (DWA):\nAfter alignment, the model computes the L2-distance between each aligned reference feature and the LR feature. A softmax weighting gives higher importance to more similar references, while irrelevant ones are ignored. This allows the model to dynamically use helpful references or fall back to single-image SR when references are poor.\nOutput Constructor:\nCombines aggregated and LR features and passes them through multiple residual blocks and sub-pixel convolution for upsampling. The model predicts a residual image, added to a bicubic-upsampled LR image to produce the final HR output.\nConclusion The FSRST model introduces a stable and efficient alternative to deformable alignment for reference-based face super-resolution.\nIts Spatial Transformer alignment provides consistent and accurate correspondence, while the distance-based aggregation flexibly handles multiple references.\nExperiments on DFD, CelebAMask-HQ, and VoxCeleb2 datasets show that FSRST outperforms previous methods like TTSR, C2-Matching, MRefSR, and HIME ‚Äî achieving higher PSNR/SSIM with fewer parameters.\nAlthough the STN-based alignment is not fully convolutional (fixed input size), the method is lightweight, effective, and adaptable for real-time applications such as video conferencing.\nFuture work aims to extend this model to video super-resolution and make the alignment module fully convolutional.\n","permalink":"/notes/reference-based_face_super-resolution_using_the_sp/","summary":"\u003cp\u003eSource code: \u003ca href=\"https://github.com/varun-jois/FSRST\"\u003ehttps://github.com/varun-jois/FSRST\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eFace super-resolution aims to reconstruct high-resolution (HR) facial images from low-resolution (LR) inputs, enhancing \u003cstrong\u003efine details\u003c/strong\u003e. This task is challenging because it is \u003cstrong\u003eill-posed\u003c/strong\u003e: many possible HR outputs can correspond to the same LR image.\u003c/p\u003e\n\u003cp\u003eTo reduce ambiguity, \u003cstrong\u003eReference-Based Super-Resolution (RefSR)\u003c/strong\u003e introduces external HR reference images that share similar content (e.g., same person‚Äôs other photos). The model can then use these reference textures and shapes to guide reconstruction.\u003c/p\u003e","title":"Reference-Based Face Super-Resolution Using the Spatial Transformer"},{"content":"Introduction Single image super-resolution (SISR) aims to restore a low-resolution (LR) image into a high-resolution (HR) image with realistic textures. With the growth of deep learning, SISR performance has greatly improved in recent years.\nCompared to SISR, reference-based super-resolution (RefSR) makes use of additional high-resolution reference images that share similar textures with the target image. This allows RefSR to generate more detailed and realistic results. Because of these promising results, many researchers have focused on RefSR methods in recent years.\nHowever, all previous RefSR methods are trained with only a single reference image. In practice, there are often multiple reference images available, such as in the CUFED5 dataset, where each LR image has five reference images of varying similarity. Yet, CUFED5‚Äôs training set provides only one reference per LR input and contains relatively few and small images. As a result, previous methods cannot effectively use multiple references. To handle multiple references, they often merge them into one large image, which consumes much GPU memory and ignores the relationships between references.\nTherefore, a new dataset and method are needed for multi-reference super-resolution. To address this gap, the authors introduce a large-scale multi-reference dataset named LMR, containing 112,142 groups of 300√ó300 training images, each with five reference images. This dataset is ten times larger than CUFED5 and supports better generalization.\nBased on LMR, the authors propose a new method called MRefSR. It introduces two key components:\nMulti-Reference Attention Module (MAM) ‚Äì fuses features from multiple reference images by treating the LR input as the query and aligned reference features as keys and values. Spatial Aware Filtering Module (SAFM) ‚Äì selects the most relevant fused features to refine the output. Overall, the work contributes (1) the first large-scale multi-reference RefSR dataset, (2) a new baseline method MRefSR designed for multiple references, and (3) experimental results showing strong improvements over existing methods.\nMethod Dataset: Large-scale Multi-reference RefSR dataset LMR\nMethod: MRefSR\nConstruction of LMR The LMR dataset is built based on the MegaDepth dataset, which was originally created for single-view depth prediction. MegaDepth collected over one million Internet photos of landmarks and used COLMAP, a Structure-from-Motion (SfM) and Multi-View Stereo (MVS) system, to reconstruct 3D models and dense depth maps. Each landmark includes many photos taken from different viewpoints, making it suitable for creating image groups with overlapping content, just like reference-based super-resolution (RefSR) requires.\nTo construct the LMR dataset, the authors first preprocessed MegaDepth to form image pairs with controlled similarity. They used three filtering rules:\nThe PSNR between the target and candidate reference images must be lower than 30 dB to remove duplicates. The two images must share similar content, ensured by checking the overlap ratio (Rolp) of matched 3D keypoints. The size ratio (Rs) of the same object in both images must not be too small, so the reference provides enough detailed texture. These ratios were computed using the existing D2-Net code. Based on these measures, each image pair was labeled with a similarity level:\nHigh (H) if Rolp \u0026gt; 30% and Rs \u0026gt; 0.9 Medium (M) if Rolp \u0026gt; 10% and Rs \u0026gt; 0.66 Low (L) otherwise After filtering, the authors obtained large image groups ‚Äî each with one target image and several reference images. Because training on full images is memory-intensive, they cropped smaller patches. For each group, they first randomly cropped a 300√ó300 patch from the target image. Then, using 3D keypoints, they located five nearby reference patches from images of different similarity levels (one H, two M, two L).\nFinally, this process produced 112,142 training groups, each containing one target patch and five reference patches. This dataset is about ten times larger than CUFED5 and offers much richer multi-reference diversity. For testing, the authors built another set of 142 groups, each with a target image and 2‚Äì6 reference images, with resolutions between 800 and 1600 pixels.\nMulti-Reference RefSR network The authors propose a multi-reference RefSR network, called MRefSR, to effectively use multiple reference images. The model is based on C2-Matching, which provides strong performance and open-source accessibility. Like C2-Matching, a Content Extractor (CE) extracts features $F_{LR}$ from the low-resolution $LR$ image, while a VGG extractor extracts multi-scale features $F_{Ref_i}$ from each reference image. The model also uses a pretrained Contrastive Correspondence Network (CCN) to estimate offsets $O_i$, aligning each reference image with the LR input.\nAfter feature extraction and alignment, the network includes two new modules: the Multi-Reference Attention Module (MAM) and the Spatial Aware Filtering Module (SAFM).\nThe MAM fuses features from multiple reference images. For each spatial location $(x, y)$, attention maps are generated to measure how similar each aligned reference feature $K_i(x, y)$ is to the LR feature $Q(x, y)$. The attention weights are computed with a softmax function, and all aligned reference features are combined into a fused reference feature $F_{fref}$ using a weighted sum. This allows the model to flexibly handle any number of reference images in both training and testing. Next, since not all fused features are reliable, the SAFM selects and refines them. It takes the concatenated features of $F_{LR}$ and $F_{fref}$ as input and generates two masks: a multiplicative mask $M_{mul}$ and an additive mask $M_{add}$. These masks are produced using convolution and Leaky ReLU layers, and $M_{mul}$ is passed through a sigmoid function to keep its values between 0 and 2. The final selected reference feature $F_{sref}$ is obtained by combining the fused feature with these masks. $$ M_{mul} = \\text{sigmoid}(f_1(F_{LR} || F_{fref})) \\cdot 2 $$\n$$ M_{add} = f_2(F_{LR}||F_{fref}) $$\n$$ F_{sref} = F_{fref} \\odot M_{mul} + M_{add} $$\n$$ X_{SR} = \\mathcal{G}(F_{LR}, F_{sref}) $$\nwhere\n$||$ denotes feature concatenation, $\\odot$ denotes element-wise multiplication, $f_1$ and $f_2$ are nonlinear mapping functions (convolutions + LeakyReLU), and $\\mathcal{G}$ is the restoration module that reconstructs the final super-resolved image. Finally, a restoration module $G$ takes both the LR features $F_{LR}$ and the selected reference features $F_{sref}$ to reconstruct the high-resolution output image $X_{SR}$.\nIn summary, MRefSR extends C2-Matching by adding multi-reference attention fusion and spatial-aware filtering, enabling flexible and effective use of multiple reference images for super-resolution.\nConclusion In this paper, the author proposed a large-scale multi-reference RefSR dataset: LMR. Unlike CUFED5, the only training RefSR dataset available before, LMR has 5 reference images for each LR input image. What‚Äôs more, LMR contains 112,142 groups of 300√ó300 training images, 10 times the number of CUFED5, and the image size is also much larger than CUFED5.\nBesides, the author proposed a new multi-reference baseline RefSR method, named MRefSR. A multi- reference attention module (MAM) for feature fusion of an arbitrary number of reference images, and a spatial aware filtering module (SAFM) for the fused feature selection. With LMR enabling multi-reference RefSR training, the method effectively models the relationship among multiple references, thus achieving significant improvements over SOTA approaches on both quantitative and qual- itative evaluations. And the method solves the mismatch problem of previous methods using a single reference image for training but testing with multiple reference images.\n","permalink":"/notes/lmr_a_large-scale_multi-reference_dataset_for_refe/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eSingle image super-resolution\u003c/strong\u003e \u003cstrong\u003e(SISR)\u003c/strong\u003e aims to restore a low-resolution (LR) image into a high-resolution (HR) image with realistic textures. With the growth of deep learning, SISR performance has greatly improved in recent years.\u003c/p\u003e\n\u003cp\u003eCompared to SISR, \u003cstrong\u003ereference-based super-resolution (RefSR)\u003c/strong\u003e makes use of additional high-resolution reference images that share similar textures with the target image. This allows RefSR to generate more detailed and realistic results. Because of these promising results, many researchers have focused on RefSR methods in recent years.\u003c/p\u003e","title":"LMR: A Large-Scale Multi-Reference Dataset for Reference-based Super-Resolution"},{"content":"Introduction Image synthesis has advanced rapidly, with diffusion models now leading in generating complex, high-resolution images. Unlike GANs, which struggle with stability and mode collapse, and autoregressive transformers, which require billions of parameters, diffusion models achieve strong results in class-conditional generation, super-resolution, and inpainting using fewer parameters and more stable training.\nDespite their success, diffusion models are still computationally expensive. Because they model every pixel, including imperceptible details, both training and inference require enormous GPU time and memory. This makes them less accessible and environmentally costly.\nPerceptual compression removes small, imperceptible pixel details but keeps the overall visual appearance. It focuses on what humans can see clearly, not on exact pixel accuracy. Autoencoders or GANs often perform this kind of compression. Semantic compression goes further. It removes even visible low-level details but preserves the meaning or concept of the image. For example, the exact texture of a face may change, but the idea of ‚Äúa person wearing glasses‚Äù remains. Latent diffusion models operate in this stage, learning high-level structure and meaning instead of pixel noise. To solve this, the authors propose training diffusion models in a latent space instead of pixel space. The idea is to first use an autoencoder to compress images into a smaller, perceptually equivalent representation, and then train the diffusion model on this compact latent data. This reduces computation while keeping visual quality.\nThe resulting method, called Latent Diffusion Model (LDM), combines an autoencoder and a diffusion U-Net, and can include transformer-based conditioning for tasks like text-to-image generation. This design makes high-resolution synthesis more efficient and scalable.\nIn summary, LDMs (1) scale better to large data, (2) significantly reduce training and inference cost, (3) produce faithful, detailed reconstructions, and (4) support versatile conditioning for multi-modal tasks such as text- or layout-based image generation.\nMethod The authors propose separating compression and generation into two stages. Instead of training directly in pixel space, they use an autoencoder to learn a latent space that keeps perceptual quality but greatly lowers computational complexity. The U-Net architecture further helps capture spatial structure, so there is no need for strong compression that harms image quality. In addition, the learned latent space can serve as a general-purpose representation for training other generative models or for tasks such as CLIP-guided image synthesis.\nPerceptual Image Compression The perceptual image compression model is an autoencoder trained with both a perceptual loss and a local patch-based adversarial loss to produce realistic, non-blurry reconstructions. The encoder compresses an image $x$ into a latent representation $z = Œµ(x)$, and the decoder reconstructs it as $\\tilde{x} = D(z)$. The image is downsampled by a factor $f$.\nTo keep the latent space stable, two types of regularization are used: a KL penalty (as in VAEs) or vector quantization (as in VQGANs). Unlike earlier methods that flattened the latent space into one dimension, this model keeps the 2D spatial structure of the latent representation, allowing for milder compression and higher reconstruction quality while preserving fine image details.\nLatent Diffusion Models Latent Diffusion Models (LDMs) are based on diffusion models, which learn to generate data by gradually denoising random noise through a reverse Markov process. In this framework, each step predicts a cleaner version of a noisy input, trained with a simple mean-squared error objective. By applying diffusion in the latent space learned from the perceptual autoencoder, the model focuses on meaningful, semantic image features instead of pixel-level noise. Unlike previous transformer-based methods that used discrete tokens, LDMs use a U-Net built from 2D convolutions to better capture spatial image structure. During generation, latent samples are denoised step by step and then decoded into final images with a single pass through the decoder.\nA reverse Markov process is the step-by-step denoising process that diffusion models learn ‚Äî it reverses the fixed forward noising process to transform pure noise back into a clean, generated image.\nConditioning Mechanisms Conditioning mechanisms allow diffusion models to generate images based on extra input information $y$, such as text, semantic maps, or other images. To achieve this, the model learns a conditional distribution $p(z|y)$ by adding the condition $y$ to the denoising network. The authors enhance the U-Net backbone of the LDM with a cross-attention mechanism, which lets the network focus on relevant parts of $y$ while generating the image. A separate encoder $œÑ_Œ∏$ converts the condition into an embedding that interacts with the U-Net through attention layers. This setup allows flexible conditioning from different modalities, meaning the same diffusion model can handle tasks like text-to-image, layout-to-image, or other image-to-image synthesis efficiently.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) \\cdot V $$\n$$ Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\quad K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\quad V = W_V^{(i)} \\cdot \\tau_\\theta(y) $$\n$K$ and $V$ come from the conditioning input (e.g., text embeddings). $Q$ comes from the U-Net feature map at the current diffusion step. Use in super-resolution Latent Diffusion Models (LDMs) can perform super-resolution efficiently by conditioning on low-resolution images. The method simply concatenates the LR input with the U-Net input, allowing the model to learn how to reconstruct high-frequency details.\nUsing a pretrained autoencoder with downsampling factor (f=4), the model (LDM-SR) is trained on ImageNet with bicubic 4√ó downsampling, following SR3‚Äôs setup. A user study confirms that LDM-SR produces more visually pleasing results. To further enhance detail, a perceptual loss is added as a guiding mechanism. Finally, since bicubic degradation limits generalization, a more robust version called LDM-BSR is trained using diverse degradations to handle real-world low-quality inputs.\nConclusion Latent diffusion models, a simple and efficient way to significantly improve both the training and sampling efficiency of denoising diffusion models without degrading their quality. Based on this and cross-attention conditioning mechanism, the experiments could demonstrate favorable results compared to SOTA methods across a wide range of conditional image synthesis tasks without task-specific architectures.\n","permalink":"/notes/high-resolution_image_synthesis_with_latent_diffus/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eImage synthesis has advanced rapidly, with diffusion models now leading in generating complex, high-resolution images. Unlike GANs, which struggle with stability and mode collapse, and autoregressive transformers, which require billions of parameters, diffusion models achieve strong results in class-conditional generation, super-resolution, and inpainting using f\u003cstrong\u003eewer parameters and more stable training\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eDespite their success, diffusion models are still \u003cstrong\u003ecomputationally expensive\u003c/strong\u003e. Because they model every pixel, including \u003cstrong\u003eimperceptible details\u003c/strong\u003e, both training and inference require enormous GPU time and memory. This makes them less accessible and environmentally costly.\u003c/p\u003e","title":"Latent Diffusion Models"},{"content":"Reinforcement Learning\nIntroduction Recent LLMs are rapidly advancing toward AGI. Post-training has emerged as an important component of the full training pipeline, which enhances accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI‚Äôs o1 series models were the first to introduce inference-time scaling by increasing the length of the CoT reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning.\nHowever, the challenge of effective test-time scaling (efficient reasoning at inference) remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models, RL, and search algorithms such as MCTS and Beam Search. However, none of these methods has achieved general reasoning performance comparable to OpenAI‚Äôs o1 series models.\nThis paper proposes a RL-only-based approach DeepSeek-R1-Zero which directly applied RL to the base model without relying on supervised fine-tuing (SFT) as a preliminary step. The network use DeepSeek-V3-Base as the foundation and apply GRPO (a reinforcement learning framework). After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks, showing strong reasoning performance, and matching OpenAI o1-0912.\nHowever, DeepSeek-R1-Zero suffers from poor readability and language mixing. To address these issues and further enhance reasoning performance, the DeepSeek introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline.\nDeepSeek-R1-Zero ‚Üí trained only with RL, no SFT (‚Äúpure RL from base model‚Äù). DeepSeek-R1 ‚Üí adds extra cold-start SFT + synthetic data generation + another RL phase. Training pipeline:\nCollect thousands of cold-start data to fine-tune DeepSeek-V3-Base model. Perform reasoning-oriented RL like DeepSeek-R1-Zero. Near convergence in the RL process, Create new SFT(supervised fine-tuning) data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, the obtained checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\nFinally, they perform distillation of DeepSeek-R1 into smaller models (based on Qwen2.5-32B). Even after removing RL, these distilled models retain reasoning skills, showing that large-model reasoning discoveries can be transferred. Notably, the 14B distilled model beats all open-source baselines on reasoning benchmarks.\nContributions Post-Training: Large-Scale Reinforcement Learning on the Base Model\nDeepSeek-R1-Zero: RL applied directly to base model without SFT ‚Üí achieves self-verification, reflection, long CoT reasoning. DeepSeek-R1: Pipeline with 2 RL + 2 SFT stages ‚Üí improves reasoning, alignment, and non-reasoning abilities. Distillation: Smaller Models Can Be Powerful Too\nReasoning patterns learned by large models can be distilled into smaller ones. The open-source DeepSeek-R1-Distill family (1.5B ‚Äì 70B parameters) performs exceptionally well, matching or surpassing strong baselines like OpenAI o1-mini. DeepSeek-R1-Distill-Qwen-7B: 55.5% AIME 2024. DeepSeek-R1-Distill-Qwen-32B: 72.6% AIME 2024, 94.3% MATH-500, 57.2% LiveCodeBench. Summary of Evaluation Results Reasoning Tasks:\nDeepSeek-R1 reaches 79.8 % Pass@1 on AIME 2024 (‚âà OpenAI o1-1217). On MATH-500 ‚Üí 97.3 %, Codeforces ‚Üí 2 029 Elo (\u0026gt; 96 % human). Performs slightly below DeepSeek-V3 in engineering tasks. Knowledge Tasks:\nOn MMLU (90.8 %), MMLU-Pro (84 %), GPQA Diamond (71.5 %), DeepSeek-R1 beats DeepSeek-V3 and is close to OpenAI o1-1217.\nOther Abilities:\nExcels in writing, summarization, code generation, and instruction following. Achieves 87.6 % length-controlled win-rate (AlpacaEval 2.0) and 92.3 % win-rate (ArenaHard). Especially strong on long-context understanding and non-exam queries. Approach Previous work has heavily relied on large amounts of supervised data to enhance model performance. The DeepSeek team demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. The following sections introduce: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models.\nDeepSeek-R1-Zero DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\nProximal Policy Optimization (PPO) is the standard RL method used in LLM fine-tuning (e.g., RLHF). It involves three models: a policy model that generates responses, a reward(value) model that scores them, and a critic model that estimates a baseline (the expected reward) to compare with the reward. The policy and critic are trained together, using the difference between the actual reward and the baseline to encourage better-than-average outputs while maintaining training stability. But the reward model is fixed which only provides the scores. However, PPO requires a large critic network (often the same size as the policy model), which doubles computational cost.\nTo address this, Group Relative Policy Optimization (GRPO) simplifies the process by removing the critic and computing the baseline directly from the average reward of a group of sampled responses, making reinforcement learning more efficient for large LLMs like DeepSeek-R1-Zero.\nReinforcement Learning Algorithm Goal: Train the policy model $\\pi_\\theta$ to generate above-average answers while keeping training stable and close to a reference model.\nObjective Function:\n$$ J_{GRPO}(\\theta) = E[{q \\sim P(Q), \\{o_i\\}^G_{i=1} \\sim \\pi_{\\theta_{\\text{old}}}(O|q)}] \\\\ \\frac{1}{G} \\sum_{i=1}^{G} \\Big( \\min\\Big( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip}\\Big(\\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1-\\varepsilon, 1+\\varepsilon\\Big)A_i \\Big) - \\beta D_{KL}(\\pi_\\theta||\\pi_{\\text{ref}}) \\Big) $$ $q‚àºP(Q)$: sample a question/prompt from the dataset. $ {o_i}^G_{i=1} \\sim \\pi_{\\theta_{\\text{old}}}(O|q)$: use the old model to generate G different answers to the same question. $\\frac{1}{G} \\sum_{i=1}^{G}$: averages the result over the G samples (the group). $\\frac{\\pi_\\theta}{\\pi_{\\theta_{old}}}$: probability ratio showing how the model‚Äôs belief changes. $A_i$: advantage, computed within the group, measures how much better each answer is than group average ($baseline=mean(r_1‚Äã,r_2‚Äã,‚Ä¶,r_G‚Äã)$): $$ A_i = \\frac{r_i - {mean}(r_1,\\dots,r_G)}{{std}(r_1,\\dots,r_G)} $$\n$r_i$: reward for response $o_i$. clip(¬∑): limits updates to ensure stability (same as PPO). $D_{KL}(\\pi_\\theta||\\pi_{ref})$: KL penalty keeping the model close to a reference policy (e.g., base model). $$ D_{KL}(\\pi_\\theta || \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o_i|q)}{\\pi_\\theta(o_i|q)} -\\log\\frac{\\pi_{\\text{ref}}(o_i|q)}{\\pi_\\theta(o_i|q)} - 1 $$\n$\\varepsilon, \\beta$: hyperparameters controlling clipping and KL weight. Where do the rewards $r_i$ come from? ‚Üí From Reward Modeling, which defines how to score each generated answer.\n$$ \\boxed{\\text{Total computations} \\propto O \\times G} $$\n$O$ = number of prompts/questions sampled in a batch $G$ = number of responses per prompt Reward Modeling To train DeepSeek-R1-Zero, a rule-based reward system was adopted that mainly consists of two types of rewards:\n$$ r_i=r_i(accuracy)+r_i(format) $$\nAccuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. If the answer matches the correct one, reward = 1; else 0 (or scaled). Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‚Äò‚Äô and ‚Äò‚Äô tags. Reward = 1 if the output format is correct, else 0. Training Template To train DeepSeek-R1-Zero, begining by designing a straightforward template that guides the base model to follow to the specified instructions. This following template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. prompt will be replaced with the specific reasoning question during training. Limit the constraints to this structural format, avoiding any content-specific biases to ensure that the model‚Äôs natural progression can be observed accurately during the RL process.\nAlthough DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek R1-Zero struggles with challenges like poor readability, and language mixing(the base model DeepSeek-V3 is multilingual, and RL doesn‚Äôt penalize mixing languages). To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data.\nGood readability (_after_SFT):\nTo compute the sum of the first 5 even numbers: 2 + 4 + 6 + 8 + 10 = 30. 30 Poor readability (DeepSeek-R1-Zero):\nsum=2+4+6+8+10=\u0026gt;=30?? yes right 30 correct result final output=30 30\nDeepSeek-R1 DeepSeek-R1: Reinforcement Learning with Cold Start\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows.\nCold Start Construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. The data can include reasoning examples in the prompt (few-shot) or detailed reasoning in the response, ensuring the model learns to generate readable, step-by-step solutions. To collect such data, there are several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, they collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data are readability of responses and better performance by designing a readable pattern that includes a summary at the end of each response.\nReasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on cold-start data, the same GRPO-based RL process as DeepSeek-R1-Zero is applied to enhancing the model‚Äôs reasoning capabilities in math, logic, science, and coding tasks. However, RL caused language mixing in Chain-of-Thought reasoning, so they introduced a language-consistency reward, computed as the ratio of target-language words in the reasoning text. The final reward combines accuracy and language consistency: $r_{final} = r_{accuracy} + r_{lang}$Although this slightly reduces task performance, it produces more readable, human-preferred outputs. The model is trained until convergence, resulting in the final DeepSeek-R1 model.\nRejection Sampling and Supervised Fine-Tuning After reasoning-oriented RL training is finished, they use the RL-trained checkpoint to generate new supervised fine-tuning (SFT) data for the next round, aiming to improve model‚Äôs capabilities in writing, role-playing, and other general-purpose tasks. They produce two types of data: For reasoning data, they sample many responses (like 10‚Äì20 per question) from the RL checkpoint. then they perform rejection sampling. For each prompt, they sample multiple responses and retain only the correct ones. For non-reasoning data, they add general-purpose data reuse or regenerate these using DeepSeek-V3‚Äôs SFT data.\nReinforcement Learning for all Scenarios To further align the model with human preferences, they implement a secondary reinforcement learning stage aimed at improving the model‚Äôs helpfulness and harmlessness while simultaneously refining its reasoning capabilities.\nFor reasoning data, they follow to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process. For general data, they resort to reward models to capture human preferences $r_i^{(preference)}‚àà[0,1]$. They build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, they focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, they evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\nDistillation: Empower Small Models with Reasoning Capability For distilled models, the team apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance.\nConclusion This paper shares how to enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achievesperformance comparable to OpenAI-o1-1217 on a range of tasks. They further explore distillation the reasoning capability to small dense models. They use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints.\nIn the future, there are some research directions across the following directions for DeepSeek-R1.\nGeneral Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results. Software Engineering Tasks: Due to the long evaluation times, which impact the efficiency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency. ","permalink":"/notes/deepseek-r1_incentivizing_reasoning_capability_in/","summary":"\u003cp\u003eReinforcement Learning\u003c/p\u003e\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eRecent LLMs are rapidly advancing toward AGI. \u003cstrong\u003ePost-training\u003c/strong\u003e has emerged as an important component of the full training pipeline, which enhances accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against \u003cstrong\u003epre-training\u003c/strong\u003e. In the context of reasoning capabilities, OpenAI‚Äôs o1 series models were the first to introduce \u003cstrong\u003einference-time scaling\u003c/strong\u003e by increasing the length of the CoT reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning.\u003c/p\u003e","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"},{"content":"Introduction Self-attention-based architectures, especially Transformers, have become the first choice of model in natural language processing (NLP). The main approach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset. Thanks to Transformers‚Äô computational efficiency and scalability, it has become possible to train models of record-breaking size, with over 100B parameters. With the models and datasets growing, there is still no sign of reaching a performance limit .\nHowever, convolutional architectures remain dominant in computer vision. Inspired by the success of Transformers in NLP, many studies have attempted to integrate self-attention mechanisms into CNN-like architectures. A na√Øve application of self-attention to images would require each pixel to attend to every other pixel, resulting in quadratic computational cost with respect to the number of pixels, which does not scale to realistic image sizes. Later works introduced techniques such as local, sparse, or block attention, or reduced image resolution to reduce this cost. Although these methods make self-attention more scalable for visual data, they often demand complex engineering for efficient implementation on GPUs or TPUs, and are not well-suited for large-scale datasets. Consequently, in large-scale image recognition, classical convolutional architectures such as ResNet still dominate the state of the art.\nHere, a new approach ViT is introduced that applies a standard Transformer directly to images with minimal modifications. To achieve this, the image is divided into patches, and a sequence of linear embeddings of these patches is fed into the Transformer. Each image patch is treated the same way as a token (word) in NLP applications. The model is trained for image classification in supervised learning. When trained on mid-sized datasets such as ImageNet without strong regularization, the performance is moderate; however, the picture changes dramatically when the model is trained on large-scale datasets containing 14M‚Äì300M images. In such cases, large-scale training outweighs the need for strong inductive biases. The Vision Transformer (ViT) achieves excellent results when pre-trained at sufficient scale and then fine-tuned on downstream tasks with fewer data points.\nMethods The design of the model follows the original Transformer as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures, and their efficient implementations which can be used almost out of the box.\nVision Transformer (ViT) In its input stage, an image $\\mathbf{x} \\in R^{H \\times W \\times C}$ with height H, width W, and C channels is divided into small, non-overlapping patches of size $(P, P)$. Each patch is flattened into a vector, resulting in a sequence of patch vectors $\\mathbf{x}_p \\in R^{N \\times (P^2 \\cdot C)}$, where $N = \\frac{H W}{P^2}$ represents the total number of patches (also the effective sequence length for the Transformer). These flattened patch vectors are then linearly projected into a latent feature space of dimension $D$ using a trainable weight matrix. The output of this projection forms the patch embeddings, which serve as the Transformer‚Äôs token representations, the same as the word embeddings in NLP models.\nSimilar to BERT, ViT introduces an additional learnable [class] token that is placed in the beginning of the sequence of patch embeddings. Its output vector, after passing through all Transformer layers, represents the entire image and is used for classification. The model attaches a classification head to this [class] token output, which is implemented as a multilayer perceptron (MLP) with one hidden layer during pre-training and a single linear layer during fine-tuning.\n$$ output=softmax(Head([class] token)) $$\nPositional Encoding Because the Transformer itself has no spatial concept, ViT adds positional embeddings to the patch embeddings to retain information about the patches‚Äô relative locations. The authors use simple, learnable 1D positional embeddings instead of more complex 2D-aware ones, as they found little improvement from the latter.\nTransformer Encoder The resulting sequence (class token + patch embeddings + positional embeddings) is then fed into a standard Transformer encoder identical to that used in NLP. Each encoder block alternates between multi-head self-attention (MSA) and feed-forward MLP sublayers, with layer normalization (LN) and residual connections applied around both.\nThe key computation within one layer of the model can be summarized by the following equations:\n$$ \\begin{aligned} z_0 \u0026= [\\,x_{class};\\; x_p^1 E;\\; \\ldots;\\; x_p^{N} E\\,] + E_{pos}, \\quad E \\in R^{(P^2 \\cdot C) \\times D}, \\quad E_{pos} \\in R^{(N+1) \\times D} \\\\ z'_{\\ell} \u0026= MSA(LN(z_{\\ell-1})) + z_{\\ell-1}, \\quad \\ell = 1, \\ldots,L \\\\ z_{\\ell} \u0026= MLP(LN(z'_{\\ell})) + z'_{\\ell}, \\quad \\ell = 1, \\ldots,L \\\\ y \u0026= LN(z^{0}_{L}) \\end{aligned} $$ Symbol Meaning P Patch size C Number of color channels N Number of patches D Embedding dimension $x_p^i \\in R^{P^2 \\cdot C}$ is the flattened vector of the i-th image patch. $E \\in R^{(P^2 \\cdot C) \\times D}$ is the linear projection matrix for patch embeddings. $E_{\\text{pos}} \\in R^{(N+1) \\times D}$ provides positional embeddings for all tokens, including the class token. $\\text{MSA}$ denotes the multi-head self-attention mechanism that models global dependencies across all patches. Self-attention computes: $\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$ $\\text{MLP}$ is a two-layer feed-forward network with a GELU (Gaussian Error Linear Unit) activation function to each patch token separately, adding non-linearity. There‚Äôs no cross-patch connection. The input and output dimensions are the same $D$. The residual connections (‚Äú+‚Äù terms) help preserve gradients and stabilize training. Final $LN(z_L^0)$ applied after all Transformer layers (not shown in block diagram). Together, these operations define one forward pass through the ViT encoder, where $L$ is the total number of Transformer layers.\nInductive Bias A defining characteristic of the Vision Transformer is that it possesses much weaker image-specific inductive bias than convolutional neural networks (CNNs). In CNNs, design principles such as local connectivity, two-dimensional neighborhood structure, and translation equivariance are built into the convolution operation at every layer. These biases make CNNs naturally suited for visual data.\nViT, in contrast, uses global self-attention, allowing every image patch to communicate with all other patches at once. This means ViT does not have any built-in understanding of local areas or spatial hierarchy ‚Äî the model itself doesn‚Äôt ‚Äúknow‚Äù which patches are next to each other. Inside ViT, only the small MLP layers work separately on each patch (they don‚Äôt mix information between patches). The positional embeddings just tell the model roughly where each patch is in the image, but they don‚Äôt teach it how nearby regions are related. As a result, ViT must learn all spatial relationships by itself from the training data, instead of having them designed into the architecture like CNNs do. Therefore, ViT depends heavily on large-scale training data to learn these spatial patterns effectively.\nHybrid Architecture The authors also explore a hybrid architecture that combines the strengths of CNNs and Transformers. Instead of feeding the Transformer with raw image patches, they use CNN feature maps as input. In this case, each patch corresponds to a small region (for example, 1√ó1) of the CNN‚Äôs feature map. The later steps are the same. This hybrid approach introduces some of the beneficial inductive biases of CNNs (such as local feature extraction) while maintaining the global modeling capacity and scalability of the Transformer.\nFine-tuning and Higher Resolution During training, the Vision Transformer is first pre-trained on a large dataset and later fine-tuned on a smaller, task-specific dataset. When fine-tuning, the authors remove the old classification head (used in pre-training) and replace it with a new one that matches the number of classes in the new task.\nSometimes, it helps to fine-tune using higher-resolution images than were used during pre-training. In that case, the patch size stays the same, so the model now receives more patches (a longer input sequence). However, the position embeddings learned during pre-training may no longer align with the new number of patches. To fix this, the authors resize the position embeddings matrix in 2D so that they fit the new image resolution.\nThis resolution adjustment and patch extraction are the only points at which an inductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\nExperiments The authors compare Vision Transformer (ViT) with ResNet and hybrid CNN‚ÄìTransformer models to evaluate its representation learning ability. Models are pre-trained on large datasets, ImageNet (1.3M images), ImageNet-21k (14M images), and JFT-300M (303M images), and then fine-tuned on smaller benchmarks such as CIFAR-10/100, Oxford Pets, Oxford Flowers-102, and the VTAB suite.\nThree ViT variants are tested: Base (86M params), Large (307M), and Huge (632M), corresponding to 12, 24, and 32 Transformer layers. Smaller patch sizes give better resolution but higher computational cost. Models are trained with the Adam optimizer, large batch size, and linear learning-rate warm-up, and fine-tuned at higher image resolutions for better results.\nWhen pre-trained on JFT-300M, ViT-H/14 and ViT-L/16 outperform both Big Transfer (BiT) ResNets and Noisy Student EfficientNet models on nearly all datasets while using less computation. Even with public ImageNet-21k pre-training, ViT achieves competitive accuracy.\nA key result is that dataset size is critical: ViT underperforms ResNets on small data (ImageNet-1k) but surpasses them as the dataset grows. Larger ViT models continue to improve with more data, while ResNets plateau. Hybrid CNN‚ÄìTransformer models help for smaller data but offer no advantage at large scale.\nIn few-shot evaluation, the Vision Transformer is not fully retrained. Instead, its pre-trained features are frozen, and only a simple linear classifier is trained on top of them using a very small number of labeled examples per class, such as 5 images for each category in ImageNet. This setup tests how well the model‚Äôs learned representations can generalize to new tasks when only a few labeled samples are available, showing the quality and transferability of the pre-trained features.\nAlso, the authors explore self-supervised pre-training for Vision Transformers, inspired by how Transformers in NLP achieve strong results through large-scale self-supervised learning rather than supervision alone. They experiment with a masked patch prediction task, similar to BERT‚Äôs masked language modeling, where parts of an image are hidden and the model learns to predict them. Using this method, the smaller ViT-B/16 model reaches 79.9% accuracy on ImageNet, which is 2% better than training from scratch but still 4% lower than supervised pre-training. The authors note that while self-supervision improves performance, it does not yet match large-scale supervised results. They suggest that exploring contrastive pre-training methods could be a promising direction for future research.\nConclusion ViT is the direct application of Transformers to image recognition. Unlike prior works using self-attention in computer vision, it do not introduce image-specific inductive biases into the architecture apart from the initial patch extraction step. Instead, it interpret an image as a sequence of patches and process it by a standard Transformer encoder as used in NLP. This simple, yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets. Thus, ViT matches or exceeds the sota on many image classification datasets, meantime being relatively cheap to pre-train.\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other computer vision tasks, such as detection and segmentation. Another challenge is to continue exploring self-supervised pre-training methods. The initial experiments show improvement from self-supervised pre-training, but there is still large gap between self-supervised and large-scale supervised pre-training. Finally, further scaling of ViT would likely lead to improved performance.\n","permalink":"/notes/vit/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eSelf-attention-based architectures, especially \u003cstrong\u003eTransformers\u003c/strong\u003e, have become the first choice of model in \u003cstrong\u003enatural language processing (NLP)\u003c/strong\u003e. The main approach is to \u003cstrong\u003epre-train\u003c/strong\u003e on a large text corpus and then \u003cstrong\u003efine-tune\u003c/strong\u003e on a smaller task-specific dataset. Thanks to Transformers‚Äô computational efficiency and scalability, it has become possible to train models of record-breaking size, with over 100B parameters. With the models and datasets growing, there is still no sign of reaching a performance limit .\u003c/p\u003e","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"content":"Abstraction Bayesian Optimization is a method for finding the best solution when evaluating the objective function is slow or expensive (taking minutes or hours).\nIt is mainly used for problems with fewer than 20 variables and where evaluations may contain noise.\nThe method works by:\nBuilding a surrogate model of the unknown function using a Bayesian machine learning method called Gaussian Process regression, which predicts both the function value and its uncertainty. Using an acquisition function (expected improvement, knowledge gradient or entropy search) to decide where to sample next ‚Äî balancing exploration and exploitation. The tutorial also extends expected improvement to handle noisy evaluations, supported by a formal decision-theoretic argument, rather than ad hoc fixes.\n1 Introduction Bayesian Optimization (BayesOpt) is a machine learning‚Äìbased method for optimizing expensive, black-box functions, it‚Äôs designed for black-box derivative-free global optimization. It aims to solve\n$$ \\max_{x \\in A} f(x) $$\nwhere $f(x)$ is expensive to evaluate, derivative-free, and continuous.\nTypical Problem Setting\nThe input x lies in a continuous space R^d with small dimensionality (usually d ‚â§ 20). ‚Üí search space The feasible set A is simple, such as a box constraint or simplex. ‚Üí search area The objective function f is: Continuous (needed for Gaussian Process modeling). Expensive ‚Äî each evaluation might take hours or cost resources. Black-box ‚Äî no known analytic structure like linearity or convexity. Derivative-free ‚Äî we can only observe f(x), not gradients. Possibly noisy ‚Äî measurements may include Gaussian noise. At first, we pretend every time we evaluate f(x), we get the exact same result. Later in the paper, the author adds stochastic noise ‚Äî meaning repeated evaluations of the same xxx might give slightly different results (like random fluctuations). Concept Meaning Intuition Search space The entire region of possible inputs x, its dimensionality tells us how complex the problem is. it‚Äôs the multi-dimensional space that defines where you can look. Feasible set / search area The subset of that space that you actually allow x to take, i.e., with all constraints applied. this is the region inside the search space that satisfies all limits (bounds, rules). BayesOpt is for global optimization of black-box functions. It builds a surrogate model of f(x) using a Bayesian machine learning technique, typically Gaussian Process (GP) regression. It then chooses where to sample next using an acquisition function (e.g., Expected Improvement, Knowledge Gradient, or Entropy Search). This balances exploration (trying uncertain areas) and exploitation (sampling promising areas).\nThe ability to optimize expensive black-box derivative-free functions makes BayesOpt extremely versatile. Recently it has become extremely popular for tuning hyperparameters in machine learning algorithms. And it‚Äôs also suitable for engineering design, scientific experiments and reinforcement Learning.\nWhat makes Bayesian Optimization unique compared to general surrogate-based optimization approaches are using surrogates developed using bayesian statistics and choosing new points using a probabilistic acquisition rule instead of heuristics. (reasoning probabilistically about what it already knows and what it‚Äôs uncertain about)\n2 Overview of BayesOpt BayesOpt consists of two main components: a Bayesian statistical model for modeling the objective function, and an acquisition function for deciding where to sample next.\nBayesian Statistical Model Models the unknown objective function $f(x)$.\nTypically a Gaussian Process (GP).\nProduces a posterior distribution:\n$$ f(x) \\sim \\mathcal{N}(\\mu_n(x), \\sigma_n^2(x)) $$\nwhere:\n$\\mu_n(x)$: predicted mean (best guess) $\\sigma_n(x)$: predicted uncertainty Updated each time new data (evaluations of f) are observed.\nAcquisition Function Decides where to sample next based on the GP‚Äôs posterior. Measures the ‚Äúvalue‚Äù of sampling a new point x: High $\\mu_n(x)$: promising area. High $\\sigma_n(x)$: uncertain area. Common types: Expected Improvement (EI), Knowledge Gradient (KG), Entropy Search (ES), and Predictive Entropy Search (PES). After evaluating the objective according to an initial space-filling experimental design, often consisting of points chosen uniformly at random, they are used iteratively to allocate the remainder of a budget of N function evaluations.\nAlgorithm 1: Basic Bayesian Optimization Loop\nStart with a Gaussian Process (GP) model that guesses how f might behave. Test f at a few random starting points spread across the space. Repeat until you run out of trials: Update the GP using all results collected so far. Use the acquisition rule to pick the next best point to try. Test the real function at that point to get a new result ( y_n = f(x_n) ). Add this new point to your data. Give the final answer: The point with the best actual value, or The point the GP predicts to be the best. 3 Gaussian Process (GP) Regression Gaussian Process (GP) Regression is a Bayesian way to model an unknown function $f(x)$.\nIt assumes that any collection of function values $[f(x_1), f(x_2), \u0026hellip;, f(x_k)]$ follows a multivariate normal distribution with a specific mean vector and covariance matrix.\nSo instead of guessing one possible curve for $f(x)$, we assume a probability distribution over all possible smooth curves.\n3.1 Initialization Steps Step 1: Define the Prior Before we see any data, we describe our belief about $f(x)$ using:\nA mean function $\\mu_0(x)$ ‚Üí gives the average expected value, often set to 0 (no bias). A covariance (kernel) function $\\Sigma_0(x_i, x_j)$ ‚Üí shows how similar two points are. Close points = high correlation (similar f values) Far points = low correlation (independent values) Together, they form the prior:\n$$ f(x_{1:k}) \\sim \\text{Normal}(\\mu_0(x_{1:k}), \\Sigma_0(x_{1:k}, x_{1:k})) $$\nStep 2: Update with Observed Data (Bayes‚Äô Rule) Once we have some known data $(x_1, f(x_1)), \u0026hellip;, (x_n, f(x_n))$, we update our belief to get the posterior distribution, what we now believe about the function after seeing real values.\nFor a new point x:\n$$ f(x)|f(x_{1:n}) \\sim \\text{Normal}(\\mu_n(x), \\sigma_n^2(x)) $$\nwhere:\n$\\mu_n(x)$: the posterior mean ‚Äî our best prediction at $x$ $\\sigma_n^2(x)$: the posterior variance ‚Äî how uncertain we are at $x$ The GP uses the kernel to decide how much nearby points influence the prediction:\nIf $x$ is near known points ‚Üí high confidence, low uncertainty. If $x$ is far from all known points ‚Üí low confidence, high uncertainty. Step 3: Posterior Formula $$ \\mu_n(x) = \\Sigma_0(x, x_{1:n})\\Sigma_0(x_{1:n}, x_{1:n})^{-1}(f(x_{1:n}) - \\mu_0(x_{1:n})) + \\mu_0(x) $$\n$$ \\sigma_n^2(x) = \\Sigma_0(x, x) - \\Sigma_0(x, x_{1:n})\\Sigma_0(x_{1:n}, x_{1:n})^{-1}\\Sigma_0(x_{1:n}, x) $$\nMeaning:\nThe new prediction $\\mu_n(x)$ = weighted average of nearby known values (old belief + weighted correction from known data) The uncertainty $\\sigma_n^2(x)$ = initial uncertainty minus what we‚Äôve learned (reduce uncertainty) Step 4: Practical Notes Instead of directly inverting large matrices, use Cholesky decomposition for stability and speed. Add a tiny number (e.g., $10^{-6}$) to the diagonal of the covariance matrix to prevent numerical errors. The same formulas work for: Many new points at once (matrices) Continuous domains (theoretically infinite points) 3.2 Choosing a Mean Function and Kernel Kernel choice The kernel (covariance function) defines how much two inputs (x) and (x\u0026rsquo;) are correlated. Points that are close in the input space are more strongly correlated, the kernel must be positive semi-definite (it cannot give negative variances):\n$$ if (||x - x\u0026rsquo;|| \u0026lt; ||x - x\u0026rsquo;\u0026rsquo;||), then (Œ£_0(x, x\u0026rsquo;) \u0026gt; Œ£_0(x, x\u0026rsquo;\u0026rsquo;)). $$\nIf you combine things in any way, the total uncertainty you calculate will never be negative.\nPower Exponential (Gaussian) Kernel $$ Œ£_0(x, x\u0026rsquo;) = Œ±_0 \\exp(-||x - x\u0026rsquo;||^2) $$\nThe is the most common kernel and produces very smooth functions.\n$Œ±_0$ controls overall variance (how much $f(x)$ can vary). $Œ±_i$ inside $||x - x\u0026rsquo;||^2 = \\sum_i Œ±_i (x_i - x\u0026rsquo;_i)^2$ control how quickly correlation decreases as inputs differ. Mat√©rn Kernel $$ Œ£_0(x, x\u0026rsquo;) = Œ±_0 \\frac{2^{1-ŒΩ}}{Œì(ŒΩ)} (\\sqrt{2ŒΩ}||x - x\u0026rsquo;||)^{ŒΩ} K_ŒΩ(\\sqrt{2ŒΩ}||x - x\u0026rsquo;||) $$\nAdds a parameter $ŒΩ$ that controls smoothness. Smaller $ŒΩ$ produces rougher functions, larger $ŒΩ$ gives smoother ones. $K_ŒΩ$ is the modified Bessel function. Mean function The mean function expresses the expected trend of $f(x)$ before seeing data. The most common choice is a constant mean: $Œº_0(x) = Œº$. And If $f(x)$ is believed to have a trend, a parametric mean can be used:\n$$ Œº_0(x) = Œº + \\sum_{i=1}^{P} Œ≤_i Œ®_i(x) $$\nwhere $Œ®_i(x)$ are basis functions, often low-order polynomials.\nFor example, $Œ®(x) = x$ gives a linear trend, $Œ®(x) = [1, x, x^2]$ gives a quadratic trend.\n3.3 Choosing Hyperparameters The mean and kernel functions contain parameters (like $\\alpha_0, \\nu, \\mu$) called hyperparameters, grouped in a vector $\\eta$. These control how the Gaussian Process behaves (for example, how smooth it is or what its average level is).\nMaximum Likelihood Estimation (MLE) $$ \\hat{\\eta} = \\arg\\max_{\\eta} P(f(x_{1:n}) | \\eta) $$\nChoose hyperparameters that make the observed data most likely under the GP model. It‚Äôs simple and widely used. But it can give unreasonable results if the model overfits (e.g., too smooth or too wiggly).\nMaximum a Posteriori (MAP) $$ \\hat{\\eta} = \\arg\\max_{\\eta} P(\\eta | f(x_{1:n})) = \\arg\\max_{\\eta} P(f(x_{1:n}) | \\eta) P(\\eta) $$\nSimilar to MLE, but adds a prior $P(\\eta)$ on the hyperparameters. This prior prevents extreme or unrealistic parameter values. The MLE is a special case of MAP when $P(\\eta)$ is constant (flat). Common priors include uniform, normal, or log-normal distributions.\nFully Bayesian Approach $$ P(f(x) = y| f(x_{1:n})) = \\int P(f(x) = y| f(x_{1:n}), \\eta) P(\\eta | f(x_{1:n})) d\\eta $$\nInstead of choosing a single best $\\eta$, it integrates over all possible values of the hyperparameters. It produces more robust uncertainty estimates but is computationally expensive. In practice, it‚Äôs approximated using sampling methods (e.g., MCMC). MAP can be viewed as an approximation to this full Bayesian inference.\nDon‚Äôt fix Œ∑; instead, consider all possible Œ∑, weighted by how likely each one is.\nBut this high-dimensional and usually cannot be computed exactly, so in practice, we approximate it by sampling:\n$$ P(f(x) = y |f(x_{1:n})) \\approx \\frac{1}{J} \\sum_{j=1}^{J} P(f(x) = y |f(x_{1:n}), \\eta = \\eta_{j}) $$\nwhere the samples $\\eta_j$ are drawn from $P(\\eta | f(x_{1:n}))$. This is typically done using MCMC (Markov Chain Monte Carlo).\n3. Summary table Method In short Pros Cons MLE Fit the data best Finds hyperparams that best fit data Simple MAP Fit the data but stay reasonable Adds prior to control extremes More stable Fully Bayesian Consider all possible fits, weighted by probability computationally expensive Integrates over all possible scenarios 4 Acquisition Functions Expected Imrpvement(EI), Knowledge Gradient(KG), Entropy Search(ES)\n4.1 Expeced Improvement Goal: Decide where to sample next so that we are likely to improve our current best result.\nSuppose we have already tested n points. The best value so far is\n$$ f_n^* = \\max_{m \\le n} f(x_m) $$\nParameters:\nn is the number of points we have already evaluated so far. m is just an index variable If we evaluate a new point x, its value f(x) is uncertain. The improvement is how much better it is than the current best:\n$$ I(x) = [f(x) - f_n^*]^+ = \\max(f(x) - f_n^*, 0) $$\nSince f(x) is a random variable under the Gaussian Process model, we take the expected value of this improvement:\n$$ EI_n(x) = E_n[[f(x) - f_n^*]^+] $$\nBecause $f(x) \\sim \\mathcal{N}(\\mu_n(x), \\sigma_n^2(x))$, EI can be computed in closed form:\n$$ EI_n(x) = (\\mu_n(x) - f_n^*)\\Phi(z) + \\sigma_n(x)\\phi(z), \\quad z = \\frac{\\mu_n(x) - f_n^*}{\\sigma_n(x)} $$\nwhere $\\Phi$ is the normal CDF and $\\phi$ is the normal PDF.\n$Œ¶(z)$ = the cumulative distribution function (CDF) of the standard normal.\n‚Üí It gives the probability that a standard normal variable is ‚â§ z.\n$œï(z)$ = the probability density function (PDF) of the standard normal.\n‚Üí It gives the height of the bell curve at z.\nGoal ‚Üí How much do I expect to improve the best result I‚Äôve found so far if I test at this new point x?\n$EI_n(x)$ = predicted gain √ó chance it‚Äôs true + uncertainty √ó possible surprise\nEI =ÔºàÂπ≥ÂùáËÉΩÊèêÂçáÂ§öÂ∞ë √ó ÊèêÂçáÁöÑÂèØËÉΩÊÄßÔºâ + Ôºà‰∏çÁ°ÆÂÆöÊÄß √ó Áî±‰∏çÁ°ÆÂÆöÊÄßÂ∏¶Êù•ÁöÑÊΩúÂú®Êî∂ÁõäÔºâ\nFirst term: expected improvement if you trust the mean. Second term: extra improvement that might happen because the model is uncertain. The next sampling point is chosen by maximizing EI:\n$$ x_{n+1} = \\arg\\max_x EI_n(x) $$\nInterpretation:\nEI balances two goals:\nExploitation: sampling where the predicted mean $\\mu_n(x)$ is high. Exploration: sampling where uncertainty $\\sigma_n(x)$ is high. This trade-off helps the algorithm explore new areas and improve known good ones efficiently.\n4.2 Knowledge Gradient The Knowledge Gradient (KG) acquisition function measures the expected value of information gained from sampling a new point.\nUnlike Expected Improvement (EI), which focuses on immediate improvement at the sampled point, KG evaluates how much better our overall knowledge about the objective becomes after sampling.\nEI assumes the final solution must be one of the evaluated points. KG relaxes this: after we take one more sample, we can still choose any point (evaluated or not) as our final decision. Therefore, the value of sampling comes not just from finding a better local result, but from improving the entire model‚Äôs understanding of the objective surface. Mathematical Form:\n$$ KG_n(x) = E_n[\\mu_{n+1}^* - \\mu_n^*] $$\nwhere\n$\\mu_n^* = \\max_{x\u0026rsquo;} \\mu_n(x\u0026rsquo;)$: current predicted maximum, $\\mu_{n+1}^* = \\max_{x\u0026rsquo;} \\mu_{n+1}(x\u0026rsquo;)$: predicted maximum after taking a new sample at (x), The expectation $\\mathbb{E}_n[\\cdot]$ averages over possible outcomes of the new observation. Interpretation:\nKG measures the expected increase in the best achievable posterior mean after taking one new sample.\nAlgorithm 2 Simulation-based computation Purpost: estimate how much the best mean prediction might improve if we sample at x.\nSteps:\nFind the current best mean value\n$$ \\mu_n^* = \\max_{x\u0026rsquo;} \\mu_n(x\u0026rsquo;) $$\nThis is the best prediction under the current Gaussian Process (GP).\nSimulate what could happen if we sample at x\nRepeat J times (Monte Carlo simulation):\nDraw a random possible observation\n$$ y_{n+1} \\sim \\mathcal{N}(\\mu_n(x), \\sigma_n^2(x)) $$\n(equivalently, $y_{n+1} = \\mu_n(x) + \\sigma_n(x)Z ; Z\\sim\\mathcal{N}(0,1)$).\nUpdate the GP posterior using this ‚Äúimagined‚Äù observation $(x, y_{n+1})$, obtaining a new mean function $\\mu_{n+1}(\\cdot)$.\nCompute the new best mean value\n$$ \\mu_{n+1}^* = \\max_{x\u0026rsquo;} \\mu_{n+1}(x\u0026rsquo;) $$\nCompute the gain for this scenario\n$$ \\Delta^{(j)} = \\mu_{n+1}^* - \\mu_n^* $$\nAverage over all J simulations\nThe Knowledge Gradient estimate is\n$$ KG_n(x) = \\frac{1}{J}\\sum_{j=1}^J \\Delta^{(j)} $$\nAlgorithm 3: Multi-start Stochastic Gradient Ascent Goal: Find the best next sampling point $x$ that maximizes $KG_n(x)$.\nProcess:\nStart from multiple random initial points $x_0^{(r)}$ (r = 1,‚Ä¶,R).\nFor each start, perform T stochastic gradient ascent steps:\nCompute stochastic gradient $G$ (estimated using Algorithm 4).\nUpdate $x_t^{(r)} = x_{t-1}^{(r)} + \\alpha_t G$,\nwhere $\\alpha_t = a / (a + t)$ is a decreasing step size.\nAfter T steps, estimate $KG_n(x_T^{(r)})$ using simulation (Algorithm 2).\nReturn the point with the largest estimated KG value.\nNotes:\nUsing multiple random starts helps avoid local optima. This method converges to a local maximum of the KG function. Algorithm 4 ‚Äî Simulation of Stochastic Gradients Purpose: Compute an unbiased estimate of the gradient $\\nabla KG_n(x)$.\nSteps:\nFor each of J simulations: Sample a random variable $Z \\sim \\mathcal{N}(0,1)$. Generate a possible observation $y_{n+1} = \\mu_n(x) + \\sigma_n(x)Z$. Update the GP posterior using $(x, y_{n+1})$ to obtain new mean $\\mu_{n+1}$. Compute the new best posterior mean value $\\mu^*{n+1} = \\max{x\u0026rsquo;} \\mu{n+1}(x\u0026rsquo;)$. Evaluate its gradient w.r.t. the sampled point x. Average over all J samples to estimate $\\nabla KG_n(x)$. 4.3 Entropy Search and Predictive Entropy Search Entropy Search (ES) and Predictive Entropy Search (PES) are acquisition functions in Bayesian optimization that try to reduce uncertainty about the position of the global optimum $x^*$.\nInstead of asking ‚Äúwhich point will improve the function value most,‚Äù they ask ‚Äúwhich point will tell me the most about where the best value is.‚Äù\nEntropy measures uncertainty. If the posterior over the location of the global optimum x has high entropy, we are unsure where the best point is. A good new observation is one that most reduces this entropy.\nEntropy Search (ES) Purpose: Measures how much uncertainty (entropy) about the true optimum $x^*$ will go down if we sample at point x.\n$P_n(x^*)$: the current posterior belief over where the global optimum lies. $H(P_n(x^*))$: its entropy, representing uncertainty. After sampling at a candidate point x, the posterior changes to $P_n(x^*|f(x))$. The expected reduction in entropy is $$ ES_n(x) = H(P_n(x^*)) - E_{f(x)}[H(P_n(x^*|f(x)))] $$\nThis means we prefer points x where observing $f(x)$ is expected to most reduce our uncertainty about $x^*$. Computing ES directly is difficult, because it requires calculating entropy over many possible function outcomes.\nPredictive Entropy Search (PES) Purpose: Measures how much uncertainty (entropy) about the true optimum $x^*$ will go down if we sample at point x.\nPES reformulates the same idea in a simpler way. Instead of measuring the entropy of the optimum location directly, it measures the mutual information between $f(x)$ and $x^*$:\n$$ PES_n(x) = H(P_n(f(x))) - \\mathbb{E}_{x^*}[H(P_n(f(x)|x^*))] $$\nThis is mathematically equivalent to ES but easier to approximate in practice. PES estimates how much knowing the value of $f(x)$ would reduce uncertainty about where the optimum is.\nIntuitive difference from EI and KG\nExpected Improvement (EI) focuses on increasing the best function value so far. Knowledge Gradient (KG) focuses on improving the overall model prediction. ES and PES focus on learning information that narrows down the true location of the optimum. Entropy Search and Predictive Entropy Search choose sampling points that give the most information about the global optimum.\nThey are more global and information-driven than EI or KG but are computationally more complex.\n4.4 Multi-Step Optimal Acquisition Functions Bayesian optimization can be viewed as a sequential decision process: each sample depends on past results. Standard methods like EI, KG, ES, and PES are one-step optimal, choosing the next point assuming only one evaluation remains.\nA multi-step optimal strategy would plan several future evaluations ahead, maximizing total expected reward. However, computing it is extremely hard due to the dimensionality.\nRecent studies have tried approximate multi-step methods using reinforcement learning and dynamic programming, but they are not yet practical. Experiments show that one-step methods already perform nearly as well, so they remain the preferred approach in practice.\n5 Exotic Bayesian Optimization Noisy Evaluations Gaussian Process (GP) regression can handle noisy observations by adding noise variance to the covariance matrix. In practice, the noise variance is often unknown and treated as a hyperparameter. If noise varies across the domain, it can be modeled with another GP.\nAcquisition functions like EI, KG, ES, and PES naturally extend to noisy settings, but EI becomes less straightforward since the ‚Äúimprovement‚Äù is not directly observable. The KG approach is more robust under noise.\nParallel Evaluations Parallel Bayesian optimization allows evaluating several points simultaneously using multiple computing resources. Expected Improvement (EI) is extended to parallel EI, where several points $(x^{(1)}, \\dots, x^{(q)})$ are selected jointly to maximize expected improvement.\nVariants like multipoint EI and Constant Liar approximations simplify optimization. Similar extensions exist for KG, ES, and PES. Parallel versions are computationally harder but useful for speeding up optimization on modern systems.\nConstraints In real problems, sampling may be limited by constraints $g_i(x) \\ge 0$ (g is the constraint). These constraints can be as expensive to evaluate as $f(x)$. EI can be extended to check improvement only among feasible points, i.e., points that satisfy all $g_i(x) \\ge 0$.\nRecent work also studied constrained Bayesian optimization under noisy or uncertain feasibility.\nMulti-Fidelity and Multi-Information Source Evaluations Sometimes there are multiple ways to estimate the objective, each with different accuracy and cost (called fidelities). For example, $f(x, s)$ may represent evaluating $x$ with fidelity level $s$:\nlow fidelity is cheap but inaccurate high fidelity is expensive but precise The goal is to allocate a limited total budget among fidelities to maximize information gain. Methods like KG, ES, and PES can handle this setting, but EI does not generalize well because evaluating $f(x, s)$ for $s ‚â† 0$ never provides an improvement in the best objective function value seen.\nRandom Environmental Conditions and Multi-Task Bayesian Optimization Here, the objective $f(x, w)$ depends on both design variables x and random environmental variables w (e.g., weather, test fold, etc.). The aim is to optimize either the expected value $\\int f(x,w)p(w)dw$ or the sum over tasks $\\sum f(x,w)p(w)$.\nBy observing performance under different w, we can infer information about nearby conditions, reducing the need for full evaluations. This setup is widely used in engineering, machine learning (cross-validation folds), and reinforcement learning. Modified EI, KG, and PES methods apply here.\nDerivative Observations Sometimes gradient (derivative) information is available along with function values. Gradients can be incorporated into GP models to improve predictions and optimization speed. While EI does not directly benefit from derivatives, KG can use them effectively.\nGradient-based updates improve convergence and numerical stability, especially in regions where function evaluations are expensive.\n6 Software 7 Conclusion and Research Directions 7.1 Conclusion The paper reviews Bayesian Optimization (BO) including Gaussian Process (GP) regression, and key acquisition functions such as expected improvement (EI), knowledge gradient (KG), entropy search (ES), and predictive entropy search (PES). And paper extends discussion to more complex cases (noise, constraints, multi-fidelity, multi-task, etc.).\n7.2 Future Research Directions Theory and Convergence: There is a need for a deeper theoretical understanding of BO. Multi-step optimal algorithms are known to exist but are hard to compute. We lack finite-time performance guarantees and full understanding of convergence rates. Beyond Gaussian Processes: Most BO methods use GPs, but new statistical models may better capture some types of problems. Research should aim to develop alternative models suited for specific applications. High-Dimensional Optimization: Current BO struggles when the number of parameters is large. New methods should leverage structure in high-dimensional problems. Exotic Problem Structures: BO should handle more complex, real-world conditions (multi-fidelity data, environmental randomness, derivative information). Combining method development with practical applications can reveal new challenges and innovations. Real-World Impact: BO has strong potential in chemistry, materials science, and drug discovery, where experiments are expensive and slow. However, few researchers in these fields currently use BO ‚Äî so expanding awareness and applications is important. ","permalink":"/notes/a_tutorial_on_bayesian_optimization/","summary":"\u003ch1 id=\"abstraction\"\u003eAbstraction\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eBayesian Optimization\u003c/strong\u003e is a method for finding the best solution when evaluating the objective function is \u003cstrong\u003eslow or expensive\u003c/strong\u003e (taking minutes or hours).\u003c/p\u003e\n\u003cp\u003eIt is mainly used for problems with \u003cstrong\u003efewer than 20 variables\u003c/strong\u003e and where evaluations may contain \u003cstrong\u003enoise\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThe method works by:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBuilding a \u003cstrong\u003esurrogate model\u003c/strong\u003e of the unknown function using a \u003cstrong\u003eBayesian machine learning method called Gaussian Process regression\u003c/strong\u003e, which predicts both the function \u003cstrong\u003evalue\u003c/strong\u003e and its \u003cstrong\u003euncertainty\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eUsing an \u003cstrong\u003eacquisition function\u003c/strong\u003e (expected improvement, knowledge gradient or entropy search) to decide \u003cstrong\u003ewhere to sample next\u003c/strong\u003e ‚Äî balancing exploration and exploitation.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe tutorial also extends expected improvement to handle \u003cstrong\u003enoisy evaluations\u003c/strong\u003e, supported by a \u003cstrong\u003eformal decision-theoretic argument\u003c/strong\u003e, rather than ad hoc fixes.\u003c/p\u003e","title":"A Tutorial on Bayesian Optimization"},{"content":" CrossNet and CrossNet++ Both are for Reference-Based Super-Resolution (RefSR), using a low-resolution (LR) image and a high-resolution (HR reference) image to make a sharper, high-quality output.\nThe performance of CrossNet drops with the increasing of perspective parallax, the improvement of CrossNet++:\nTwo-stage warping ‚Üí improves alignment Self-supervised flow estimation ‚Üí uses FlowNet to estimate motion between LR and Ref images Cross-scale alignment ‚Üí Aligns features at multiple resolutions Hybrid loss functions ‚Üí warping + landmark + super-resolution loss Real-world performance ‚Üí produces smoother, sharper, and more realistic results, suitable for a variety of scenarios Abstraction CrossNet++ focuses on reference-based super-resolution (RefSR), improving a low-resolution image using a high-resolution reference from another camera. This task is hard because of large scale differences (8√ó) and big parallax (~10%) between the two views.\nTo solve this, CrossNet++ introduces an end-to-end two-stage network with:\nCross-scale warping modules, align images at multiple zoom levels to narrow down parallax, handle scale and parallax differences. Image encoder and fusion decoder, extract multi-scale features and combine them to reconstruct a high-quality super-resolved image. It uses new hybrid loss functions comprising warping loss, landmark loss and super-resolution loss to improve accuracy and stability by stabilizing the training of alignment module and helps to improve super-resolution performance.\ntwo-stage wrapping, hybrid loss\n1 Introduction The development of method:\npatch-matching + patch-synthesis + iteratively applying non-uniform warping Causes grid artifacts, incapable of handling the non-rigid image deformation Directly warping between the low and high-resolution images is inaccurate. Such iterative combination of patch matching and warping introduces heavy computational burden. The difference between rigid deformation and non-rigid deformation:\nRigid deformation = viewpoint change, like camera movement. Non-rigid deformation = object itself changes shape (face expression, fabric fold, petal bending). Grid artifacts = tiny square patterns caused by wrong image enlargement or alignment.\nwarping + synthesis It cannot effectively handle large-parallax cases that widely existed in real-world data. pre-warping + re-warping + synthesis CrossNet++ is a unified framework enabling fully end-to-end training which does not require pretraining the flow estimator. Two-stage pipeline: Two-stage cross-scale warping module. stage 1: Uses FlowNet to estimate motion (optical flow) between the low-resolution (LR) and reference (Ref) images without needing ground-truth flow (self-supervised). This produces a roughly aligned ‚Äúwarped-Ref‚Äù image. stage 2: Further refines alignment between the warped-Ref and LR image for more accurate warping. Hybrid loss: warping loss, landmark loss and super-resolution loss. warping loss: supervise the flow estimation implicitly. landmark loss: supervise the flow estimation explicitly. Without ground-truth flow = the model learns to estimate motion on its own, using only the images, not any pre-labeled motion data.\nInterpolation = predict inside known area Extrapolation = predict outside known area 2 Related Work 3 Preliminary of CrossNet 3.3 Network Structure 3.3.1 Alignment Module The alignment module aims to align the reference image $I_{REF}$ with the low-resolution image $I_{LR}$. CrossNet++ adopts a warping-based alignment using two-stage optical flow estimation.\nIn the first stage, a modified FlowNet (denoted as $Flow_1$) predicts the flow field between an upsampled LR image $I_{LR‚Üë}$ and the reference image $I_{REF}$:\n$$ V_1^0 = Flow_1(I_{LR‚Üë}, I_{REF}) $$ where $V_1^0$ represents the flow at scale 0 (the original image scale). The upsampled LR image $I_{LR‚Üë}$ is obtained via a single-image SR method:\n$$ I_{LR‚Üë} = SISR(I_{LR}) $$ Then, the reference image is spatially warped using this flow to produce the pre-aligned reference:\n$$ \\hat{I}_{REF} = Warp(I_{REF}, V_1^0) $$ In the second stage, the pre-aligned reference \\( \\hat{I}_{\\mathrm{REF}} \\) and the upsampled LR image \\( I_{LR}\\uparrow \\) are again input to another flow estimator \\( Flow_2 \\) to compute multi-scale flow fields:\n$$ {V_2^3, V_2^2, V_2^1, V_2^0} = Flow_2(I_{LR‚Üë}, \\hat{I}_{REF}) $$ These multi-scale flows are used later in the synthesis network to refine alignment and reconstruct the final super-resolved image.\nthis two-stage alignment, coarse warping followed by multi-scale refinement‚Äîallows CrossNet++ to handle large parallax and depth variations, achieving more accurate correspondence and better alignment quality than the original CrossNet.\n3.3.2 Encoder Through the alignment module, we obtain four flow fields at different scales. The encoder receives the pre-aligned reference image $\\hat I_{REF}$ and the upsampled LR image $I_{LR‚Üë}$, then extracts their feature maps at four different scales.\nThe encoder has five convolutional layers with 64 filters of size ( 5 $\\times$ 5 ).\nThe first two layers (stride = 1) extract the feature map at scale 0. The next three layers (stride = 2) produce lower-resolution feature maps for scales 1 to 3. These operations are defined as: where $\\sigma$ is the ReLU activation, $*_1$ and $*_2$ denote convolutions with strides 1 and 2 respectively, and $F^i$ is the feature map at scale $i$.\n$$ F^0 = \\sigma(W^0 *_{1} I) $$ $$ F^i = \\sigma(W^i *_{2} F^{i-1}), \\\\ \\quad i = 1, 2, 3, $$ Unlike the original CrossNet, CrossNet++ uses a shared encoder for both $\\hat I_{REF}$ and $I_{LR‚Üë}$ instead of two separate encoders, which reduces about 0.41 M parameters while maintaining accuracy.\nThe resulting feature sets are:\n$$ {F^0_{LR}, F^1_{LR}, F^2_{LR}, F^3_{LR}} \\quad \\text{and} \\quad {F^0_{REF}, F^1_{REF}, F^2_{REF}, F^3_{REF}}. $$ Finally, each reference feature map $F^i_{REF}$ is warped using the multi-scale flow fields $V^i_2$ from to produce the aligned feature maps:\n$$ \\hat{F}^i_{REF} = Warp(F^i_{REF}, V^i_2), \\\\ \\quad i = 0, 1, 2, 3. $$ In short, the encoder extracts multi-scale feature maps for both LR and reference images using shared convolutional layers, then aligns the reference features to the LR features through warping with multi-scale flow fields, which provides precise, scale-consistent alignment for the next fusion step.\n3.3.3 Decoder After feature extraction and alignment, the decoder fuses the LR and reference feature maps and generates the final super-resolved image.\nIt follows a U-Net-like structure, which progressively upsamples the feature maps from coarse to fine scales.\nTo create the decoder features at scale $i$, the model concatenates:\nthe warped reference features $\\hat{F}^i_{REF}$, the LR image features $F^i_{LR}$¬†, and the decoder feature from the next coarser scale $F^{i+1}_{D}$ (if available). Then a deconvolution layer (stride 2, filter size 4 $\\times$ 4) is applied:\n$$ F^3_{D} = \\sigma(W^3_{D} *_{2} (F^3_{LR}, \\hat{F}^3_{REF})) $$ where $*_2$ is deconvolution with stride 2 and $\\sigma$ is the activation (ReLU).\n$$ F^i_{D} = \\sigma(W^i_{D} *_{2} (F^i_{LR}, \\hat{F}^i_{REF}, F^{i+1}_{D})), \\\\quad i = 2, 1, $$ After that, three more convolutional layers (filter sizes (5 $\\times$ 5), channels {64, 64, 3}) perform post-fusion to synthesize the final image $I_p$:\n$$ F^0_{D} / F_1 = \\sigma(W_1 *_{1} (F^0_{LR}, \\hat{F}^0_{REF}, F^1_{D})) $$ $$ F_2 = \\sigma(W_2 *_{1} F_1) $$ $$ I_p = \\sigma(W_p *_{1} F_2), $$ where $*_{1}$¬†means convolution with stride 1.\nThe decoder takes aligned multi-scale features from LR and reference images, fuses them step by step through deconvolutions and convolutions, and finally reconstructs the high-resolution output image $I_p$, the sharp, super-resolved result.\n3.4 Loss Function warping loss, landmark loss ‚Üí encourage flow estimator to generate precise flow.\nsuper-resolution loss ‚Üí is responsible for the final synthesized image.\n3.4.1 Warping Loss Used in the first-stage Flow Estimator to regularize the generated optical flow.\nIt ensures that the warped reference image $\\hat I_{REF}$ is close to the ground-truth HR image $I_{HR}$, assuming both share a similar viewpoint.\nThe loss minimizes pixel-wise intensity differences:\n$$ L_{warp} = \\frac{1}{2N} \\sum_{i,s,c} (\\hat{I}_{REF}(s, c) - I_{HR}(s, c))^2 $$ where $N$ is the total number of samples, $i$, $s$, and $c$ iterate over training samples, spatial locations and color channels respectively.\n3.4.2 Landmark Loss This loss provides directional geometric guidance for large-parallax cases.\nIt uses SIFT feature matching to find corresponding landmark pairs $(p, q)$ between the HR and reference images, and applies the flow field $V^0_1$ to warp these landmarks.\nThe warped landmark $\\hat{p}^j$ is computed as:\n$$ \\hat{p}^j = p^j + V^0_1[p^j] $$ and the landmark loss penalizes the distance between warped and target landmarks:\n$$ L_{lm} = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{m_i} | \\hat{p}^j - q^j |_2^2 $$ where $m_i$¬†is the number of landmark pairs in image $i$.\nThis term helps the flow estimator predict more accurate motion fields, especially when viewpoints differ greatly.\n3.4.3 Super-Resolution Loss This loss directly trains the model to synthesize the final super-resolved image $I_p$, comparing it with the ground-truth high-resolution image $I_{HR}$ using the Charbonnier penalty (a smooth $L_1$ loss):\n$$ L_{sr} = \\frac{1}{N} \\sum_{i,s,c} \\rho(I_{HR}(s, c) - I_p(s, c)) $$ $$ \\rho(x) = \\sqrt{x^2 + 0.001^2}. $$ 4 Experiment Flower dataset and LFVideo dataset\n14 $\\times$ 14 angular samples of size 376 $\\times$ 541. training and testing: central 8 $\\times$ 8 grid of angular samples top-left 320 $\\times$ 512 for training and testing training: 3243 images from Flower and 1080 images from LFVideo testing: 100 images from Flower and 270 images from LFVideo ","permalink":"/notes/crossnet++_cross-scale_large-parallax_warping_for/","summary":"\u003caside\u003e\n\u003ch2 id=\"crossnet-and-crossnet\"\u003e\u003cstrong\u003eCrossNet\u003c/strong\u003e and \u003cstrong\u003eCrossNet++\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eBoth are for \u003cstrong\u003eReference-Based Super-Resolution (RefSR),\u003c/strong\u003e using a \u003cstrong\u003elow-resolution (LR)\u003c/strong\u003e image and a \u003cstrong\u003ehigh-resolution (HR reference)\u003c/strong\u003e image to make a sharper, high-quality output.\u003c/p\u003e\n\u003cp\u003eThe performance of \u003cstrong\u003eCrossNet\u003c/strong\u003e drops with the increasing of perspective parallax, the improvement of \u003cstrong\u003eCrossNet++:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTwo-stage warping\u003c/strong\u003e ‚Üí improves alignment\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-supervised flow estimation\u003c/strong\u003e ‚Üí uses \u003cstrong\u003eFlowNet\u003c/strong\u003e to estimate motion between LR and Ref images\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCross-scale alignment\u003c/strong\u003e ‚Üí Aligns features at \u003cstrong\u003emultiple resolutions\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHybrid loss functions\u003c/strong\u003e ‚Üí warping + landmark + super-resolution loss\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReal-world performance\u003c/strong\u003e ‚Üí produces smoother, \u003cstrong\u003esharper\u003c/strong\u003e, and more realistic results, suitable for a variety of scenarios\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003ch1 id=\"abstraction\"\u003eAbstraction\u003c/h1\u003e\n\u003cp\u003eCrossNet++ focuses on \u003cstrong\u003ereference-based super-resolution (RefSR),\u003c/strong\u003e improving a low-resolution image using a high-resolution reference from another camera. This task is hard because of \u003cstrong\u003elarge scale differences (8√ó)\u003c/strong\u003e and \u003cstrong\u003ebig parallax (~10%)\u003c/strong\u003e between the two views.\u003c/p\u003e","title":"CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution"},{"content":"paper resource\nRWKV = a bridge between LSTM and Transformer (less computation and memory, parallel in training). Replace self-attention with a time-mixing mechanism that behaves like an LSTM in time but a Transformer in training and inference xLSTM = scale up the LSTM family to match or even surpass Transformer-level performance. Keep the recurrent spirit of LSTM, but redesign gates and memory (mLSTM) so it can train and scale efficiently like Transformers. Speciality: memory mixing, RWKV Abstract The paper revisits LSTMs, whose key innovations are the constant error carousel and gating mechanisms ‚Äî originally solved the vanishing-gradient problem and made long-term memory possible. Although Transformers later surpassed LSTMs thanks to their parallelizable self-attention, the authors ask whether LSTMs can be scaled up, like modern LLMs, to billions of parameters while overcoming their known limits.\nTo achieve this, they introduce:\nExponential gating ‚Äî a new gating function with improved normalization and stability. Modified memory structures: sLSTM ‚Äî uses scalar memory and scalar updates with new ‚Äúmemory mixing.‚Äù ‚Üí memory mixing mLSTM ‚Äî introduces matrix-based memory that supports full parallelization and a covariance-based update rule. A new memory architecture ‚Üí parallelization By stacking these enhanced cells into residual xLSTM blocks, they create architectures that combine the strengths of LSTMs and Transformers.\nExperiments show that xLSTMs can match or even outperform Transformers and State Space Models in both performance and scaling.\nüëâ Code: github.com/NX-AI/xlstm\n1 Introduction 1.1 LSTM $$ c_t = f_t c_{t-1} + i_t z_t, \\quad h_t = o_t \\psi(c_t) $$\nUpdate the cell state / long-term memory: $$ c_t = f_t c_{t-1} + i_t z_t $$\n$c_t$: Cell state, real-valued vector, the internal long-term memory after update $f_t$: Forget gate, values in (0, 1), decide how much of $c_{t-1}$ to keep $c_{t-1}$: Previous cell state, vector, carries long-term memory $i_t$: Input gate, values in (0, 1), decides how much new info to write $z_t$: Cell input / candidate memory, usually $\\tanh(\\cdot)$ output, the new content that could be added Produce the output / hidden state / short-term memory: $$ \\quad h_t = o_t \\psi(c_t) $$\n$h_t$: Hidden state, vector, output of the cell (short-term memory) $o_t$: Output gate, values in (0, 1), controls what part of memory is shown outside $\\psi(c_t)$: Activation function (often $\\tanh(c_t$)), squashes memory to bounded range Three Main Limitations of LSTMs Can‚Äôt revise stored information Once an LSTM stores something in its cell state, it struggles to update or replace it later. xLSTM fix: introduces exponential gating, allowing flexible updating of stored values. Limited storage capacity Traditional LSTMs store information in a single scalar cell state, forcing compression and loss of details. xLSTM fix: uses a matrix memory, which can hold richer, multi-dimensional information. No parallelization LSTM depends on sequential hidden-to-hidden connections, meaning each step waits for the previous one. xLSTM fix: changes the memory mixing structure to make computation parallelizable across time steps. 2 Extended LSTM Two main modifications: exponential gating and novel memory structures. Two variants mombined into xLSTM blocks, stacked with residual connections to build xLSTM architectures, both can have multiple memory cells and heads: sLSTM ‚Äì scalar memory, scalar update, memory mixing across cells. mLSTM ‚Äì matrix memory, covariance (outer product) update, fully parallelizable. 2.2 sLSTM sLSTM = LSTM + exponential gates + normalization state + stabilizer state + multiple memory cells.\nThe exponential gates $i_t$ and $f_t$ make it easier to amplify or reduce memory dynamically.\n‚Üí Helps sLSTM revise stored information better (a key limitation of classical LSTM).\nThe normalizer state $n_t$ keeps things numerically stable, so exponential growth doesn‚Äôt blow up.\nThe stabilizer state $m_t$ keeps their scale controlled, prevents numerical overflow during training.\nThe Multiple heads each with its own LSTM-like structure to compute its own $h_t$, then combined, just like multi-head attention in Transformers, allowing the network to learn different kinds of temporal patterns in parallel.\nNew Memory Mixing: In an sLSTM, each time step has multiple memory cells ‚Üí a vector computed by recurrent matrices R, each cell stores part of the long-term memory, we allow these memory cells to talk to each other.\nMemory mixing = different parts (dimensions) of the memory cells communicate and influence each other.\n2.3 mLSTM mLSTM = LSTM + exponential gates + normalization state + stabilizer state + multiple memory cells.\nmLSTM replaces the small one-number memory $c_t$ of LSTM with a key‚Äìvalue memory matrix, so it can store, search, and update information like attention, but still works as a recurrent network (RNN).\n$q_k, k_t, v_t$ ‚Üí same like query, key, value in transformer‚Ä¶ uses a matrix memory because it wants to store relationships between features (keys and values), not just single values like traditional LSTM. The normalizer state $n_t$ is the weighted sum of key vectors, keeps record of the strength of the gates. Multiple heads and multiple cells are equivalent as there is no memory mixing. 2.4 xLSTM Architecture 2.4.1 xLSTM Blocks Each block takes an input (sequence or features), passes it through an sLSTM or mLSTM cell, adds some non-linear layers (MLPs) and residual/skip connections, finally outputs a transformed sequence representation.\nPatterns are easier to separate after mapping into a higher-dimensional space. Like for better points classification, we can map each point from 2D ‚Üí 3D.\nWhen an xLSTM processes a sequence, it wants to distinguish different histories, for example:\n‚ÄúThe dog chased the cat‚Äù vs ‚ÄúThe cat chased the dog.‚Äù These sequences may look similar in lower dimensions (both use same words), but when we map them into a higher-dimensional representation, the model can more easily tell them apart.\nSo each xLSTM block:\nexpands data into a higher space (‚Äúup-projection‚Äù), applies non-linear transformations, and then compresses back (‚Äúdown-projection‚Äù). That makes it easier for the model to separate different contexts or meanings.\nType Memory type Recurrent connections Parallelization Up-proj position Storage capacity sLSTM Post up-projection Scalar memory (vector) ‚úÖ via matrices R ‚ùå¬†Sequential after LSTM Smaller mLSTM Pre up-projection Matrix memory ‚ùå¬†No recurrent matrices ‚úÖ parallelizable before LSTM Much larger 2.4.2 xLSTM Architecture Figure 1: The extended LSTM (xLSTM) family.\nFrom left to right:\nThe original LSTM memory cell with constant error carousel and gating. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule. mLSTM and sLSTM in residual blocks yield xLSTM blocks. Stacked xLSTM blocks give an xLSTM architecture. The constant error carousel is the additive update of the cell state $c_{t‚àí1}$ (green) by cell inputs $z_t$ and moderated by sigmoid gates (blue).\nThe gating mechanisms:\nForget gate decides what to erase. Input gate decides what to add. Output gate decides what to show. 4 Experiments LSTM and xLSTM models far outperform Transformers and State Space Models on tasks that need long-term memory and state tracking; xLSTM, especially when combining sLSTM + mLSTM, achieves the best all-around performance, showing that recurrent memory architectures still beat attention models for logical and structured reasoning.\nThe paper uses perplexity (ppl) as the main evaluation metric for language modeling. It measures how well the model predicts the next token in a text sequence.\nThe model is confident and accurate ‚Üí the model gives high probability to the correct next word ‚Üí it‚Äôs confident ‚Üí low perplexity. The model is confused and often wrong ‚Üí the model gives low probability ‚Üí it‚Äôs uncertain or wrong ‚Üí high perplexity. Scaling Laws Next token prediction perplexity of xLSTM, RWKV-4, Llama, and Mamba on the SlimPajama validation set when trained on 300B tokens from SlimPajama. Model sizes are 125M, 350M, 760M, and 1.3B. The scaling laws indicate that for larger models xLSTM will perform well too.\n5 Limitations sLSTM not parallelizable:\nIts memory mixing prevents full parallel execution. Custom CUDA version is faster, but still ~2√ó slower than mLSTM. mLSTM kernels not optimized:\nCurrent CUDA implementation is ~4√ó slower than FlashAttention. Could be improved with better GPU kernels. High computation cost:\nmLSTM processes (d \\times d) matrices, which increases compute load,\nthough it can still be parallelized using standard matrix ops.\nGate initialization sensitivity:\nForget-gate parameters must be tuned carefully for stability. Memory limits at long contexts:\nLarge matrix memory may overload at very long sequence lengths,\nbut works fine up to ~16k tokens.\nNot fully optimized yet:\nArchitecture and hyperparameters weren‚Äôt exhaustively tuned due to cost. More optimization could further boost performance. ","permalink":"/notes/xlstm_extended_long_short-term_memory/","summary":"\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2405.04517\"\u003epaper resource\u003c/a\u003e\u003c/p\u003e\n\u003caside\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRWKV\u003c/strong\u003e = a \u003cem\u003ebridge\u003c/em\u003e between LSTM and Transformer (less computation and memory, parallel in training). Replace self-attention with a time-mixing mechanism that behaves like an LSTM in time but a Transformer in training and inference\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003exLSTM\u003c/strong\u003e = s\u003cem\u003ecale up\u003c/em\u003e the LSTM family to match or even surpass Transformer-level performance. Keep the recurrent spirit of LSTM, but redesign gates and memory (mLSTM) so it can train and scale efficiently like Transformers.\n\u003cul\u003e\n\u003cli\u003eSpeciality: memory mixing, \u003cdel\u003eRWKV\u003c/del\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003eThe paper revisits \u003cstrong\u003eLSTMs\u003c/strong\u003e, whose key innovations are the \u003cstrong\u003econstant error carousel\u003c/strong\u003e and \u003cstrong\u003egating mechanisms\u003c/strong\u003e ‚Äî originally solved the vanishing-gradient problem and made long-term memory possible. Although \u003cstrong\u003eTransformers\u003c/strong\u003e later surpassed LSTMs thanks to their \u003cstrong\u003eparallelizable self-attention\u003c/strong\u003e, the authors ask whether LSTMs can be scaled up, like modern LLMs, to \u003cstrong\u003ebillions of parameters\u003c/strong\u003e while overcoming their known limits.\u003c/p\u003e","title":"xLSTM: Extended Long Short-Term Memory"},{"content":"Abstract Transformers are very powerful for language tasks and training is ****faster on GPUs because of parallization, but they use a lot of memory and computing power, especially when processing long text, their cost grows very fast (quadratically).\nRNNs, on the other hand, use less memory and computation cause they grow linearly but are slower to train and not as good at handling long sentences.\nThe new model RWKV mixes the good parts of both, it trains quickly and gets good performance like a Transformer and also runs efficiently like an RNN.\nMemory and computation:\nIn a Transformer, each token looks at every other token through self-attention. If you have N tokens in a sentence, it compares every pair, so total comparisons = N √ó N = N¬≤. That‚Äôs why Memory and computation both grow quadratically. In an RNN, the model reads tokens one by one, passing information step by step. So for N tokens, it just does N steps, the total cost = N (linear). 1 Introduction RWKV reformulates the attention mechanism with a variant of linear attention, replacing traditional dot-product token interaction with more effective channel-directed attention. This implementation, without approximation, offers the lowest computational and memory complexity.\n2 Background 2.1 Recurrent Neural Networks (RNNs) Popular RNN architectures such as LSTM and GRU. Although these RNNs can be factored into two linear blocks (W and U) and an RNN-specific block, the data dependency relying on previous time steps prohibits parallelizing these typical RNNs.\n$$ \\begin{aligned} f_t \u0026= \\sigma_g\\left(W_f x_t + U_f h_{t-1} + b_f\\right), \\\\ i_t \u0026= \\sigma_g\\left(W_i x_t + U_i h_{t-1} + b_i\\right), \\\\ o_t \u0026= \\sigma_g\\left(W_o x_t + U_o h_{t-1} + b_o\\right), \\\\ \\tilde{c}_t \u0026= \\sigma_c\\left(W_c x_t + U_c h_{t-1} + b_c\\right), \\\\ c_t \u0026= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t, \\\\ h_t \u0026= o_t \\odot \\sigma_h(c_t). \\end{aligned} $$ Model Depends on Parallelizable? RNN computed previous state ‚ùå No RWKV raw previous input ‚úÖ Yes (for training) 2.2 Transformers and AFT Standard Transformer self-attention Matrix form\n$$ \\text{Attn}{Attn}(Q,K,V)=\\text{softmax}(QK^\\top)V $$ Per token (t)\n$$ \\text{Attn}{Attn}(Q,K,V)_t=\\frac{\\sum_{i=1}^{T}\\exp(q_t^\\top k_i) \\odot v_i}{\\sum_{i=1}^{T}\\exp(q_t^\\top k_i)} $$ The weighted value equation in multi-head attention. Weight of token $i$ for query $t$ is $\\exp(q_t^\\top k_i)$. Output is a weighted average of $v_i$ AFT (Attention-Free Transformer) variant $$ \\text{Attn}{Attn}^{+}(W,K,V)_t=\\frac{\\sum_{i=1}^{t}\\exp(w_{t,i}+k_i)\\odot v_i}{\\sum_{i=1}^{t}\\exp(w_{t,i}+k_i)} $$ Replace $q_t^\\top k_i$ with (learned) position bias $w_{t,i}$ + a key score $k_i$. Causal: sum only $i\\le t$. Still a normalized weighted average, but weights depend on position via $w_{t,i}$. Variables:\ni: past token index, what we‚Äôre reading from memory t: current token index, what we‚Äôre generating now RWKV‚Äôs simplification Define the bias as a decay with distance:\n$$ w_{t,i}=-(t-i)w \\qquad w\\in(\\mathbb{R}{\\ge 0})^d $$ Per-channel vector $w$ ‚áí older tokens get weight $e^{-(t-i)w}\\le 1$. Now weights depend only on how far back a token is, not on $q_t$. This structure lets us keep running sums, so we don‚Äôt need all pairwise scores. Result:\nSame attention-like effect, but computed with O(T) time and O(1) memory per step (like an RNN), while still trainable in parallel across tokens (like a Transformer) by prefix-scan tricks.\nModel Has Query? How it computes weights Complexity Key idea Transformer ‚úÖ Yes all tokens attend to all O(N¬≤) Full pairwise attention, strong but heavy AFT ‚ùå No Uses position bias O(N) Removes query, uses learned positional weights RWKV ‚ùå No Adds time-decay weights O(N) AFT idea + RNN-style update (linear, efficient) 3 RWKV Four fundamental elements:\nR: The Receptance vector acts as the receiver of past information. W: The Weight signifies the positional weight decay vector, a trainable parameter within the model. K: The Key vector performs a role analogous to K in traditional attention mechanisms. V: The Value vector functions similarly to V in conventional attention processes. The difference between Time Mix and Channel Mix:\nTime Mix: Builds each token‚Äôs vector using time order and previous token info Channel Mix: Refines each token‚Äôs vector by mixing internal dimensions (features). It processes each token separately to mix and refine its feature channels. 3.1 Architecture The RWKV model is composed of stacked residual blocks. Each block consists of a time-mixing and a channel-mixing sub-block, embodying recurrent structures to leverage past information.\nThis model uses a unique attention-like score update process, which includes a time-dependent softmax operation improving numerical stability and mitigating vanishing gradients. It ensures that the gradient is propagated along the most relevant path. Additionally, layer normalization incorporated within the architecture aids in stabilizing the gradients, effectively addressing both vanishing and exploding gradient issues.\n3.1.1 Token Shift In this architecture, all linear projection vectors (R, K, V in time-mixing, and R‚Ä≤, K‚Ä≤ in channel- mixing) involved in computations are produced by linear interpolation between current and previous timestep inputs, facilitating a token shift.\nThe vectors for time-mixing computation are linear projections of linear combinations of the current and previous inputs of the block:\n$$ \\begin{aligned} r_t \u0026= W_r \\cdot \\left(\\mu_r \\odot x_t + (1 - \\mu_r) \\odot x_{t-1}\\right), \\\\ k_t \u0026= W_k \\cdot \\left(\\mu_k \\odot x_t + (1 - \\mu_k) \\odot x_{t-1}\\right), \\\\ v_t \u0026= W_v \\cdot \\left(\\mu_v \\odot x_t + (1 - \\mu_v) \\odot x_{t-1}\\right). \\end{aligned} $$ The channel-mixing inputs:\n$$ \\begin{aligned} r'_t \u0026= W'_{r} \\cdot \\left(\\mu'_{r} \\odot x_t + (1 - \\mu'_{r}) \\odot x_{t-1}\\right), \\\\ k'_t \u0026= W'_{k} \\cdot \\left(\\mu'_{k} \\odot x_t + (1 - \\mu'_{k}) \\odot x_{t-1}\\right). \\end{aligned} $$ 3.1.2 WKV Operator $$ wkv_t = \\frac{\\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} \\odot v_i + e^{u + k_t} \\odot v_t}{\\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} + e^{u + k_t}} $$ The difference of treating W between AFT and RWKV:\nPairwise matrix: Each token pair has its own weight Channel-wise vector: One decay weight per feature channel 3.1.3 Output Gating Time Mixing:\n$$ o_t = W_o \\cdot (\\sigma(r_t) \\odot wkv_t) $$ Channel Mixing:\n$$ o'_t = \\sigma(r'_t) \\odot (W'_v \\cdot \\max(k'_t, 0)^2) $$ 4 Trained Models and Computing Costs Adam optimizer, bfloat16 precision, the auxiliary loss introduced by PaLM‚Ä¶\n4.2 Scaling Laws Scaling laws in language models refer to the mathematical relationships that describe how the performance of a language model changes with respect to various factors.\nScaling laws are important for two primary reasons:\nthey allow us to make predictions and plans regarding the costs and performance of large models before they are trained via interpolation and extrapolation. the contexts in which they fail provides rich feedback on important areas for future research. Explain Interpolation and Extrapolation:\nInterpolation: predicting within the range of data you already know. For example, you trained models with 1B, 5B, and 10B parameters, then you interpolate to guess how a 7B model will perform. Extrapolation: predicting beyond the known range. For example, you trained up to 10B, and now you try to estimate performance of a 100B model. 5 Evaluation Evaluation direction and questions:\nCompetitiveness: Tests if RWKV performs as well as Transformers when both use the same computing power.\nLong Context: Tests if RWKV can handle very long texts better than Transformers,\nespecially when Transformers become too slow or costly for long sequences.\n6 Inference Experiments float32 precision, the HuggingFace Transformers,\n7 Future Work 1. Increase Model Expressivity\nImprove time-decay formulas. Explore better initialization of model states. Goal: more powerful representations without losing efficiency. 2. Improve Computational Efficiency\nUse parallel scan in $wkv_t$¬†step. Target complexity: $O(B \\log(T)d)$. 3. Apply to Encoder‚ÄìDecoder Architectures\nReplace cross-attention with RWKV mechanism. Useful for seq2seq and multimodal models. Boosts efficiency in both training and inference. 4. Use of Model State (Context)\nUsed for interpretability and predictability. Could enhance safety and control via prompt tuning. Modify hidden states to guide model behavior. 5. Larger Internal States\nImprove long-term memory and context understanding. Increase performance across various tasks. 8 Conclusion We introduced RWKV, a new approach to RNN models exploiting the potential of time-based mixing components. RWKV introduces several key strategies that allow it to capture locality and long-range dependencies while addressing limitations of current architectures by: (1) replacing the quadratic QK attention with a scalar formulation at linear cost, (2) reformulating recurrence and sequential inductive biases to enable efficient training parallelization and efficient inference, and (3) enhancing training dynamics using custom initializations.\nWe benchmark the proposed architecture in a wide variety of NLP tasks and show comparable performance to SoTA with reduced cost. Further experiments on expressivity, interpretability, and scaling showcase the model capabilities and draw parallels in behavior between RWKV and other LLMs.\nRWKV opens a new route for scalable and efficient architectures to model complex relationships in sequential data. While many alternatives to Transformers have been proposed with similar claims, ours is the first to back up those claims with pretrained models with tens of billions of parameters.\n9 Limitations 1. Performance Limitation (Memory Loss over Long Contexts)\nLinear attention is efficient but may lose fine details over long sequences. RWKV compresses history into a single vector, unlike Transformers that keep all token interactions. Its recurrent design limits ability to fully ‚Äúlook back‚Äù at distant tokens. 2. Dependence on Prompt Engineering\nRWKV relies more on well-designed prompts than Transformers. Poor prompts may cause information loss between prompt and continuation. ","permalink":"/notes/rwkv/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003eTransformers are very powerful for language tasks and training is ****faster on GPUs because of \u003cstrong\u003eparallization\u003c/strong\u003e, but they use \u003cstrong\u003ea lot of memory and computing power\u003c/strong\u003e, especially when processing long text, their cost grows \u003cstrong\u003every fast\u003c/strong\u003e (quadratically).\u003c/p\u003e\n\u003cp\u003eRNNs, on the other hand, use \u003cstrong\u003eless memory and computation\u003c/strong\u003e cause they grow linearly but are \u003cstrong\u003eslower to train\u003c/strong\u003e and not as good at handling long sentences.\u003c/p\u003e\n\u003cp\u003eThe new model \u003cstrong\u003eRWKV\u003c/strong\u003e mixes the good parts of both, it trains quickly and gets good performance like a Transformer and also runs efficiently like an RNN.\u003c/p\u003e","title":"RWKV: Reinventing RNNs for the Transformer Era"},{"content":"Abstract The game of Go:\nThe most challenging of classic games for AI, because:\nEnormous search space The difficulty of evaluating board positions and moves Concept Meaning Example in Go AI Solution Enormous search space Too many possible moves and future paths ‚Üí impossible to explore all At every turn, Go has ~250 legal moves; across 150 moves ‚Üí (250^{150}) possibilities Policy network narrows down the choices (reduces breadth of search) Hard-to-evaluate positions Even if you know the board, it‚Äôs hard to know who‚Äôs winning Humans can‚Äôt easily assign a numeric score to a mid-game position Value network predicts win probability (reduces depth of search) AlphaGo Imagine AlphaGo is a smart player who has:\nintuition ‚Üí from the policy network judgment ‚Üí from the value network planning ability ‚Üí from MCTS Integrating deep neural networks with Monte Carlo Tree Search (MCTS).\nThe main innovations include:\nTwo Neural Networks: Policy Network: Selects promising moves ‚Üí the probability of each move. Value Network: Evaluates board positions ‚Üí the likelihood of winning. Training Pipeline: Supervised Learning (SL) from expert human games to imitate professional play. Reinforcement Learning (RL) through self-play, improving beyond human strategies. Integration with MCTS: Combines the predictive power of neural networks with efficient search. Reduces: breadth (number of moves to consider) depth (number of steps to simulate) of search. MCTS It first adds all legal moves (children) under the current position in the tree. In every simulation, AlphaGo chooses one branch to go deeper into the tree (not all of them). It decides which one based on three main metrics: Policy Prior (P) ‚Üí the probability of the move from policy network Visit Count (N) ‚Üí how many times we‚Äôve already explored this move during simulations. Q-Value (Q) ‚Üí average win rate from past simulations Symbol Meaning Source Role in decision P(s,a) Policy prior (initial move probability) From policy network Guides initial exploration N(s,a) Number of times this move was explored Counted during simulations Balances exploration vs exploitation Q(s,a) Average predicted win rate (past experience) From value network results of simulations Exploitation: ‚Äúkeep doing what worked‚Äù Results:\nWithout search, AlphaGo already played at the level of strong Go programs. With the neural-network-guided MCTS, AlphaGo achieved a 99.8% win rate against other programs. It became the first program ever to defeat a human professional Go player (Fan Hui, European champion) by 5‚Äì0. Introduction Optimal value function $v^*(s)$ For any game state $s$, there exists a theoretical function that tells who will win if both players play perfectly. Computing this function exactly means searching through all possible sequences of moves. Search space explosion Total possibilities ‚âà $b^d$, where $b$: number of legal moves (breadth), $d$: game length (depth). For Go: ( $b$ ‚âà 250, $d$ ‚âà 150) ‚Üí bigggg number ‚Üí impossible to compute exhaustively. Reducing the search space ‚Äî two key principles: (1) Reduce depth using an approximate value function $v(s)$: Stop (truncate) deep search early. Use an approximate evaluator to predict how good a position is instead of exploring all future moves. This worked in chess, checkers, and Othello, but was believed to be impossible (‚Äúintractable‚Äù) for Go because Go‚Äôs positions are much more complex. (2) Reduce breadth using a policy $p(a|s)$: Instead of exploring all moves, only sample the most likely or promising ones. This narrows down which actions/moves to consider, saving enormous computation. Example: Monte Carlo rollouts: Simulate random games (using the policy) to estimate how good a position is. Maximum depth without branching at all, by sampling long sequences of actions for both players from a policy $p$. This method led to strong results in simpler games (backgammon, Scrabble), and weak amateur level in Go before AlphaGo. ‚ÄúSimulate‚Äù and ‚ÄúRoll out‚Äù basically mean the same thing in this context.\n‚ÄúSimulate‚Äù ‚Üí a general word: to play out an imaginary game in your head or computer. ‚ÄúRoll out‚Äù ‚Üí a more specific term from Monte Carlo methods, meaning ‚Äúplay random moves from the current position until the game ends.‚Äù So ‚Üí every rollout is one simulation of a complete (or partial) game.\nrollout = one simulated playthrough. Monte Carlo Rollout Monte Carlo rollout estimates how good a position is by:\nStarting from a given board position (s). Playing many simulated games to the end (using policy-guided moves ‚Üí reduce breadth). Recording each game‚Äôs result (+1 for win, ‚àí1 for loss). Averaging all outcomes to estimate the win probability for that position. $$ v(s) \\approx \\text{average(win/loss results from rollouts)} $$\nGoal:\nApproximate the value function $v(s)$, the expected chance of winning from position $s$.\nIt‚Äôs simple but inefficient ‚Äî great for small games, too slow and noisy for Go.\nMonte Carlo tree search MCTS uses Monte Carlo rollouts to estimate the value of each state. As more simulations are done, the search tree grows and values become more accurate. It can theoretically reach optimal play, but earlier Go programs used shallow trees and simple, hand-crafted policies or linear value functions (not deep learning). These older methods were limited because Go‚Äôs search space was too large. Training pipeline of AlphaGo Deep convolutional neural networks (CNNs) can represent board positions much better. So AlphaGo uses CNNs to reduce the search complexity in two ways: Evaluating positions with a value network ‚Üí replaces long rollouts (reduces search depth). Sampling moves with a policy network ‚Üí focuses on likely moves (reduces search breadth). Together, this lets AlphaGo explore much more efficiently than traditional MCTS. Supervised Learning (SL) Policy Network $p_\\sigma$: trained from human expert games. Fast Policy Network $p_\\pi$: used to quickly generate moves during rollouts. Reinforcement Learning (RL) Policy Network $p_\\rho$: improves the SL policy through self-play, optimizing for winning instead of just imitating humans. Value Network $v_\\theta$: predicts the winner from any board position based on self-play outcomes. Final AlphaGo system = combines policy + value networks inside MCTS for strong decision-making. Supervised learning of policy networks Panel(a): Better policy-network accuracy in predicting expert moves ‚Üí stronger actual gameplay performance.\nThis proves that imitation learning (supervised policy $p_œÉ$) already provides meaningful playing ability before any reinforcement learning or MCTS.\nFast Rollout Policy networks $p_\\pi(a|s)$\nA simpler and faster version of the policy network used during rollouts in MCTS. Uses a linear softmax model on small board-pattern features (not deep CNN). Much lower accuracy (24.2 %) but extremely fast takes only 2 ¬µs per move (vs. 3 ms for the full SL policy). Trained with the same supervised learning principle on human moves. Reinforcement learning of policy networks Structure of the policy network = SL policy network initial weights œÅ = œÉ Step What happens What‚Äôs learned Initialize Copy weights from SL policy (œÅ = œÉ) Start with human-like play Self-play Pick current p and an older version p Generate thousands of full games (self-play) Reward +1 for win, ‚àí1 for loss Label each move sequence, and collect experience (state, action, final reward) Update Update weights œÅ by SGD Policy network Repeat Thousands of games Stronger, self-improving policy Reinforcement learning of value networks Step What happens What‚Äôs learned Initialize Start from the trained RL policy network; use it to generate self-play games Provides realistic, high-level gameplay data Self-play RL policy network plays millions of games against itself Produce diverse board positions and their final outcomes (+1 win / ‚àí1 loss) Sampling Randomly select one position per game to form 30 M independent (state, outcome) pairs Avoids correlation between similar positions Labeling Each position (s) labeled with the final game result (z) Links every board state to its real win/loss outcome Training Train the value network (v_Œ∏(s)) by minimizing MSE Learns to predict winning probability directly from a position Evaluation Compare against Monte Carlo rollouts (pœÄ, pœÅ) Matches rollout accuracy with 15 000√ó less computation Result MSE ‚âà 0.23 (train/test), strong generalization Reliable position evaluation for use in MCTS Problem of naive approach of predicting game outcomes from data consisting of complete games:\nThe value network was first trained on all positions from the same human games. Consecutive positions were almost identical and had the same win/loss label. The network memorized whole games instead of learning real position evaluation ‚Üí overfitting (MSE = 0.37 test). Solution\nGenerate a new dataset: 30 million self-play games, take only one random position per game. Each sample is independent, so the network must learn general Go patterns, not memorize. Result: good generalization (MSE ‚âà 0.23) and accurate position evaluation. Searching with policy and value networks (MCTS) Panel Step What happens Which network helps a Selection Traverse the tree from root to a leaf by selecting the move with the highest combined score Q + u(P). Uses Q-values (average win) and policy priors P (from policy network). b Expansion When reaching a leaf (a position never explored before), expand it: generate its possible moves and initialize their prior probabilities using the policy network RL policy network c Evaluation Evaluate this new position in two ways: ‚ë† Value network (v_Œ∏(s)): predicts win probability instantly. ‚ë° Rollout with fast policy p_œÄ: quickly play random moves to the end, get final result (r). Value net + Fast policy d Backup Send the evaluation result (average of (v_Œ∏(s)) and (r)) back up the tree ‚Äî update each parent node‚Äôs Q-value (mean of all results from that branch). None directly (update step) The core idea Each possible move/edge (s, a) in the MCTS tree stores 3 key values:\nSymbol Meaning Source P(s,a) Prior probability ‚Äî how promising this move looks before searching From the policy network N(s,a) How many times this move has been tried From search statistics Q(s,a) Average win rate from playing move a at state s From past simulations Step 1: Selection ‚Äî choose which move to explore next At each step of a simulation, AlphaGo selects the move $a_t$ that maximizes:\n$$ a_t = \\arg\\max_a [Q(s_t, a) + u(s_t, a)] $$\nwhere the bonus term $u(s,a)$ encourages exploration:\n$$ u(s,a) \\propto \\frac{P(s,a)}{1 + N(s,a)} $$\n$Q(s,a)$: ‚ÄúHow good this move has proven so far.‚Äù $u(s,a)$: ‚ÄúHow much we should still explore this move.‚Äù ‚Üí Moves that are both good (high Q) and underexplored (low N) get priority.\nAs N increases, the bonus term shrinks ‚Äî the search gradually focuses on the best moves.\nStep 2: Expansion When the search reaches a leaf (a position not yet in the tree):\nThe policy network $p_\\sigma(a|s)$ outputs a probability for each legal move. Those values are stored as new P(s,a) priors for the new node. Initially $N(s,a) = 0$ $Q(s,a) = 0$ Now the tree has grown ‚Äî this new node represents a new possible future board.\nStep 3: Evaluation ‚Äî estimate how good the leaf is Each leaf position $s_L$ is evaluated in two ways:\nValue network $v_Œ∏(s_L)$: directly predicts win probability. Rollout result $z_L$: fast simulation (using the fast rollout policy $p_œÄ$) until the game ends +1 if win ‚àí1 if loss. Then AlphaGo combines the two results:\n$$ V(s_L) = (1 - Œª)v_Œ∏(s_L) + Œªz_L $$\n$Œª$ = mixing parameter (balances between value net and rollout). If $Œª$ = 0.5, both count equally. Step 4: Backup ‚Äî update the tree statistics The leaf‚Äôs evaluation $V(s_L)$ is propagated back up the tree:\nEvery move (edge) $(s, a)$ that was used to reach that leaf gets updated:\n$$ N(s,a) = \\sum_{i=1}^{n} 1(s,a,i) $$\n$$ Q(s,a) = \\frac{1}{N(s,a)} \\sum_{i=1}^{n} 1(s,a,i) V(s_L^i) $$\n$1(s,a,i)$ = 1 if that move was part of the i-th simulation, else 0. $V(s_L^i)$ = evaluation result from that simulation‚Äôs leaf. So, Q(s,a) becomes the average value of all evaluations ( $r$ and $vŒ∏$) in its subtree.\nStep 5: Final move decision After thousands of simulations, the root node has a set of moves with:\n$P(s_0, a)$: from policy network, $Q(s_0, a)$: average win rate, $N(s_0, a)$: visit counts. AlphaGo chooses the move with the highest visit count (N) ‚Äî the most explored and trusted move.\nWhy SL policy network performed better than RL policy network for MCTS?\nPolicy Behavior Effect in MCTS SL policy Mimics human experts ‚Üí gives a diverse set of good moves MCTS can explore several promising branches efficiently RL policy Optimized for winning ‚Üí focuses too narrowly on top 1‚Äì2 moves MCTS loses diversity ‚Üí gets less exploration benefit So, for MCTS‚Äôs exploration stage, a broader prior (SL policy) performs better.\nBut for value estimation, the RL value network is superior ‚Äî because it predicts winning chances more accurately.\nImplementation detail Evaluating policy \u0026amp; value networks takes much more compute than classical search. AlphaGo used: 40 search threads, 48 CPUs, 8 GPUs for parallel evaluation. The final system ran asynchronous multi-threaded search: CPUs handle the tree search logic, GPUs compute policy and value network evaluations in parallel. This allowed AlphaGo to efficiently combine deep learning with massive search.\nAll programs were allowed 5 s of computation time per move.\nDiscussion In this work we have developed a Go program, based on a combination of deep neural networks and tree search. We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.\nSelect those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network. Policy network ‚Üí ‚Äúprobability of choosing a move‚Äù\nValue network ‚Üí ‚Äúprobability of winning from a position‚Äù\n","permalink":"/notes/mastering-go-mcts/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eThe game of Go:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe most challenging of classic games for AI, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnormous search space\u003c/li\u003e\n\u003cli\u003eThe difficulty of evaluating board positions and moves\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eConcept\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n          \u003cth\u003eExample in Go\u003c/th\u003e\n          \u003cth\u003eAI Solution\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eEnormous search space\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eToo many possible moves and future paths ‚Üí impossible to explore all\u003c/td\u003e\n          \u003ctd\u003eAt every turn, Go has ~250 legal moves; across 150 moves ‚Üí (250^{150}) possibilities\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003ePolicy network\u003c/strong\u003e narrows down the choices (reduces \u003cem\u003ebreadth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eHard-to-evaluate positions\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eEven if you know the board, it‚Äôs hard to know who‚Äôs winning\u003c/td\u003e\n          \u003ctd\u003eHumans can‚Äôt easily assign a numeric score to a mid-game position\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eValue network\u003c/strong\u003e predicts win probability (reduces \u003cem\u003edepth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"alphago\"\u003e\u003cstrong\u003eAlphaGo\u003c/strong\u003e\u003c/h2\u003e\n\u003caside\u003e\n\u003cp\u003eImagine AlphaGo is a \u003cem\u003esmart player\u003c/em\u003e who has:\u003c/p\u003e","title":"Mastering the game of Go with MCTS and Deep Neural Networks"},{"content":"Abstract What‚Äôs the Reference-based Super-resolution (RefSR) Network:\nSuper-resolves a low-resolution (LR) image given an external high-resolution (HR) reference image The reference image and LR image share similar viewpoint but with significant resolution gap (8√ó). Solve the problem Existing RefSR methods work in a cascaded way such as patch matching followed by synthesis pipeline with two independently defined objective functions Divide the image into many small patches (like tiny squares), each patch is compared with a reference image to find its most similar region. But every patch makes its decision independently. ‚Üí Inter-patch misalignment Because of the small misalignments, the grid of patch boundaries in the final image shows. ‚Üí Grid artifacts Old methods trained two steps separately ‚Üí Inefficient training The challenge large-scale (8√ó) super-resolution problem the spatial resolution is increased by 8 times in each dimension (width and height). So the total number of pixels increases from 8√ó8 to 64√ó64. patch matching ‚Üí warping Structure:\nimage encoders extract multi-scale features from both the LR and the reference images cross-scale warping layers spatially aligns the reference feature map with the LR feature map warping module originated from spatial transformer network (STN) fusion decoder aggregates feature maps from both domains to synthesize the HR output Scale Resolution (relative) Example size What it focuses on Scale 0 √ó1 (full resolution) 512√ó512 Fine details (small shifts) Scale 1 √ó2 smaller 256√ó256 Medium motions Scale 2 √ó4 smaller 128√ó128 Larger motions Scale 3 √ó8 smaller 64√ó64 Very large motions Scale 4‚Äì5 √ó16, √ó32 smaller 32√ó32, 16√ó16 Extremely coarse view (too little detail) Result Using cross-scale warping, our network is able to perform spatial alignment at pixel-level in an end-to-end fashion, which improves the existing schemes both in precision (around 2dB-4dB) and efficiency (more than 100 times faster).\nspatial alignment at pixel-level ‚Üí precision and efficiency precision efficiency 1. Introduction The two critical issues in RefSR:\nImage correspondence between the two input images High resolution synthesis of the LR image. The ‚Äòpatch maching + synthesis‚Äô pipeline ‚Üí the end-to-end CrossNet ‚Üí results comparisons.\nüí° Flow estimator Input: feature maps from LR and Ref encoders.\nComputation:\nThe module compares these features (using convolutions) and learns to predict displacement vectors (optical flow).\nOutput: a flow map $F(x, y) = (\\Delta x, \\Delta y)$.\nUse: the warping layer applies this flow map to the reference feature map:\n$$ \\tilde{R}(x, y) = R(x + \\Delta x, y + \\Delta y) $$\nso the warped reference aligns with the LR image.\nNon-rigid deformation: when an object changes its shape or structure in the image (for example, bending, twisting, or changing due to different camera angles).\nRigid = only simple shifts, rotation, or scaling. Non-rigid = more complex distortions ‚Äî like bending, stretching, or perspective change. Grid artifacts: visible blocky or checker-like patterns that appear because the image was reconstructed from many small, rigid square patches that don‚Äôt align smoothly.\nGrid artifacts occur when an image is reconstructed from many small square patches that don‚Äôt align perfectly at their borders. The Laplacian is a mathematical operator that measures how much a pixel value differs from its surroundings.\nIn other words, it tells you where the image changes quickly ‚Äî that‚Äôs usually at edges or texture details.\n2. Related Work Multi-scale deep super resolution we employ MDSR as a sub-module for LR images feature extraction and RefSR synthesis.\nMDSR stands for Multi-scale Deep Super-Resolution Network Used for Feature extraction ‚Üí understanding what‚Äôs in the LR image RefSR synthesis ‚Üí combining LR and reference features to output the high-resolution result Warping and synthesis We follow such ‚Äúwarping and synthesis‚Äù pipeline. However, our approach is different from existing works in the following ways:\nOur approach performs multi-scale warping on feature domain at pixel-scale which accelerates the model convergence by allowing flow to be globally updated at higher scales. a novel fusion scheme is proposed for image synthesis. concatenation, linearly combining images 3. Approach 3.1 Fully Conv Cross-scale Alignment Module It is necessary to perform spatial alignment for reference image, since it is captured at different view points from LR image.\nCross-scale warping We propose cross-scale warping to perform non-rigid image transformation.\nOur proposed cross-scale warping operation considers a pixel-wise shift vector ( V ):\n$$ I_o = warp(y_{Ref}, V) $$\nwhich assigns a specific shift vector for each pixel location, so that it avoids the blocky and blurry artifacts.\nüí° Pixel-wise shift vector (V) ‚Üí patch matching\n( V ) represents a flow field, where each pixel gets its own small movement vector (Œîx, Œîy). A flow field is a map (like a vector field) that assigns a motion vector to every pixel in the image. So instead of moving the entire image or patch, CrossNet can move each pixel individually ‚Äî very flexible. The equation:\n$$ I_o = warp(y_{Ref}, V) $$\nmeans:\nThe output image ( $I_o$ ) is generated by warping the reference image ( $y_{Ref}$ ) according to the flow field ( $V$ ).\nEach pixel in ( $y_{Ref}$ ) is shifted by its corresponding vector in ( $V$ ).\nCross-scale flow estimator Purpose: Predict pixel-wise flow fields to align the upsampled LR image with the HR reference.\nModel: Based on FlowNetS, adapted for multi-scale correspondence.\nInputs:\n$I_{LR‚Üë}$ : LR image upsampled by MDSR (SISR) $I_{REF}$ : reference image Outputs: Multi-scale flow fields\n${V^{(3)}, V^{(2)}, V^{(1)}, V^{(0)}}$ (coarse ‚Üí fine).\nModification: √ó4 bilinear upsampling with ‚Üí two √ó2 upsampling modules + skip connections + deconvolution ‚Üí finer, smoother flow prediction.\nAdvantage:\nCaptures both large and small displacements; Enables accurate, non-rigid alignment Reduces warping artifacts. üí° How it works (coarse-to-fine refinement)\nThe coarse flow field (V¬≥) roughly aligns big structures. The next flow (V¬≤) refines alignment for medium details. The fine flows (V¬π, V‚Å∞) correct small local misalignments and textures. These flow fields are combined hierarchically ‚Äî like zooming in step-by-step to improve precision. 3.2 End-to-end Network Structure Network structure of CrossNet\nüí° Network:\na LR image encoder a reference image encoder a decoder ‚Üí U-Net LR image Encoder Goal: Extract multi-scale feature maps from the low-resolution (LR) image for alignment and fusion.\nStructure:\nUses a Single-Image SR (SISR) upsampling to enlarge the LR image first. Then applies 4 convolutional layers (5√ó5 filters, 64 channels). Each layer creates a feature map at a different scale (0 ‚Üí 3). Stride = 1 for the first layer, stride = 2 for deeper ones (downsampling by 2). Output:\nA set of multi-scale LR feature maps. $$ F_{LR}^{(0)}, F_{LR}^{(1)}, F_{LR}^{(2)}, F_{LR}^{(3)} $$\nActivation: ReLU (œÉ).\nReference image encoder Goal: Extract and align multi-scale reference features from the HR reference image.\nStructure:\nUses the same 4-scale encoder design as the LR encoder. Produces feature maps . $$ {F_{REF}^{(0)}, F_{REF}^{(1)}, F_{REF}^{(2)}, F_{REF}^{(3)}}¬†$$\nLR and reference encoders have different weights, allowing complementary feature learning. Alignment:\nEach reference feature map $F_{REF}^{(i)}$ is warped using the cross-scale flow $V^{(i)}$. This generates spatially aligned reference features $$ \\hat{F}{REF}^{(i)} = warp(F_{REF}^{(i)}, V^{(i)}) $$\nDecoder Goal: Fuse the low-resolution (LR) features and warped reference (Ref) features across multiple scales to reconstruct the super-resolved (SR) image. Structure Overview The decoder follows a U-Net‚Äìlike architecture. It performs multi-scale fusion and up-sampling using deconvolution layers. Each scale combines: The LR feature at that scale $F_{LR}^{(i)}$, The warped reference feature $\\hat{F}_{REF}^{(i)}$, The decoder feature from the next coarser scale $F_{D}^{(i+1)}$ (if available). üí° Equations (Eq. 6)\nFor the coarsest scale (i = 3):\n$$ F_{D}^{(3)} = \\sigma \\big( W_{D}^{(3)} \\star (F_{LR}^{(3)}, \\hat{F}{REF}^{(3)}) + b{D}^{(3)} \\big) $$\nFor finer scales (i = 2, 1, 0):\n$$ F_{D}^{(i)} = \\sigma \\big( W_{D}^{(i)} \\star (F_{LR}^{(i+1)}, \\hat{F}{REF}^{(i+1)}, F{D}^{(i+1)}) + b_{D}^{(i)} \\big) $$\nwhere:\n$\\star$ denotes the deconvolution operation (transposed convolution). $W_{D}^{(i)}$: deconvolution filters (size 4√ó4, 64 filters, stride 2). $\\sigma$: activation function (ReLU). $b_{D}^{(i)}$ : bias term. Thus, features are progressively upsampled and refined from coarse ‚Üí fine.\nüí° Post-Fusion (Eq. 7)\nAfter obtaining the final decoder feature map $F_{D}^{(0)}$,\nthree convolutional layers (filter size 5√ó5) are applied to refine and generate the SR image:\n$$ \\begin{aligned} F_1 \u0026amp;= \\sigma(W_1 * F_{D}^{(0)} + b_1), \\ F_2 \u0026amp;= \\sigma(W_2 * F_1 + b_2), \\ I_p \u0026amp;= \\sigma(W_p * F_2 + b_p), \\end{aligned} $$\nwhere:\n$W_1, W_2, W_p$: convolution filters with channel numbers {64, 64, 3}, $I_p$: final super-resolved output image. 3.3 Loss Function Goal: Train CrossNet to generate super-resolved (SR) outputs $I_p$ close to the ground-truth HR images $I_{HR}$.\nFormula:\n$$ L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{s} \\rho(I_{HR}^{(i)}(s) - I_{p}^{(i)}(s)) $$\nPenalty: Uses the Charbonnier loss\n$$ \\rho(x) = \\sqrt{x^2 + 0.001^2} $$\nA smooth, robust version of L1 loss that reduces the effect of outliers.\nVariables:\n$N$: number of training samples $s$: pixel (spatial location) $i$: training sample index 4. Experiment 4.1 Dataset Dataset: The representative Flower dataset and Light Field Video (LFVideo) dataset. Each light field image has: 376 √ó 541 spatial samples 14 √ó 14 angular samples Model training: Each light field image has: 320 √ó 512 spatial samples 8 √ó 8 angular samples Test generalization: Datasets: Stanford Light Field dataset Scene Light Field dataset During testing, they apply the big input images using a sliding window approach: Window size: 512√ó512 Stride: 256 4.2 Evaluation Training setup: Trained for 200K iterations on Flower and LFVideo datasets. Scale factors: √ó4 and √ó8 super-resolution. Learning rate: 1e-4 / 7e-5 ‚Üí decayed to 1e-5 / 7e-6 after 150K iterations. Optimizer: Adam (Œ≤‚ÇÅ = 0.9, Œ≤‚ÇÇ = 0.999). Comparisons: Competes with RefSR methods (SS-Net, PatchMatch) and SISR methods (SRCNN, VDSR, MDSR). Evaluation metrics: PSNR, SSIM, and IFC on √ó4 and √ó8 scales. Reference images from position (0,0); LR images from (1,1) and (7,7). Results: CrossNet achieves 2‚Äì4 dB PSNR gain over previous methods. CrossNet consistently outperforms the resting approaches under different disparities, datasets and scales. Quantitative evaluation of the sota SISR and RefSR algorithms, in terms of PSNR/SSIM/IFC for scale factors √ó4 and √ó8 respectively.\nMetric Meaning PSNR (Peak Signal-to-Noise Ratio) Measures reconstruction accuracy (higher = clearer, less error). SSIM (Structural Similarity Index) Measures structural similarity to the ground truth (higher = more visually similar). IFC (Information Fidelity Criterion) Evaluates how much visual information is preserved (higher = better detail). Generalization During training, apply a parallax augmentation procedure this means they randomly shift the reference image by ‚Äì15 to +15 pixels both horizontally and vertically. The purpose is to simulate different viewpoint disparities (parallax changes) make the model more robust to viewpoint variations. They initialize the model using parameters pre-trained on the LFVideo dataset, Then re-train on the Flower dataset for 200 K iterations to improve generalization. The initial learning rate is 7 √ó 10‚Åª‚Åµ, which decays by factors 0.5, 0.2, 0.1 at 50 K, 100 K, 150 K iterations. Table 2 and 3 show PSNR comparison results: Their re-trained model (CrossNet) outperforms PatchMatch [11] and SS-Net [2] on both Stanford and Scene Light Field datasets. The improvement is roughly +1.79 ‚Äì 2.50 dB (Stanford) and +2.84 dB (Scene LF dataset). Efficiency within 1 seconds machine: 8 Intel Xeon CPU (3.4 GHz) a GeForce GTX 1080 GPU 4.3 Discussion Flows at scale 0‚Äì3 were coherent (good). Flows at scale 4‚Äì5 were too noisy ‚Äî because those very small maps (like 32√ó32) lost too much information. Training setup:\nTrain both CrossNet and CrossNet-iw with the same procedure: Pre-train on LFVideo dataset Fine-tune on Flower dataset 200 K iterations total Additionally, CrossNet-iw pretraining: Pre-train only the flow estimator WS-SRNet using an image warping task for 100 K iterations. Train the whole network jointly for another 100 K iterations. FlowNetS+ adds extra upsampling layers\nIn the original FlowNetS: The final flow map is smaller than the input (maybe ¬º or ¬Ω resolution). This is fine for rough alignment but loses small motion details. In FlowNetS+: They add extra upsampling layers so the final flow map is finer (closer to full size). That‚Äôs why it aligns better ‚Äî it can describe tiny pixel movements more accurately. Downsampling = make smaller. Upsampling = make bigger.\n","permalink":"/notes/crossnet_an_end-to-end_reference-based_super_resol/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003eWhat‚Äôs the \u003cstrong\u003eReference-based Super-resolution (RefSR)\u003c/strong\u003e Network:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSuper-resolves a \u003cstrong\u003elow-resolution (LR)\u003c/strong\u003e image given an external \u003cstrong\u003ehigh-resolution (HR) reference image\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe reference image and LR image share similar viewpoint but with significant resolution gap (8√ó).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"solve-the-problem\"\u003eSolve the problem\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eExisting RefSR methods work in a cascaded way such as \u003cstrong\u003epatch matching\u003c/strong\u003e followed by \u003cstrong\u003esynthesis pipeline\u003c/strong\u003e with two independently defined objective functions\n\u003cul\u003e\n\u003cli\u003eDivide the image into many small \u003cstrong\u003epatches\u003c/strong\u003e (like tiny squares), each patch is compared with a \u003cstrong\u003ereference image\u003c/strong\u003e to find its most similar region.\u003c/li\u003e\n\u003cli\u003eBut every patch makes its decision independently. ‚Üí \u003cstrong\u003eInter-patch misalignment\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eBecause of the small misalignments, the \u003cstrong\u003egrid\u003c/strong\u003e of patch boundaries in the final image shows. ‚Üí \u003cstrong\u003eGrid artifacts\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eOld methods trained \u003cstrong\u003etwo steps separately ‚Üí Inefficient training\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThe challenge large-scale (8√ó) super-resolution problem\n\u003cul\u003e\n\u003cli\u003ethe \u003cstrong\u003espatial resolution is increased by 8 times\u003c/strong\u003e in each dimension (width and height).\u003c/li\u003e\n\u003cli\u003eSo the total number of pixels increases from 8√ó8 to 64√ó64.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003epatch matching ‚Üí warping\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStructure:\u003c/p\u003e","title":"CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping"},{"content":" Chain of thought:\nA series of intermediate natural language reasoning steps that lead to the final output ‚Äî significantly improves the ability of large language models to perform complex reasoning. Chain-of-thought prompting:\nA simple and broadly applicable method for enhancing reasoning in language models. Improving performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Two simple methods to unlock the reasoning ability of LLMs Thinking in steps helps:\nWhen the model explains each step before the answer, it understands the problem better.\nLearning from examples:\nWhen we show a few examples with step-by-step answers, the model learns to do the same.\nFew-shot prompting means giving the model a few examples in the prompt to show it how to do a task before asking it to solve a new one.\nWhat this paper do Combine these two ideas help the language models think step by step to generate a clear and logical chain of ideas that shows how they reach the final answer. Given a prompt that consists of triples: \u0026lt;input, chain of thought, output\u0026gt; Why this method is important It doesn‚Äôt need a big training dataset. One model can do many different tasks without extra training. Greedy decoding: let the model choose the most likely next word each time\nResult It only works well for giant models, not smaller ones. It only works well for more-complicated problems, not the simple ones. Chain-of-thought prompting with big models gives results as good as or better than older methods that needed finetune for each task. Ablation study: It‚Äôs like testing which parts of your model really matter.\nRelated work Some methods make the input part of the prompt better ‚Äî for example, adding clearer instructions before the question. But this paper does something different (orthogonal): it improves the output part, by making the model generate reasoning steps (chain of thought) before giving the final answer. ","permalink":"/notes/chain-of-thought-prompting/","summary":"\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eChain of thought:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA series of intermediate \u003cstrong\u003enatural language reasoning steps\u003c/strong\u003e that lead to the final output ‚Äî significantly improves the ability of large language models to perform complex reasoning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eChain-of-thought prompting:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple and broadly applicable method for\n\u003cul\u003e\n\u003cli\u003eenhancing reasoning in language models.\u003c/li\u003e\n\u003cli\u003eImproving performance on a range of \u003cstrong\u003earithmetic\u003c/strong\u003e, \u003cstrong\u003ecommonsense\u003c/strong\u003e, and \u003cstrong\u003esymbolic\u003c/strong\u003e\nreasoning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003chr\u003e\n\u003ch2 id=\"two-simple-methods-to-unlock-the-reasoning-ability-of-llms\"\u003eTwo simple methods to unlock the reasoning ability of LLMs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eThinking in steps helps:\u003c/strong\u003e\u003c/p\u003e","title":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"},{"content":"Initial Impression üëâüèº The light field refres to the representation of a scene. Unlike the tranditional 2D image, it adds an extra dimention: the direction of light. This means each image captures not only the length and width, but also the direction of light.\nTraditional image: 2D spatially (width, height), but each pixel carries a 3-value vector (RGB, 3 channels for color). Light filed(width, heidht, X-direction, Y-direction): 4D images, but each pixel carries a 3-value vector. As a result, light field data is massive and challenging to process.\nTo address it, researchers have turned into machine learning to efficiently handle and enhance light field imaging.\nThe paper mentions 4 main areas that AI can helps:\nDepth estimation Reconstruction Compression Quelity evaluation Abstract üëâüèº Content:\nBackground Motivation Research focus Paper content the existing learning-based solutions frameworks evaluation methods datasets future research directions Background Traditional cameras capture only 2D images. Light field imaging records the direction of light rays, enabling realistic and immersive 3D-like representations. Motivation Light field data provides richer visual information but comes with large data volume and high computational cost. Machine learning and deep networks offer efficient and intelligent ways to process this data. Research Focus Learning-based methods are applied to:\nDepth estimation Reconstruction and super-resolution Compression and quality enhancement Paper Goals The paper surveys existing learning-based techniques, summarizes the most promising frameworks, reviews current datasets, and evaluation methods and outlook for future research directions.\n1. Introduction üëâüèº Content:\nConcept and potential of light field imaging Market growth and application fields Technical challenges and processing tasks Shift toward learning-based solutions Purpose and structure of the paper Concept and Potential Light field imaging is a promising 3D imaging technology that records light rays traveling through every point in space and in every direction. Unlike 2D photography, it captures angular information, providing a sense of depth, realism, and immersion. This enables photo-realistic rendering and supports 6-DoF (Degrees of Freedom) experiences for next-generation immersive media, broadcasting, and gaming. Market and Applications The light field market is expanding rapidly, driven by glasses-free 3D displays and multi-view visualization systems. It supports a range of applications such as virtual reality, augmented reality, 3D reconstruction, and computational photography. Challenges and Processing Tasks High-dimensional light field data introduces issues like data redundancy, storage complexity, and inter-view correlation. Essential processing tasks include: Spatial and angular super-resolution to enhance image quality Compression algorithms for efficient data storage and transmission Depth estimation for 3D scene reconstruction These tasks are more complex than traditional 2D image processing due to added angular dimensions. Shift Toward Learning-Based Solutions Traditional geometry-based methods struggle with large datasets and occlusion problems. Deep learning and data-driven frameworks now dominate, improving efficiency and performance in reconstruction, compression, and depth estimation. Learning frameworks enable automation and scalability, addressing the computational challenges of high-dimensional data. Purpose and Structure of the Paper This review provides a comprehensive overview of learning-based solutions for light field imaging. It discusses: Fundamentals of light field imaging and data acquisition Key processing tasks and related learning frameworks Benchmark datasets and evaluation methods Current challenges and future research directions The goal is to summarize progress, identify open issues, and provide a roadmap for future research in learning-based light field processing. 2. Light field imaging background üëâüèº Content:\nLight field fundamentals Light field acquisition Light field visualization 2.1 Light Field Fundamentals üëâüèº Content:\nConcept of the Plenoptic Function Dimensional Reduction Light Field Representation Concept of the Plenoptic Function To reproduce realistic 3D scenes, cameras must capture light from many viewpoints.\nA light field describes all light rays in 3D space ‚Äî their positions, directions, colors, and intensity.\nThe complete description is given by the plenoptic function, a 7-dimensional (7D) function:\n$$ P(Œ∏, œÜ, Œª, œÑ, V_x, V_y, V_z) $$\nwhich includes direction, wavelength, time, and position of every ray in space.\nDimensional Reduction Capturing the full 7D plenoptic function is practically impossible. Under constant lighting and static scenes, wavelength (Œª) and time (œÑ) can be ignored. This simplifies the representation to a 5D function: $$ P(Œ∏, œÜ, V_x, V_y, V_z) $$\nThe 5D form forms the foundation for Neural Radiance Fields (NeRF), while further simplification leads to a 4D light field representation. Light Field Representation Two-Plane Parameterization\nThe 4D light field assumes light rays travel in straight lines. Each ray is defined by its intersection with two parallel planes: (u, v) ‚Üí spatial coordinates on the image/focal plane (Œ©) (s, t) ‚Üí coordinates on the camera plane (Œ†) The resulting function: $$ P(u, v, s, t) $$\ndefines the light field in terms of spatial and angular information. This two-plane parameterization makes light fields easier to store, process, and compress using 2D image arrays. A 4D light field can be expressed as an array of 2D images indexed by (u, v) for spatial coordinates and (s, t) for different viewpoints. This enables the use of standard 2D codecs for compression and native 2D algorithms for processing. Epipolar Plane Image (EPI)\nEpipolar Plane Image (EPI), a 2D slice of the light field showing spatial‚Äìangular relationships. Looks like a single 2D picture made by stacking the same pixel row from all those camera views. Coordinates ‚Üí (u, s) or (v, t) (only one spatial + one view direction) In the EPI, each object in the scene becomes a slanted line: If an object is close, its line is steeper (because it moves more between views). If it‚Äôs far, its line is flatter (moves less). Lines in the EPI correspond to scene depth ‚Äî analyzing their slopes allows depth estimation and 3D reconstruction. Epipolar Plane Image (EPI)\n2.2 Light Field Acquisition üëâüèº Acquisition methods:\nSingle-Camera Systems Multi-Camera Arrays Other Methods Computer-generated light fields Handheld or SLAM-based systems LiDAR-assisted capture Overview Light fields can be acquired by single plenoptic cameras, camera arrays, or synthetic and LiDAR-based systems, each balancing data density, resolution, and complexity.\nSingle-Camera Systems Plenoptic (lenslet-based) cameras use microlens arrays to capture dense angular information in one shot. Examples: Raytrix and Lytro cameras. Lytro ‚Üí The Lytro camera captures a 4-D light field by recording many tiny viewpoint images through its microlens array ‚Äî those images form the SAI stack. Multi-Camera Arrays Arrays of monocular cameras (planar or spherical) capture scenes from different viewpoints. The camera layout defines the angular sampling and overall field of view. Other Methods Computer-generated light fields provide accurate depth maps for benchmarking. Handheld or SLAM-based systems reconstruct light fields from multiple frames. LiDAR-assisted capture combines sensors and cameras for precise, automated 3D data. 2.3 Light field visualization üëâüèº Content:\nGoal and use cases Visualization approaches Display principles and challenges Perceptual limitations Goal and Use Cases The main goal is to provide a true 3D visual experience, essential for immersive and interactive applications. Visualization may be passive (no interaction) or active (viewer can move or rotate objects). Used in areas like medical imaging, VR/AR, and 3D display systems. Visualization Approaches Based on the 4D light-field representation combining spatial and angular data. Passive use cases render fixed views; active use cases synthesize new views via view interpolation. (View interpolation means creating new viewpoints images that were never actually captured by a real camera, but are mathematically generated from nearby real ones. Rendering quality strongly influences perceptual realism. Display Principles and Challenges Two key properties: Angular resolution (baseline between views) the baseline = the distance between two camera viewpoints. Small baseline ‚Üí cameras are close together Wide baseline ‚Üí cameras are far apart Spatial resolution (image detail) 3D displays must reproduce directional light rays accurately to recreate depth and realism. Capturing dense samples and rendering multiple view angles remain computationally demanding. Perceptual Limitations Standard 2D displays lack realistic cues like vergence and accommodation, limiting immersion. Light-field displays address this by replicating real light rays to the viewer‚Äôs eyes but face: Finite ray sampling Vergence‚Äìaccommodation conflict Restricted field of view (FoV) Horizontal- or vertical-parallax-only designs causing viewing discomfort. 3. Learning‚Äëbased light field processing üëâüèº Content:\nDepth estimation Autoencoders Stereo matching and refinement End‚Äëto‚Äëend feature extraction and disparity regression Light field reconstruction Spatial super‚Äëresolution Single‚Äëview super‚Äëresolution and refinement End‚Äëto‚Äëend residual learning Angular super‚Äëresolution EPI super‚Äëresolution Depth estimation and warping Multi‚Äëplane image generation Spatio‚Äëangular reconstruction Neural scene representation Compression Learning‚Äëbased view synthesis on the decoder side Learning‚Äëbased view synthesis on the encoder and decoder sides End‚Äëto‚Äëend light field compression architecture Other light field imaging applications This section summarizes the most prominent light field processing tasks studied in the literature and highlights the learning-based imaging techniques deployed for each processing task.\nTerms and explanation:\nTerm Simple meaning Example / Analogy EPI volume Stack of many light-field slices showing how points move across views Like stacking many thin image strips to form a 3D block Stereo view Two or more photos of the same scene from different angles Like your left and right eye views SAI stack Group of small 2D images from a light field camera Like a grid of mini photos from slightly different directions Depth map Image showing distance of each pixel (white = near, black = far) Like a 3D scanner‚Äôs output Disparity volume 3D data showing pixel shifts between views Large shift ‚Üí close object, small shift ‚Üí far object Details about the difference between Light Field representation:\nTerm Relation to Two-Plane Parameterization Simple meaning EPI A slice through the Two-Plane representation. Shows pixel movement (as slanted lines) between multiple views ‚Äî used for depth estimation. Stereo view it‚Äôs a simpler / smaller subset of the Two-Plane model (only 2 views). Like taking only the left and right images from the light field. SAI stack it‚Äôs directly sampled from the Two-Plane model (a grid of (s, t) views, each with its own (u, v) image). Many small 2D images from slightly different viewpoints ‚Äî a practical way to store the light field. Relationship between Disparity volume and Depth map:\nConcept Type What it represents Used for Disparity volume 3D data (x, y, disparity) All possible matching shifts between views Intermediate step (to find the best match) Depth map 2D image (x, y), 3D data (x, y, depth) Actual distance of each pixel Final result Disparity and depth are inversely related:\n$$ \\text{Depth} = \\frac{f \\times B}{\\text{Disparity}} $$\nwhere:\n(f) = focal length of the camera, (B) = distance between two camera views (baseline). So:\nLarge disparity (big shift) ‚Üí close object. Small disparity (tiny shift) ‚Üí far object.\nDepth map\n3.1 Depth estimation üëâüèº Three main approaches:\nAutoencoders Stereo matching and refinement End-to-end feature extraction and disparity regression Depth estimation model architecture\nOverview Goal: Estimate the distance of each pixel from the camera to recover the scene‚Äôs 3D structure. Light field imaging enables capturing a scene from multiple viewpoints so depth information is implicitly encoded in the light field representation and can be acquired by computing the inter-view pixel disparity information. disparity volume ‚Üí depth map Main challenges: occlusions, non-Lambertian surfaces, and texture-less regions, which make accurate estimation difficult. A Lambertian surface is an ideal matte surface ‚Äî it reflects light equally in all directions. That means no matter where you look from, the brightness of that point stays the same. Recent progress focuses on learning-based approaches, achieving higher accuracy than traditional geometry-based methods. Autoencoders Take an EPI volume ‚Üí compress it (encoder) ‚Üí learn the hidden features ‚Üí expand it (decoder).\nClassical method:\nHeber \u0026amp; Pock: five-block CNN estimating line orientation in EPIs; Orientation refers to the direction each camera is facing. later extended to a U-shaped encoder‚Äìdecoder, Further to U-shaped encoder‚Äìdecoder with skip connections and 3D filters. Step Explanation Input Horizontal and vertical EPI volumes (from the light field). Each EPI shows slanted lines ‚Äî the slope encodes depth. Model A 5-block CNN that scans small ‚Äúwindows‚Äù (patches) of the EPI. Each CNN layer extracts features and estimates the orientation (slope) of the EPI lines. Later versions evolved into a U-shaped encoder‚Äìdecoder (U-Net) for better reconstruction. Output A depth map, where every pixel‚Äôs value = estimated distance from the camera. Alperovich et al.:\nan autoencoder that encodes horizontal and vertical EPI stacks simultaneously using six stages of residual blocks to improve robustness. Then, the compressed representation is expanded using three decoder pathways to address the disparity, diffusion, and specularity estimation problems. Step Explanation Input Combined horizontal + vertical EPI stacks from the light field. Model An autoencoder with: 6 residual blocks (for stronger feature extraction) in the encoder, and 3 decoder branches (to handle disparity, diffusion, specularity). It compresses the light field into a latent code, then reconstructs richer outputs. Output A disparity volume ‚Äî a 3D array where each pixel position stores multiple disparity hypotheses (possible shifts). From this volume, a final depth map can be derived later. Analysis:\nPros: captures compact depth features, handles EPI geometry directly. Cons: computationally heavy; limited to 2D EPI slices, less effective in occluded regions. Stereo Matching and Refinement Computes disparity between SAIs using neural stereo-matching networks.\nTypical pipeline:\nCoarse disparity estimation via networks like FlowNet 2.0 or encoder‚Äìdecoder CNNs. Refinement using residual or occlusion-aware learning to correct depth errors. Examples:\nRogge et al. (belief propagation + residual refinement);\nGuo et al. (encoder‚Äìdecoder concatenation of SAIs).\nAnalysis:\nPros: exploits full 4D light-field correlations, good for complex geometry. Cons: high computation cost; sensitive to reflections and non-Lambertian surfaces. End-to-End Feature Extraction and Disparity Regression Fully end-to-end CNNs learn features and regress depth directly.\nMethods:\nEpinet: Horizontal, vertical, and diagonal SAI stacks ‚Üí Multi-stream CNN feature extraction ‚Üí Regression network ‚Üí Depth map Leistner et al.: Vertical \u0026amp; horizontal SAI stacks ‚Üí Siamese U-Net ‚Üí Autoencoder regression module ‚Üí Classification + regression fusion ‚Üí Depth map Two-stream CNN: Horizontal \u0026amp; vertical EPIs ‚Üí Multi-scale feature extraction (four convolutional stages) ‚Üí Feature concatenation ‚Üí Multi-label regression ‚Üí Depth map Zhu et al.: Focal stacks + center view + EPIs ‚Üí Hybrid feature extraction ‚Üí Fully connected + softmax layers ‚Üí Pixel-wise disparity classification ‚Üí Depth map Tsai et al.: Multi-view SAIs ‚Üí Residual blocks + spatial pyramid pooling ‚Üí Cost volume construction + attention module ‚Üí Disparity regression ‚Üí Depth map Multi-scale Cost-Volume Method: Shifted SAI feature maps (multi-disparity levels) ‚Üí 4D cost volume (low memory footprint) ‚Üí Multi-scale feature extraction ‚Üí Regression ‚Üí Depth map Analysis:\nPros: highest accuracy, unified optimization of feature + disparity learning. Cons: large data/training demand; reduced performance on wide-baseline scenes. üëâüèº Summary:\nDepth estimation for the wide baseline scenario, with an acceptable trade-off between accuracy and computation, is still an open research problem.\n3.2 Light field reconstruction üëâüèº Content:\nSpatial super‚Äëresolution Single‚Äëview super‚Äëresolution and refinement End‚Äëto‚Äëend residual learning Angular super‚Äëresolution EPI super‚Äëresolution Depth estimation and warping Multi‚Äëplane image generation Spatio‚Äëangular reconstruction Neural scene representation To enable higher spatial and angular resolutions, the development of light field reconstruction/ super-resolution (SR) methods has gained significant attention.\nSpatial SR = make each image (each view) sharper ‚Äî more pixels, more detail. Angular SR = make more viewpoints ‚Äî fill in missing views between existing ones. üí° Spatial super-resolution (SR) focuses on improving the static spatial resolution ‚Äî that is, the sharpness, clarity, and detail of each individual sub-aperture image (still view).\nAngular consistency ensures smooth and coherent transitions between different viewpoints, maintaining stable motion perception and correct 3D geometry when the view changes or the scene is refocused.\nLight-field SR must improve both:\nSpatial resolution ‚Üí each view looks higher spatial resolution. Angular consistency ‚Üí all views agree about geometry and depth. System Spatial resolution (image sharpness) Angular resolution (number of viewpoints) Baseline Notes Plenoptic camera Low High (dense) Narrow Compact but blurry Camera rig High Low (sparse) Wide Sharp but heavy and complex Spatial super‚Äëresolution Examples of architectures for the spatial, angular, and spatio-angular super-resolution (SR) frameworks.\na ‚Üí Single-view SR using single image super-resolution (SISR) network and inter-view enhancement, b ‚Üí end-to-end residual learning, c ‚Üí warping and residual learning for refinement, d ‚Üí multi-plane image generation, e ‚Üí residual learning using 4D CNNs and refinement, **** f ‚Üí GAN-based method Single‚Äëview super‚Äëresolution and refinement\nStep Explanation Input (SAI stack LR) SAI stack LR: all the sub-aperture images captured by the light-field camera. low-resolution ‚Äî each view is blurry and lacks details (because each microlens only captures a small number of pixels). For example, 9√ó9¬†SAIs¬†of¬†60√ó60¬†pixels. SISR Single-Image Super-Resolution: This block applies a 2D super-resolution network (e.g., VDSR) to each SAI individually to ****improve the spatial resolution. HR(init)LF High-Resolution (initial) Light Field: Each image looks sharper, but the views may no longer be perfectly aligned (angular inconsistency). EPI EPIs (Epipolar Plane Images) are extracted. EPIs capture the geometric relationships between views ‚Äî how pixels shift across viewpoints (slanted lines). Refinement module This is a learning network (often a CNN) that analyzes the EPIs to find and correct misalignments between neighboring SAIs. Making sure all views agree in depth and structure. Residual addition Combine sharpness from the first stage and consistency from the second stage. Output (HR LF) High-Resolution Light Field: The final output light field has: High spatial resolution (sharp individual views), and High angular consistency (smooth geometry across views). For example, 9√ó9¬†SAIs¬†of¬†240√ó240¬†pixels. End‚Äëto‚Äëend residual learning\nStep Explanation Input (SAI stack) SAI stacks (Horizontal, Vertical, Diagonal, Center View), Center view ‚Üí the middle image, used as a geometric reference Feature Extraction Each SAI stack is passed through a feature extraction CNN. Extract features from each SAI direction, like ‚Äúwhat the object looks like‚Äù and ‚Äúhow it moves between views.‚Äù Feature Integration \u0026amp; Processing The feature maps from all directions (horizontal, vertical, diagonal, etc.) are merged or fused here. Upsampling Network Increases the spatial resolution (i.e., number of pixels). Output (HR LF) The final output is a high-resolution light field ‚Äî a grid of SAIs that are: Spatially sharper and Angularly consistent. Upsampling means making an image larger ‚Äî that is, increasing its resolution by creating more pixels.\nAngular super‚Äëresolution Angular Super-Resolution (SR) means synthesizing new in-between views to make the light field smoother and more complete. (view synthesis)\nEPI super‚Äëresolution\nAll of these models have the same basic structure:\nStart from a blurry / low-angular-resolution light field (few viewpoints). Use deep networks to predict high-frequency details (the fine geometry and textures missing in the low version). Reconstruct the high-angular-resolution (HR) light field, which includes the new views. Step Explanation Input (SAI stack LR) SAI stack LR: all the sub-aperture images captured by the light-field camera. low-resolution ‚Äî each view is blurry and lacks details (because each microlens only captures a small number of pixels). For example, 9√ó9¬†SAIs¬†of¬†60√ó60¬†pixels. SISR Single-Image Super-Resolution: This block applies a 2D super-resolution network (e.g., VDSR) to each SAI individually to ****improve the spatial resolution. HR(init)LF High-Resolution (initial) Light Field: Each image looks sharper, but the views may no longer be perfectly aligned (angular inconsistency). EPI EPIs (Epipolar Plane Images) are extracted. EPIs capture the geometric relationships between views ‚Äî how pixels shift across viewpoints (slanted lines). Refinement module This is a learning network (often a CNN) that analyzes the EPIs to find and correct misalignments between neighboring SAIs. Making sure all views agree in depth and structure. Residual addition Combine sharpness from the first stage and consistency from the second stage. Output (HR LF) High-Resolution Light Field: The final output light field has: High spatial resolution (sharp individual views), and High angular consistency (smooth geometry across views). For example, 9√ó9¬†SAIs¬†of¬†240√ó240¬†pixels. Depth estimation and warping\nDisparity is the shift of the same object‚Äôs position between two different views. Warping = using disparity to reposition pixels. Step Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Depth Estimator Predicts a depth map (distance information) for each SAI or for the entire light field. It learns how far each pixel is from the camera by analyzing geometric cues across views. (wraps_by_an_amount_that_depends_on_its_depth) Warping Module Uses the estimated depth maps to warp (geometrically align) all SAIs toward a common viewpoint ‚Äî usually the center view. ‚ÄúWarping‚Äù means shifting pixels according to their depth so that corresponding points from different views line up. Refinement Module Fine-tunes the warped images using a CNN. Corrects small errors from imperfect depth estimation or warping. Output (HR LF) The final reconstructed light field: All SAIs are now high-resolution (sharp textures). The views are geometrically aligned (consistent depth perception). Some basics:\nConcept Meaning Why it matters Depth map Tells how far each pixel is Needed to compute correct pixel shift Disparity The actual shift caused by viewpoint change Derived from depth Warping Moves pixels according to disparity Aligns all views Without depth All pixels shift equally ‚Üí wrong alignment Causes blur and ghosting Multi‚Äëplane image generation (MPI generation)\nStep Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Warping Module The Warping Module aligns all SAIs to a reference view (usually the center view) using a range of hypothetical depth planes. Plane-sweep volume After warping, stacks all these reprojected images together ‚Äî forming a plane-sweep volume. Create a 3D data structure that encodes how well each depth hypothesis aligns across views. No explicit depth estimator ‚Äî depth is implicitly encoded in the plane-sweep volume. 3D CNN The 3D convolutional neural network processes the plane-sweep volume to analyze spatial and depth correlations. Multi-plane image A multi-plane image (MPI) ‚Äî a set of 2D images, each representing a scene layer at a specific depth, with color + transparency (Œ±) values. Blending Module Combines (blends) all the multi-plane images (depth layers) into a single coherent high-resolution light field. Output (HR LF) The final light field: All SAIs are now high-resolution and geometrically aligned. Spatio‚Äëangular reconstruction Residual learning using 4D CNNs and refinement\nStep Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Warping Module The Warping Module aligns all SAIs to a reference view (usually the center view) using a range of hypothetical depth planes. 4D CNN A convolutional network that operates directly on the 4D light-field volume. It jointly learns: Spatial features (edges, textures inside each SAI) and Angular features (parallax). The 4D CNN predicts the missing high-frequency details for all views simultaneously. Residual Connection This ‚Äúresidual learning‚Äù means the 4D CNN only learns the difference (the missing fine details) rather than reconstructing the entire image from scratch. Refinement Module Polish the HR LF and ensure angular consistency. Output (HR LF) Output: HR LFThe final output = high-resolution light field ‚Üí same number of views as the input, ‚Üí each view sharper (higher spatial resolution) and smoothly aligned (angular consistency). GAN-based method\nStep Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Generator The generator learns to upsample the SAIs and restore missing details (edges, textures, angular consistency). GAN Discriminator It is trained on real HR light fields (ground truth) and fake HR light fields (from the generator). Its job is to classify them as real or fake. Force the generator to create results that are indistinguishable from real data ‚Äî not just pixel-wise accurate but also visually realistic (better textures, depth edges, lighting). Output (HR LF) The generator alone can take any new LR light field and output an HR version that: has high spatial detail, maintains angular consistency, and looks visually realistic (not over-smoothed). üí° Summary: Limitations of LF Reconstruction Techniques: Early reconstruction methods Slow to run. Trained for fixed view sampling patterns ‚Üí hard to generalize to new setups. Single-view SR methods (Fig. 3a) Each sub-aperture image (SAI) is processed separately. Causes geometric inconsistency between views because inter-view information isn‚Äôt used. Depth-based \u0026amp; warping methods (Fig. 3c) Work better for wide-baseline cases (larger view spacing). Depend heavily on accurate depth maps ‚Üí errors lead to tearing, ghosting, and problems with non-Lambertian (reflective) surfaces. MPI-based methods (Fig. 3d) Memory-hungry and slow to train (often need multiple GPUs and days of training). Model size grows with number of depth planes (larger depth budget ‚Üí bigger model). Can assign wrong opacity to layers ‚Üí causes blurry reconstructions. 4D CNN methods (Fig. 3e) Produce high-quality results, but have high computational cost due to expensive 4D convolutions. GAN-based methods (Fig. 3f) Need large training datasets. Training can suffer from instability and mode collapse (generator producing limited or repetitive outputs). Neural scene representation Researchers started using neural networks to represent 3D scenes which replaces 2D pictures.\nFrom image-based to neural 3D representations\nEarlier works used image-based rendering (combine nearby views). Recent advances use neural networks to represent and render 3D scenes directly. 3D representations can be: Explicit: meshes, voxels, point clouds. Implicit: continuous functions learned by networks + differentiable ray marching. NeRF ‚Äî the core idea\nNeural Radiance Fields (NeRF) represent a scene using an MLP (multi-layer perceptron). The network takes 5D coordinates ‚Üí (x, y, z, Œ∏, œÜ): spatial position + viewing direction. It outputs: Density (geometry) Color (view-dependent radiance). Rendering is done by volume rendering along camera rays. üí° How NeRF learns the scene:\nFeed the network 2D photos of the same scene taken from different camera angles, and you must know exactly where each camera was (its pose = position + orientation). ‚Üí So NeRF knows which pixel in which image corresponds to which ray in 3D space. NeRF renders its current guess (density and color) of those images using its internal 3D representation. It compares them to the real photos (pixel by pixel). It updates its weights using backpropagation to minimize the difference. To render one image (for a camera view):\nFor each pixel, shoot a ray through the 3D scene.\nSample many points along that ray (like tiny steps through space).\nAt each point, ask the network for its color and density.\nCombine (accumulate) all samples along the ray using a differentiable volume rendering formula:\n$$ C = \\sum_i T_i (1 - e^{-\\sigma_i \\Delta_i}) c_i $$\nwhere:\nC = final pixel color, $\\sigma_i$ = density, $c_i$ = color, $T_i$ = how much light passes through before reaching this point. This process is known as differentiable ray marching.\nSo the process works like this:\nCollect multiple 2D photos of the same scene, taken from different camera angles (with known position and orientation: x, y, z, Œ∏, œÜ). For each pixel in each photo, sample many 3D points along the corresponding camera ray. For each sampled point, use the network to predict its color and density (or depth). Aggregate the information from all sampled points to estimate the final color of the pixel. Compare the predicted image to the real photo, pixel by pixel, and update the model based on the error. NeRF uses a coarse network (for rough geometry) and a fine network (for detailed structure). Analysis: Pros: realistic view synthesis. Cons: very slow training and rendering. Methods to improve speed and efficiency\nSeveral approaches speed up NeRF by changing how the scene is represented, they replaced the big neural network with simpler structures:\nMethod Representation Key idea Pros / Cons Voxel-based NeRFs (Fridovich et al.) Sparse voxel grid Store opacity + spherical harmonic coefficients Faster, but memory-heavy TensoRF 4D tensors (low-rank decomposition) Factorize radiance field into compact tensor components Efficient, compact SNeRG Sparse 3D voxel grid with color \u0026amp; features Encodes view-dependent effects Fast rendering; still large memory Octree NeRF (Yu et al.) Octree structure (adaptive voxels) Sample dense regions more finely Faster, but higher memory cost NeX Extended MPI (Multi-Plane Image) Model color as function of viewing angle using spherical bases Better view-dependent rendering Ray-space embedding 4D ray embedding ‚Üí latent space Compact feature embedding Memory efficient but slower rendering KiloNeRF Thousands of tiny MLPs Divide scene into grid cells, each with a small network Great speed, coarse structure 3D Gaussian representation Continuous Gaussians Skip empty-space computation Near real-time rendering Instant-NGP (M√ºller et al.) Hash-based encoding Store features in hash table for fast lookup Very fast, compact, GPU-friendly Methods to improve quality\nVariant Key idea Benefit Mip-NeRF Multi-scale cones for anti-aliased rendering Handles different resolutions \u0026amp; reduces aliasing NeRF++ Two networks: near-field \u0026amp; far-field (spherical) Better for unbounded, complex scenes Mip-NeRF 360 Extends Mip-NeRF for unbounded scenes; uses nonlinear parameterization \u0026amp; regularization High-quality large-scene rendering Trade-offs and current challenges:\nFast methods (gaussian) ‚Üí train quickly, but lose visual quality. High-quality methods (NeRF, Mip-NeRF 360) ‚Üí photorealistic but slow to train. Explicit methods also can‚Äôt be optimized directly with gradients ‚Üí need to convert trained implicit NeRFs into their format (adds complexity). Real-time + high-quality rendering remains an open research challenge. 3.3 Compression üí° Three main strategies:\nLearning-based view synthesis on the decoder side; Learning-based view synthesis on the encoder and decoder sides; End-to-end light field compression architecture; An efficient codec should be able to explore:\nnot only the spatial and angular redundancies independently (as two-dimensional data), but also the combined spatial‚Äìangular redundancy (4D data). The key idea of this compression architecture is bitrate saving by sparsely encoding the views.\nLearning-based view synthesis on the decoder side Step Explanation Input light fields These are the original dense light-field data, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints. Key SAIs selection Instead of sending all the views, we select a few key SAIs. These key SAIs contain enough angular and spatial information to reconstruct the missing ones later. Encoder The encoder compresses these key SAIs into a bitstream (binary data) for transmission or storage. Bitstream This is the compressed data that‚Äôs transmitted or saved. It contains only the encoded information of the key SAIs (no non-key views). Decoder The decoder reconstructs the key SAIs from the bitstream. Learning-based view synthesis This is a deep neural network trained to synthesize new views (non-key SAIs) from the nearby key SAIs. Generate all non-key views ‚Üí fill in the gaps to recreate the full light field. Decoded light fields Produce and output the complete high-quality light field (same size as the original) after combining Decoded key SAIs and Generated non-key SAIs. Learning-based view synthesis on the encoder and decoder sides Idea:\nEncode only a few key views (the main ones), Use deep learning to predict or reconstruct the missing views (non-key views), Send only the residual errors to refine those predictions. Step Explanation Input light fields These are the original dense light-field data, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints. Key SAIs and Non-key SAIs The system splits the light field into: Key SAIs: a few representative images selected for transmission (e.g., every 3rd or 4th view). Non-key SAIs: the remaining views that will be predicted rather than transmitted. Encoder Takes two inputs: Key SAIs and Residuals (for the non-key SAIs), compresses both into a compact bitstream (binary data) for transmission or storage. Learning-based view synthesis (encoder side) This neural network predicts the non-key SAIs from the available key SAIs. so the encoder can calculate the prediction error (residual) ‚Üí the difference between the predicted and real non-key SAIs is computed as a residual. Residue The encoder subtracts the predicted non-key SAI (from the network) from the actual non-key SAI. only encode the difference, not the entire image ‚Äî saves lots of bitrate. Bitstream The data sent or stored ‚Äî contains compressed key SAIs + residuals. Decoder Receives and decompresses the bitstream. Reconstructs the key SAIs first. Then passes them to the learning-based view synthesis network to generate predicted non-key SAIs. Learning-based view synthesis (decoder side) Same (or similar) neural network as on the encoder side. It uses the decoded key SAIs to synthesize (predict) the non-key SAIs. Then it adds the residual (decoded correction data) to refine those synthesized views. Addition (+) and output The synthesized non-key SAIs are added to the decoded residuals ‚Üí final accurate non-key views. Combine key + non-key SAIs ‚Üí get the fully decoded light field. üí° Su et al. ‚Üí\nInstead of treating every pixel separately, they group light rays that belong to the same 3D point in the scene. After grouping rays, some nearby super-rays may still be very similar. So they merge them into larger super-rays to save even more space. Compress each super-ray using a 4D Discrete Cosine Transform (DCT). a light field varies in both: Spatial dimensions (x, y) ‚Äî inside each image, Angular dimensions (u, v) ‚Äî across different viewpoints. Better captures 4D spatial‚Äìangular redundancy but is more complex. End-to-end light field compression architecture Step Explanation Input light fields These are the original dense light-field data, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints. Encoder This is the main compression network.It takes the input light field and learns to represent it using fewer numbers (features). Bitstream The bitstream is the final compressed data produced by the encoder. It contains the quantized latent features. Decoder The decoder takes the bitstream and reconstructs (decodes) the light field. It performs the reverse of the encoder: expands the compact features back into full-resolution sub-aperture images. Decoded Light Fields These are the reconstructed sub-aperture images (SAIs).Ideally, they look almost identical to the input views, with small errors due to compression. End-to-end schemes are gaining more attention due to their effectiveness in image compression.\nüí° View synthesis drawbacks can be circumvented by neural representations that achieve a level of detail that is challenging for traditional methods.\n3.4 Other light field imaging applications Light-field (LF) images are more powerful than normal 2D photos because they capture depth, focus, and parallax ‚Äî this allows better performance in many computer vision tasks.\nDeep learning methods are now being used to apply light-field data to various new areas.\nSaliency Detection (SOD) Goal: detect which objects or regions attract human attention. LF advantage: provides both spatial and angular information, giving richer clues about object boundaries and depth. Typical model: encoder‚Äìdecoder two-stream networks: One stream uses all-in-focus (center) images, The other stream uses focal stacks or multi-view features. A comprehensive review compares deep LF-SOD models with standard RGB-D models. Face Recognition Goal: identify faces more accurately using multi-view data from light fields. LF advantage: combines intra-view (within one image) and inter-view (across multiple angles) features. Methods: VGG features with LSTM layers to model view changes. Capsule networks with a pose matrix to handle viewpoint shifts. Datasets introduced: LFFW (Light Field Faces in the Wild), LFFC (Light Field Face Constrained) ‚Äî for benchmarking LF face recognition. Light Field Microscopy Goal: use LF imaging to capture and reconstruct 3D biological structures quickly. LF advantage: captures 3D spatial information in one camera shot ‚Üí instant 3D imaging. Deep learning usage: improves speed and quality of reconstructions. Methods: Encoder‚Äìdecoder networks convert 2D LF inputs to 3D volume data. Networks with 2D and 3D residual blocks enhance reconstruction quality. Convolutional sparse coding (CSC) networks use EPIs as input for fast neuron localization. Applications: real-time visualization of cardiovascular or neuronal activity. The network uses EPIs as inputs and generates sparse codes, representing depth data, as outputs. Other applications Image classification ‚Äî improves feature learning using angular cues. Low-light imaging ‚Äî LF data helps reconstruct clear images in dark conditions. Overall, these works show that learning-based light-field imaging provides richer 3D understanding and better accuracy than normal 2D or RGB-D methods.\n4. Datasets and quality assessment Datasets Characteristics of the light field datasets used to benchmark the light field imaging systems\nQuality Assessment Light field imaging algorithms are typically evaluated using quantitative methods by comparing generated data to a ground-truth. Due to the diversity of light field acquisition procedures, distortions, and rendering processes, light field quality assessment remains a challenging task. A recent focus has been on developing more accurate objective algorithms that extract features from both spatial and angular domains for light field quality assessment. Metrics can be classified into three categories based on the availability of the reference image: full-reference (FR), reduced-reference (RR), no-reference (NR). Developing NR metrics are gaining more attention due to their success in improving accuracy. Current learning-based light field algorithms are still only evaluated using conventional PSNR and SSIM methods. In this context, the IEEE established a new standard called ‚ÄôIEEE P3333.1.4‚Äô, which defines metrics and provides recommended practices for light field quality assessment. A standardization activity, namely ‚ÄôJPEG Pleno Quality Assessment‚Äô, was recently initiated within the JPEG committee aiming to explore the most promising subjective quality assessment practices as well as the objective methodologies for plenoptic modalities in the context of multiple use cases. Summary of the objective quality assessment methods for light fields\n5. Discussion, challenges and perspectives While parallel to light fields, other plenoptic modalities like point cloud and holography have also been developed. Even though point clouds and holographic content processing and compression have advanced significantly in recent years, these content types may eventually need to be converted to light field views for visualization on the display. Light fields provide more comprehensive information when it comes to capturing scenes. Light fields capture not only the 3D objects but also the entire scene information, which can be essential in many applications like autonomous driving, that require accurate 3D recreation of the vehicle surroundings. Recent advances in using deep learning for spatio-angular reconstruction and the emergence of the NeRF-based approaches. More recent methods, such as 3D Gaussian splatting, show improvements in the quality‚Äìspeed trade-off. Enhancing the quality‚Äìspeed trade-off could enable new use cases such as real-time telepresence and robotic tasks with fewer views for reconstruction. Two neural scene representations of light fields: an explicit representation based on multiple SAIs an implicit neural representation encodes light fields as parameters of an MLP. Advances in deep learning frameworks are expected to significantly improve the performance of light field processing algorithms and solve the existing challenges. Better depth estimation or depth-free approaches are critical. Advanced methods are needed to efficiently exploit the huge amount of redundant information about the light rays in the same scene that conveys angular and spatial information Now targeting the creation of a learning-based coding standard to provide competitive compression efficiency compared to state-of-the-art light field coding solutions. The evaluation of light field imaging systems has several shortfalls related to the content and assessment approaches available. Advanced methods are needed to efficiently exploit the huge amount of redundant information about the light rays in the same scene that conveys angular and spatial information. It is essential to provide more comprehensive light field datasets from both the quantitative and content diversity perspectives. The assessment of plenoptic image quality also faces various challenges because of the variety of quality aspects and complexity of the content when compared to the assessment of 2D images. JPEG has begun developing a light field quality assessment standard, defining a framework with subjective quality assessment protocols and objective quality assessment procedures for lossy decoding of light field data within the context of multiple use cases. The IEEE is also developing a standard called ‚ÄúP3333.1.4‚ÄîRecommended Practice for the Quality Assessment of light field Imaging‚Äù that targets to establish methods of quality assessment of light field imaging based on psychophysical studies 6. Conclusions üëâüèº Content:\nCore Focus Progress and trends Challenges Future outlook Core Focus Main Tasks Reviewed:\nDepth estimation, reconstruction, super-resolution, and compression. Other Tasks:\nMicroscopy, saliency, face recognition, refocusing, and relighting also benefit from learning-based methods. Progress and Trends Deep Learning Integration:\nAI frameworks now appear in almost every stage of light field processing. Growth Drivers:\nBetter capture and display hardware and larger datasets will accelerate progress. Challenges Limited Realism:\nCurrent systems have narrow Field of View (FoV) and Depth of Field (DoF); Still far from true 6-DoF free-view exploration. Data Burden:\nExpanding datasets increase computational cost and reduce processing efficiency. Future Outlook Compression Evolution:\nLearning-based image compression is expected to greatly improve light field storage and transmission, making real-world applications more feasible. ","permalink":"/notes/learningbased_light_field_imaging/","summary":"\u003ch1 id=\"initial-impression\"\u003eInitial Impression\u003c/h1\u003e\n\u003caside\u003e\nüëâüèº\n\u003cp\u003eThe light field refres to the representation of a scene. Unlike the tranditional 2D image, it adds an extra dimention: the direction of light. This means each image captures not only the length and width, but also the direction of light.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTraditional image: \u003cstrong\u003e2D spatially\u003c/strong\u003e (width, height), but each pixel carries a \u003cstrong\u003e3-value vector\u003c/strong\u003e (RGB, 3 channels for color).\u003c/li\u003e\n\u003cli\u003eLight filed(width, heidht, X-direction, Y-direction): \u003cstrong\u003e4D images\u003c/strong\u003e, but each pixel carries a \u003cstrong\u003e3-value vector.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs a result, light field data is massive and challenging to process.\u003c/p\u003e","title":"Learning‚Äëbased light field imaging"},{"content":"1. Abstract Background:\nsummarization ‚Üí Traditional RAG works well for specific questions (‚ÄúWhen was Company X founded?‚Äù), but it struggles with broad, global ones (‚ÄúWhat are the main ideas in all these documents?‚Äù). scalability ‚Üí (Such questions need summarization of the whole dataset, not just retrieving a few passages ‚Äî that‚Äôs called query-focused summarization (QFS).) Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. we need to combine scalability and summarization: combines knowledge graph generation and query-focused summarization GraphRAG,\na graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text.\nBuild a graph index in two stages:\nderive an entity knowledge graph from the source documents. a knowledge graph (nodes = entities, edges = relationships) pre-generate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user.\n1. Introduction GraphRAG,\nuses an LLM to construct a knowledge graph a knowledge graph, nodes correspond to key entities in the corpus and edges represent relationships between those entities. it partitions the graph into a hierarchy of communities of closely related entities, before using an LLM to generate community-level summaries. GraphRAG answers queries through map-reduce processing of community summaries. In the map step ‚Üí the summaries are used to provide partial answers to the query independently and in parallel, In the reduce step ‚Üí the partial answers are combined and used to generate a final global answer. GraphRAG contrasts with vector RAG (text embeddings) in its ability to answer queries that require global sensemaking over the entire data corpus.\n2. Background Adaptive Benchmarking ‚Üí the process of dynamically generating evaluation benchmarks tailored to specific domains or use cases.\nGenerating test questions based on the current knowledge base. Measuring how well the model adapts when the corpus changes. Evaluating both retrieval and generation quality together. 3. Methods The high-level data flow of the GraphRAG approach and pipeline:\nCommunity detection is used to partition the graph index into groups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time.\nnum of duplicates ‚Üí edge weights claims ‚Üí similarity Entities \u0026amp; Relationships ‚Üí Knowledge Graph\nComponent Purpose Typical Technique (as described or implied) LLM extraction Identify entities/relations/claims Prompt-based, few-shot examples Entity matching Merge identical names Exact string match (default), fuzzy possible Graph construction Store nodes/edges Simple adjacency list or NetworkX graph Edge weighting Track frequency of relationships Count duplicates Aggregation \u0026amp; summarization Produce node/edge descriptions LLM summarization Community detection Find clusters Leiden algorithm (modularity optimization) For a given community level, the global answer to any user query is generated as follows:\nPrepare community summaries. Community summaries are randomly shuffled and divided into chunks of pre-specified token size. This ensures relevant information is distributed across chunks, rather than concentrated (and potentially lost) in a single context window. Map community answers. Intermediate answers are generated in parallel. The LLM is also asked to generate a score between 0-100 indicating how helpful the generated answer is in answering the target question. Answers with score 0 are filtered out. Reduce to global answer. Intermediate community answers are sorted in descending order of helpfulness score and iteratively added into a new context window until the token limit is reached. This final context is used to generate the global answer returned to the user. ","permalink":"/notes/from_local_to_global_a_graphrag_approach_to_query-/","summary":"\u003ch1 id=\"1-abstract\"\u003e1. Abstract\u003c/h1\u003e\n\u003cp\u003eBackground:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003esummarization ‚Üí\u003c/strong\u003e Traditional RAG works well for \u003cem\u003especific\u003c/em\u003e questions (‚ÄúWhen was Company X founded?‚Äù), but it struggles with \u003cem\u003ebroad\u003c/em\u003e, \u003cem\u003eglobal\u003c/em\u003e ones (‚ÄúWhat are the main ideas in all these documents?‚Äù).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003escalability ‚Üí\u003c/strong\u003e (Such questions need \u003cstrong\u003esummarization of the whole dataset\u003c/strong\u003e, not just retrieving a few passages ‚Äî that‚Äôs called \u003cstrong\u003equery-focused summarization\u003c/strong\u003e (QFS).) Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems.\u003c/li\u003e\n\u003cli\u003ewe need to combine \u003cstrong\u003escalability\u003c/strong\u003e and \u003cstrong\u003esummarization:\u003c/strong\u003e combines knowledge graph generation and query-focused summarization\u003c/li\u003e\n\u003c/ol\u003e\n\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eGraphRAG\u003c/strong\u003e,\u003c/p\u003e","title":"From Local to Global A GraphRAG Approach to Query-"},{"content":"Abstract Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. pre-trained models non-parametric memory differentiable access mechanism - In soft differentiable access mechanism, we don‚Äôt discard any chunks. - In Hard retrieval (standard RAG), the retriever picks the top-k passages We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. pre-trained models ‚Üí seq2seq model non-parametric memory ‚Üí a dense vector index of Wikipedia differentiable access mechanism ‚Üí a pre-trained neural retriever 1. Prompt (question) arrives. 2. Seq2seq encoder turns it into query vector q. 3. Retriever compares q to all memory keys k_i (Wikipedia passage vectors). 4. Compute similarity scores s_i = q ‚ãÖ k_i. 5. Apply softmax ‚Üí attention weights Œ±_i. 6. Read vector r = Œ£ Œ±_i v_i (weighted mixture of passage info). 7. Feed r (plus q) into seq2seq decoder ‚Üí generate answer token by token. 8. Gradients flow through Œ±_i ‚Üí retriever learns to attend to more relevant chunks. text chunk ‚Üí retriever encoder ‚Üí key/value ‚Üí FAISS index ‚Üí query embedding ‚Üí top-k retrieval ‚Üí generator We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. same retrieved passages ‚Üí RAG-Sequence different passages per token ‚Üí RAG-Token It‚Äôs often used for knowledge-intensive tasks, not free-form story generation. Discussion We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining.\nThis is one of RAG‚Äôs biggest advantages over standard language models: - You can update its knowledge base without retraining its parameters. The retriever learns the mapping ‚Üí parametric The index just holds the results ‚Üí non-parametric Retriever model A neural network that encodes queries and documents into vectors. Parametric ‚Äî it has learnable weights (parameters) Retrieval index (memory) The database of all document embeddings (keys + values) Non-parametric ‚Äî stored outside the model‚Äôs paras Index = structure that accelerates similarity search using ANN methods (cluster-and-search) (ANN -\u0026gt; Approximate nearest neighbor). Key = pre-computed document embedding; Value = original text (encoded later when used). Hard retrieval = pick top-k texts ‚Üí concatenate ‚Üí generator sees text. Soft retrieval = mix all embeddings by attention ‚Üí generator sees one read vector. Generator (e.g., BART or T5): a Transformer-based seq2seq model. 1. Introduction RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\nWe can make an anology: - RAG\u0026#39;s retriever is like encoder, because it summarizes what info the model should pay attention to before generation. - RAG\u0026#39;s generator is like decoder, because it generate the sequence token-by-token. Steps The retriever (Dense Passage Retriever, henceforth DPR) provides latent documents conditioned on the input, The seq2seq model (BART) then conditions on these latent documents together with the input to generate the output. 2. Methods Our models leverage 2 components:\na retriever $p_Œ∑(z|x)$ a generator $p_Œ∏(y_i|x, z, y_{1:i‚àí1})$ x = the query (e.g., a question or a sentence you want to search with) z = a text passage (a possible relevant document) Œ∑ = the parameters of the model/retriever **p(z‚à£x) = the prob that passage z is relevant to the query x** --- y1:i-1 = the previous i-1 tokens z = the retrieved passage x = the original input **pŒ∏(yi|x, z, y1:i‚àí1) = the prob that generating token yi, give three inputs.** We propose 2 models (based on the average of the latent documents in different ways to produce a distribution over generated text) :\nRAG-Token ‚Üí can predict each target token based on a different doc/chunk. RAG-Sequence ‚Üí the model uses the same doc/chunk to predict each target token. 2.1 Models RAG-Sequence Model: The RAG-Sequence model uses the same retrieved doc/chunk to generate the complete seq.\nRAG-Token Model: we can draw a different latent document for each target token and marginalize accordingly.\n2.2 Retriever: DPR We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. We refer to the document index as the non-parametric memory.\nDPR (Dense Passage Retriever): a bi-encoder architecture: d(z) = a dense representation of a document produced by a BERT document encoder. q(x) = a query representation produced by a query encoder, also based on BERT. MIPS (Maximum Inner Product Search) ‚Üí The operation of finding top-k documents by inner product between query and every docs. 2.3 Generator: BART We use BART-large, a pre-trained seq2seq transformer with 400M parameters. We simply concatenate the input x and the retrieved content z.\nBART combines the strengths of BERT and GPT: - BERT: bidirectional understanding (encoder) - GPT: left-to-right generation (decoder) 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved.\nUpdating the document encoder **BERTd** during training is costly as it requires the document index to be periodically updated as **REALM** does during pre-training. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder **BERTq** and the **BART generator**. BERTd = document encoder BERTq = query encoder REALM = Retrieval-Enhanced Adaptive Language Model update the doc encoder required re-encoding all documents every few steps ‚Äî which made it extremely slow and hard to scale. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate $arg max_y p(y|x)$.\nRAG-Token Model: standard beam search RAG-Sequence Model: Thorough Decoding or Fast Decoding An autoregressive model = predicts the next token based on all previous tokens. - Thorough Decoding = Generate and score candidate answers for every retrieved document, then combine their probabilities - most accurate but slow. - Fast Decoding = Only score candidates that were actually generated during beam search, skipping others ‚Äî much faster but approximate. RAG-Token Âú®ÁîüÊàêËøáÁ®ã‰∏≠ÔºåÊ®°Âûã‰ºöÂèÇËÄÉÊØè‰∏™ chunk ‰∏ãÁöÑÊù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏ÉÔºö$p_\\theta(y_i \\mid x, z, y_{1:i-1})$ Êù•È¢ÑÊµã‰∏ã‰∏Ä‰∏™ token ÁöÑÂèØËÉΩÊÄß„ÄÇ ÁÑ∂ÂêéÊ†πÊçÆÊ£ÄÁ¥¢Âô®ÁªôÂá∫ÁöÑÊØè‰∏™ chunk ÁöÑÊùÉÈáç $p_\\eta(z|x)$ÔºåÂØπËøô‰∫õÂàÜÂ∏ÉËøõË°åÂä†ÊùÉËûçÂêàÔºåÂæóÂà∞‰∏Ä‰∏™ÁªºÂêàÁöÑ‰∏ã‰∏ÄËØçÊ¶ÇÁéáÂàÜÂ∏ÉÔºö$p\u0026rsquo;(y_i \\mid x, y_{1:i-1}) = \\sum_z p_\\eta(z|x),p_\\theta(y_i \\mid x, z, y_{1:i-1})$ Ê®°Âûã‰ªéËøô‰∏™ËûçÂêàÂàÜÂ∏É‰∏≠ÈÄâÂá∫Ê¶ÇÁéáÊúÄÈ´òÁöÑ tokenÔºåÂÜçÂ∞ÜÂÖ∂Âä†ÂÖ•Âà∞Â∑≤ÁîüÊàêÁöÑÂ∫èÂàó‰∏≠„ÄÇ ÈáçÂ§çËØ•Ê≠•È™§ÔºåÁõ¥Âà∞ÁîüÊàêÂÆåÊï¥Âè•Â≠ê„ÄÇ üí° This makes generation very fast, but because it can borrow inconsistent or partially incorrect evidence from different chunks, the final sentence may contain blended or wrong facts, even though the decoding itself is efficient.\nRAG-Sequence (Thorough Decoding) ÂÖàÂú®ÊØè‰∏™ chunk ‰∏ãÁã¨Á´ãËøêË°å beam searchÔºåÂæóÂà∞Ê¶ÇÁéáÊúÄÈ´òÁöÑÂÄôÈÄâÂè•Â≠êÔºõ ÁÑ∂ÂêéÂ∞ÜËøô‰∫õÂÄôÈÄâÂè•ÂàÜÂà´Âú®ÂÖ∂‰ªñ chunk ‰∏äÈáçÊñ∞ËÆ°ÁÆóÁîüÊàêÊ¶ÇÁéá $p_\\theta(y|x,z)$Ôºà‰ΩøÁî® teacher forcing Âº∫Âà∂ÁîüÊàêÔºâÔºå ÊúÄÂêéÊ†πÊçÆÊØè‰∏™ chunk ÁöÑÊ£ÄÁ¥¢ÊùÉÈáç $p_\\eta(z|x)$ ÂØπÂè•Â≠êÊ¶ÇÁéáËøõË°åÂä†ÊùÉÊ±ÇÂíåÔºö$p(y|x) = \\sum_z p_\\eta(z|x),p_\\theta(y|x,z)$ ÊúÄÁªàÈÄâÂá∫Êï¥‰ΩìÊ¶ÇÁéáÊúÄÈ´òÁöÑÂè•Â≠ê‰Ωú‰∏∫ËæìÂá∫„ÄÇ üí° This ‚Äúglobal reconsideration‚Äù allows the model to filter out wrong or inconsistent sentences and select the most related one overall. However, because it must compute the probability of every candidate on every chunk, the process is extremely slow.\nRAG-Sequence (Fast Decoding) ÂÖàÂú®ÊØè‰∏™ chunk ‰∏ãÁîüÊàêÊ¶ÇÁéáÊúÄÈ´òÁöÑÂÄôÈÄâÂè•Â≠êÔºå ‰ΩÜÂè™Âú®ÁîüÊàêËøáËØ•Âè•Â≠êÁöÑ chunk‰∏äËÆ°ÁÆóÊ¶ÇÁéáÔºå Êú™ÁîüÊàêËØ•Âè•Â≠êÁöÑ chunk Áõ¥Êé•ÂøΩÁï•ÔºàËÆ§‰∏∫Ê¶ÇÁéá‚âà0ÔºâÔºå ÂÜçËøõË°åÂêåÊ†∑ÁöÑÂä†ÊùÉÊ±ÇÂíå„ÄÇ üí° This method is a trade-off between the two. It still generates separate sentences for each chunk, but it skips the expensive re-evaluation on other chunks‚Äîonly using the chunks that actually produced each sentence.\nAs a result, it‚Äôs much faster than thorough decoding while keeping almost the same accuracy, though it‚Äôs still slower than RAG-Token.\nComparison Item RAG-Token RAG-Sequence (Thorough) RAG-Sequence (Fast) Fusion Timing Dynamically fuses predictions from all chunks at each token Uses a fixed chunk for the whole sentence, then re-evaluates globally Uses a fixed chunk for the whole sentence, then re-evaluates locally Fusion Granularity Token-level Sentence-level Sentence-level Decoding Method Single beam search Multiple beam searches + full re-evaluation Multiple beam searches + partial re-evaluation Cross-chunk Generation ‚úÖ Allowed ‚ùå Not allowed ‚ùå Not allowed Accuracy Medium Highest High Speed Fast Slow Faster Typical Usage Common for online inference Mainly theoretical analysis / small-scale experiments Practical trade-off in real applications Probability Computation Sum across chunks at each token Sum across chunks after full sentence generation Sum across chunks after full sentence generation Core Idea Fuse multiple chunk predictions at every step Generate each sentence independently, then globally combine Generate each sentence independently, then combine locally Key Characteristics Each word leverages all chunks ‚Äî very fast but may produce inconsistent sentences Theoretically most accurate but computationally slow Approximate yet efficient ‚Äî widely used in practice 3. Experiments For all experiments:\nNon-parametric knowledge source: the December 2018 dump Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M docs. Build a single MIPS index using FAISS with a Hierarchical Navigable Small World approximation for fast retrieval. During training:\nWe retrieve the top k documents for each query. We consider k ‚àà {5, 10} for training and set k for test time using dev data. 3.1 Open-domain Question Answering Compare with:\nThe extractive QA paradigm ‚Äì extracts short answer spans directly from retrieved documents, relying mainly on non-parametric knowledge. The Closed-Book QA approaches ‚Äì generate answers without retrieval, depending only on parametric knowledge stored in the model. Consider four popular open-domain QA datasets:\nNatural Questions (NQ) TriviaQA (TQA) WebQuestions (WQ) CuratedTrec (CT) (CT and WQ are small; models are initialized from the NQ-trained RAG model.)\nEvaluate:\nPerformance is measured using Exact Match (EM) ‚Äì a metric that checks whether the generated answer exactly matches the reference answer. üí° Focus: finding facts from retrieval, not writing sentences.\n3.2 Abstractive Question Answering Evaluate:\nThe MSMARCO NLG v2.1 task, which tests RAG‚Äôs ability to generate free-form, natural language answers in a knowledge-intensive setting. Setup:\nEach example includes a question, ten gold retrieved passages, and a full-sentence human-written answer. RAG ignores the supplied passages and treats MSMARCO as an open-domain QA task (retrieving from Wikipedia instead). Note:\nSome questions cannot be answered correctly without the gold passages (e.g., ‚ÄúWhat is the weather in Volcano, CA?‚Äù). In such cases, RAG relies on its parametric knowledge to generate reasonable responses. üí° Focus: natural, fluent language generation (NLG).\n3.3 Jeopardy Question Generation Task:\nGiven an answer entity, generate a factual Jeopardy-style question (reverse QA). Dataset:\nSearchQA, with 100K train / 14K dev / 27K test examples. Compare:\nRAG vs BART (baseline model). Evaluate:\nQ-BLEU-1 metric (favors entity matching and factual accuracy). Human evaluation on two criteria: Factuality ‚Äî whether the question is factually correct. Specificity ‚Äî whether the question is closely related to the given answer. üí° Focus: evaluate RAG‚Äôs generation abilities in a non-QA setting.\n3.4 Fact Verification Task:\nGiven a claim, classify whether it is supported, refuted, or not enough info using evidence from Wikipedia. Dataset:\nFEVER benchmark. Method:\nMap each class label to a single output token, treating the task as sequence classification. RAG trains without supervision on retrieved evidence, learning retrieval and reasoning jointly. Evaluate:\nReport label accuracy for both: 3-way classification: supports / refutes / not enough info 2-way classification: supports / refutes Purpose:\nTest RAG‚Äôs capability for reasoning-based classification, not just text generation. üí° Focus: reasoning and classification with retrieval (not generation)\n4. Results Open-domain Question Answering Abstractive Question Answering Jeopardy Question Generation Fact Verification Table 1 \u0026amp; 2 üí° Table 1:\nTo show that RAG outperforms previous retrieval-based QA systems (like DPR and REALM) and even large closed-book models (like T5), Proving that retrieval + generation can achieve state-of-the-art results without re-rankers or extractive readers. Table 2:\nTo demonstrate that RAG generalizes beyond simple QA: it performs strongly on abstractive answer generation (MSMARCO), question generation (Jeopardy), and fact verification (FEVER) showing it works for both generation and classification tasks, even without gold evidence Table 3 Table 4 \u0026amp; 5 üí° Table 4:\nTo verify through human judgment that RAG‚Äôs generated questions are more factual and more specific than those from BART, confirming that retrieval grounding improves accuracy and relevance in text generation. Table 5:\nTo measure linguistic diversity of generated text ‚Äî showing that RAG‚Äôs outputs are less repetitive and more varied (closer to human text) than BART‚Äôs, thanks to diverse retrieved contexts. Factuality ‚Üí Is the question factually correct? Specificity ‚Üí Does the question precisely match its given answer (not too generic)? Table 6 ‚ÄúAblation‚Äù means removing or changing a part of the model to test how much it matters.\nüí° Table 6 shows that RAG‚Äôs learned dense retriever is essential\nreplacing it with BM25 or freezing it significantly hurts performance. proving that jointly learned retrieval is key for strong open-domain generation and QA results. Figure 2 The heatmap (right) shows which retrieved document (y-axis) the model relies on when generating each token (x-axis) of a sentence.\nThe heatmap shows a dark blue cell at (Doc 2, ‚ÄúSun‚Äù), which means Doc 2 ‚Äî the one containing ‚ÄúThe Sun Also Rises‚Äù ‚Äî is strongly influencing this token. (The model correctly ‚Äúlooks up‚Äù the document that mentions that book.)\nAfter that, the dark blue (posterior weight) flattens ‚Äî it spreads out across documents. That means: once the model has started generating ‚ÄúThe Sun‚Ä¶‚Äù, it can finish ‚ÄúAlso Rises‚Äù without continuing to depend on that document.\nüí° After seeing one or two key words from retrieval chunks (non-parametric memory), the generator‚Äôs parametric knowledge is enough to recall and complete the title.\nThe non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory.\n","permalink":"/notes/retrieval-augmented_generation_for_knowledge-inten/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003ePre-trained models\u003c/strong\u003e with a \u003cstrong\u003edifferentiable access mechanism\u003c/strong\u003e to \u003cstrong\u003eexplicit non-parametric memory\u003c/strong\u003e have so far been only investigated for extractive downstream tasks.\n\u003cul\u003e\n\u003cli\u003epre-trained models\u003c/li\u003e\n\u003cli\u003enon-parametric memory\u003c/li\u003e\n\u003cli\u003edifferentiable access mechanism\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-html\" data-lang=\"html\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e- In soft differentiable access mechanism, we don‚Äôt discard any chunks.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e- In Hard retrieval (standard RAG), the retriever picks the top-k passages\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\n\u003cli\u003eWe introduce RAG models where the parametric memory is \u003cstrong\u003ea pre-trained seq2seq model\u003c/strong\u003e and the non-parametric memory is \u003cstrong\u003ea dense vector index of Wikipedia\u003c/strong\u003e, accessed with \u003cstrong\u003ea pre-trained neural retriever\u003c/strong\u003e.\n\u003cul\u003e\n\u003cli\u003epre-trained models ‚Üí seq2seq model\u003c/li\u003e\n\u003cli\u003enon-parametric memory ‚Üí a dense vector index of Wikipedia\u003c/li\u003e\n\u003cli\u003edifferentiable access mechanism ‚Üí a pre-trained neural retriever\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-html\" data-lang=\"html\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e1. Prompt (question) arrives.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e2. Seq2seq encoder turns it into query vector q.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e3. Retriever compares q to all memory keys k_i (Wikipedia passage vectors).\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e4. Compute similarity scores s_i = q ‚ãÖ k_i.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e5. Apply softmax ‚Üí attention weights Œ±_i.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e6. Read vector r = Œ£ Œ±_i v_i  (weighted mixture of passage info).\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e7. Feed r (plus q) into seq2seq decoder ‚Üí generate answer token by token.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e8. Gradients flow through Œ±_i ‚Üí retriever learns to attend to more relevant chunks.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003etext chunk ‚Üí retriever encoder ‚Üí key/value ‚Üí FAISS index ‚Üí query embedding ‚Üí top-k retrieval ‚Üí generator\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\n\u003cli\u003eWe compare two RAG formulations, one which conditions on the \u003cstrong\u003esame retrieved passages\u003c/strong\u003e across the whole generated sequence, and another which can use \u003cstrong\u003edifferent passages\u003c/strong\u003e per token.\n\u003cul\u003e\n\u003cli\u003esame retrieved passages ‚Üí RAG-Sequence\u003c/li\u003e\n\u003cli\u003edifferent passages per token ‚Üí RAG-Token\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-html\" data-lang=\"html\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eIt‚Äôs often used for knowledge-intensive tasks, not free-form story generation.\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch1 id=\"discussion\"\u003eDiscussion\u003c/h1\u003e\n\u003cp\u003eWe conducted an thorough investigation of the learned retrieval component, validating\nits effectiveness, and we illustrated how the retrieval index can be \u003cstrong\u003ehot-swapped\u003c/strong\u003e to update the model without requiring any retraining.\u003c/p\u003e","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"content":"Von Neumann model The von Neumann model was an ‚Äúefficient bridge between software and hardware‚Äù because:\nHardware designers could build chips to execute it efficiently. Software developers could write high-level programs that compile into this model. Thus, the von Neumann model is the connecting bridge that enables programs from the diverse and chaotic world of software to run efficientby on machines from the diverse and chaotic world of hardware.\nBulk-synchronous parallel (BSP) model it is a viable candidate for the role of bridging model.\nValiant wants parallel simulations to be almost as fast as ideal ones.\nThe extra cost should be only a small constant factor, not growing with processor count. He tries to avoid efficiency loss that scales with log(p). The model should work well for any number of processors, from a few to millions. Features of BSP model A major feature of the BSP model is that it provides this option with optimal efficiency (i.e., within constant factors) provided the programmer writes programs with sufficient parallel slackness.\nBSP can automatically manage communication and memory efficiently if the program exposes\nenough parallel work. If there‚Äôs enough parallelism (many tasks per processor), the model achieves near-optimal performance with little manual tuning.\n","permalink":"/notes/a_bridging_model_for_parallel_computation/","summary":"\u003ch2 id=\"von-neumann-model\"\u003eVon Neumann model\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003evon Neumann model\u003c/strong\u003e was an \u003cem\u003e‚Äúefficient bridge between software and hardware‚Äù\u003c/em\u003e because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHardware designers\u003c/strong\u003e could build chips to execute it efficiently.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware developers\u003c/strong\u003e could write high-level programs that compile into this model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThus, the \u003cstrong\u003evon Neumann model\u003c/strong\u003e is the connecting bridge that enables programs from\nthe diverse and chaotic world of software to run efficientby on machines from the diverse and chaotic world of hardware.\u003c/p\u003e\n\u003ch2 id=\"bulk-synchronous-parallel-bsp-model\"\u003eBulk-synchronous parallel (BSP) model\u003c/h2\u003e\n\u003cp\u003eit is a viable candidate for the role of \u003cstrong\u003ebridging model\u003c/strong\u003e.\u003c/p\u003e","title":"A Bridging Model for Parallel Computation"},{"content":"Video Source\nPage Source\nPratical Source\n1. Introduction Extension of Abstract.\nSequence Modeling: Recurrent language modelsExtension of Abstract.\nOperate step-by-step, processing one token or time step at a time. one input + hidden state ‚Üí one output + hidden state (update every time) e.g., RNNs, LSTMs Encoder-decoder architectures\nComposed of two RNNs (or other models): one to encode input into a representation another to decode it into the output sequence one input + hidden state ‚Üí ‚Ä¶ ‚Üí hidden state ‚Üí one output + hidden state ‚Üí ‚Ä¶ e.g., seq2seq Disadvantages about these two: Recurrent models preclude parallelization, so it‚Äôs not good.\nEncoder-decoder models have used attention to pass the stuff from encoder to decoder more effectively.\nAttention doesn‚Äôt use recurrence and entirely relies on an attention mechanism to draw glabal dependencies between input and output. 2. Background Corresponding Work:\nSpeak clearly about the corresponding papers, the connections between u and the papers, and the differences.\nReduce sequential computation: Typically use convolutional neural networks ‚Üí the number of operations grows in the distance between positions ‚Üí transfomer: a constant number of operations\nat the cost of reduced effective resolution due to averaging attention-weighted positions we counteract this bad effect with Multi-Head Attention Self-attention: compute a representation of the sequence based on different positions\nEnd-to-end memory networks: ‚úî Reccurrence attention mechanism\n‚ùå Sequence-aligned recurrence\nTransformer: The first transduction model relying entirely on self-attention to do encoder-decoder model. ( to compute representations of its input and output without using sequence-aligned RNNs or convolution)\ntransduction model refers to making predictions or inferences about specific instances based on the data at hand, without relying on a prior generalization across all possible examples, which is different from Inductive learning.\n3. Model Architecture The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and the decoder.\nEncoder: maps an input sequence of symbol representations x ‚Üí a sequence of continuous representations z. Decoder: given z, generates an output sequence of symbols one element at a time. At each step, the model is auto-regressive. Why Using LayerNorm not BatchNorm? LayerNorm normalizes across features of a single sample, suitable for variable-length sequences.\nBatchNorm normalizes across the batch, which can be inconsistent for sequence tasks.\n3.1 Encoder and Decoder Stacks Encoder N = 6 identical layers, each layer has two sub-layers.\nmulti-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\ndimension of output = 512\nDecoder N = 6 identical layers, each layer has three sub-layers.\nmasked multi-head attention over the output of the encoder stack multi-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\n3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output where the query, keys, values, and output are all vectors.\nThe output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n( Given a query, output is computed by similarity between query and keys, then different keys have different weights, next combine the values based on the weights. )\n3.2.1 Scaled Dot-Product Attention We compute the dot products of the query with all keys, divide each by length of query ‚àö, and apply a softmax function to obtain the weights on the values.\nWhy divide each by length of query ‚àö ?\nIt prevents the input to the softmax from becoming excessively large, ensuring that the softmax outputs remain well-distributed and numerically stable. The softmax function normalize the scores into probabilities, but it doesn\u0026rsquo;t address the problem of large input magnitudes directly. If the inputs to softmax are extremely large, the e^x can become numerically unstable, leading to issues in computation.\nn refers to the length of a sequence, the number of the words.\ndk refers to the length of the one word vector.\nm refers to the number of the target words.\ndv refers to the length of the one target word vector.\nWhat there is a function of the Mask?\nWhen computing the output, I only use the key-value pairs up to the current time and do not use any later key-value pairs.\n3.2.2 Multi-Head Attention Linear projection, just like lots of channels.\nLinearly project the queries, keys and values h times to low dimensions with different, learned linear projections.\nFinally, these are concatenated (stack) and once again projected.\nWe emply h = 8 parallel attention layers, or heads.\nSo for each layer or head, we make their dimension to 64 to make the total computational cost is similar to single-head attention.\n3.2.3 Application of Attention in our Model The Transformer uses multi-head attention in three different ways:\nIn ‚Äúencoder-decoder attention‚Äù layers, queries ‚Üí previous decoder layer keys and values ‚Üí the output of the encoder In ‚Äúencoder self-attention‚Äù layers, keys, values and queries all come from the previous layer in the encoder In ‚Äúdecoder self-attention‚Äù layers, keys, values and queries all come from the previous layer in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position 3.3 Position-wise Feed-Forward Networks The fully connected feed-forward network is applied to each position separately and identically.\nThe Multi-Head Attention has already get the location information, now we need to add more expression ability by adding non-linear.\nThe output and MLP patterns are the same. The difference are the input source.\n3.4 Embedding and Softmax Embedding convert the input tokens to vectors of dimension ( 512 ), then multiply those weights by\n$\\sqrt {d_{models}}$\nWhy multiply those weights by $\\sqrt {d_{models}}$ ?\nTo boosts the magnitude, making it more aligned with other components of the model. Because the initial value is between 0~1 by using normal distribution.\nWe update the L2 norm of the embeddings to 1 finally, so we need to ensure the value of each dimension not too small. The L2 norm value of a vector to 1 is best to express a word, because it only express direction.\nSoftmax convert the decoder output to predicted next-token probabilities.\n3.5 Positional Encoding The order change, but the values don‚Äôt change. So we add the sequential information to the input.\nWe use sine and cosine functions of different frequencies [-1, 1]\n4. Why Self-Attention Length of a word ‚Üí d Number of words ‚Üí n Self-Attention ‚Üí n words ‚úñÔ∏è every words need to multiply with n words and for each two words do d multiplying. Recurrent ‚Üí d-dimension vector multiply d‚úñÔ∏èd matrix, n times Convolutional ‚Üí k kernel_size, n words, d^2 input_channels ‚úñÔ∏è output_channels (Draw picture clear) Self-Attention (restricted) ‚Üí r the number of neighbors It seems like Self-Attention architecture has lots of advantages, but it needs more data and bigger model to train to achieve the same effect.\n5. Training 5.1 Training Data and Batching Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. ‚Üí So we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation.\n5.2 Hardware and Schedule 5.3 Optimizer 5.4 Regularization Residual Dropout Label Smoothing 6. Results $N:$ number of blocks\n$d_{model}:$ the length of a token vector\n$d_{ff}:$ Feed-Forward Full-Connected Layer Intermediate layer output size\n$h:$ the number of heads\n$d_k:$ the dimension of keys in a head\n$d_v :$ the dimension of values in a head\n$P_{drop}:$ dropout rate\n$\\epsilon_{ls}:$ Label Smoothing value, the learned label value\n$train steps:$ the number of batchs\n","permalink":"/notes/attention_is_all_you_need/","summary":"\u003cp\u003e\u003ca href=\"https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.1387.top_right_bar_window_history.content.click\u0026amp;vd_source=83d7a54445de4810b52e58e4864b4605\"\u003eVideo Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1706.03762\"\u003ePage Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=kCc8FmEb1nY\"\u003ePratical Source\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"1-introduction\"\u003e1. Introduction\u003c/h1\u003e\n\u003caside\u003e\n\u003cp\u003eExtension of Abstract.\u003c/p\u003e\n\u003c/aside\u003e\n\u003ch2 id=\"sequence-modeling\"\u003eSequence Modeling:\u003c/h2\u003e\n\u003cp\u003eRecurrent language modelsExtension of Abstract.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOperate step-by-step, processing one token or time step at a time.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state ‚Üí one output + hidden state (update every time)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., RNNs, LSTMs\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEncoder-decoder architectures\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eComposed of two RNNs (or other models):\n\u003cul\u003e\n\u003cli\u003eone to encode input into a representation\u003c/li\u003e\n\u003cli\u003eanother to decode it into the output sequence\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state ‚Üí ‚Ä¶ ‚Üí hidden state ‚Üí one output + hidden state ‚Üí ‚Ä¶\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., seq2seq\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"disadvantages-about-these-two\"\u003eDisadvantages about these two:\u003c/h2\u003e\n\u003cp\u003eRecurrent models preclude parallelization, so it‚Äôs not good.\u003c/p\u003e","title":"Attention is All You Need"}]