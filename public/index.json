[{"content":" CrossNet and CrossNet++ Both are for Reference-Based Super-Resolution (RefSR), using a low-resolution (LR) image and a high-resolution (HR reference) image to make a sharper, high-quality output.\nThe performance of CrossNet drops with the increasing of perspective parallax, the improvement of CrossNet++:\nTwo-stage warping ‚Üí improves alignment Self-supervised flow estimation ‚Üí uses FlowNet to estimate motion between LR and Ref images Cross-scale alignment ‚Üí Aligns features at multiple resolutions Hybrid loss functions ‚Üí warping + landmark + super-resolution loss Real-world performance ‚Üí produces smoother, sharper, and more realistic results, suitable for a variety of scenarios Abstraction CrossNet++ focuses on reference-based super-resolution (RefSR), improving a low-resolution image using a high-resolution reference from another camera. This task is hard because of large scale differences (8√ó) and big parallax (~10%) between the two views.\nTo solve this, CrossNet++ introduces an end-to-end two-stage network with:\nCross-scale warping modules, align images at multiple zoom levels to narrow down parallax, handle scale and parallax differences. Image encoder and fusion decoder, extract multi-scale features and combine them to reconstruct a high-quality super-resolved image. It uses new hybrid loss functions comprising warping loss, landmark loss and super-resolution loss to improve accuracy and stability by stabilizing the training of alignment module and helps to improve super-resolution performance.\ntwo-stage wrapping, hybrid loss\n1 Introduction The development of method:\npatch-matching + patch-synthesis + iteratively applying non-uniform warping Causes grid artifacts, incapable of handling the non-rigid image deformation Directly warping between the low and high-resolution images is inaccurate. Such iterative combination of patch matching and warping introduces heavy computational burden. The difference between rigid deformation and non-rigid deformation:\nRigid deformation = viewpoint change, like camera movement. Non-rigid deformation = object itself changes shape (face expression, fabric fold, petal bending). Grid artifacts = tiny square patterns caused by wrong image enlargement or alignment.\nwarping + synthesis It cannot effectively handle large-parallax cases that widely existed in real-world data. pre-warping + re-warping + synthesis CrossNet++ is a unified framework enabling fully end-to-end training which does not require pretraining the flow estimator. Two-stage pipeline: Two-stage cross-scale warping module. stage 1: Uses FlowNet to estimate motion (optical flow) between the low-resolution (LR) and reference (Ref) images without needing ground-truth flow (self-supervised). This produces a roughly aligned ‚Äúwarped-Ref‚Äù image. stage 2: Further refines alignment between the warped-Ref and LR image for more accurate warping. Hybrid loss: warping loss, landmark loss and super-resolution loss. warping loss: supervise the flow estimation implicitly. landmark loss: supervise the flow estimation explicitly. Without ground-truth flow = the model learns to estimate motion on its own, using only the images, not any pre-labeled motion data.\nInterpolation = predict inside known area Extrapolation = predict outside known area 2 Related Work 3 Preliminary of CrossNet 3.3 Network Structure 3.3.1 Alignment Module The alignment module aims to align the reference image $I_{REF}$ with the low-resolution image $I_{LR}$. CrossNet++ adopts a warping-based alignment using two-stage optical flow estimation.\nIn the first stage, a modified FlowNet (denoted as $Flow_1$) predicts the flow field between an upsampled LR image $I_{LR‚Üë}$ and the reference image $I_{REF}$:\n$$ V_1^0 = Flow_1(I_{LR‚Üë}, I_{REF}) $$ where $V_1^0$ represents the flow at scale 0 (the original image scale). The upsampled LR image $I_{LR‚Üë}$ is obtained via a single-image SR method:\n$$ I_{LR‚Üë} = SISR(I_{LR}) $$ Then, the reference image is spatially warped using this flow to produce the pre-aligned reference:\n$$ \\hat{I}_{REF} = Warp(I_{REF}, V_1^0) $$ In the second stage, the pre-aligned reference \\( \\hat{I}_{\\mathrm{REF}} \\) and the upsampled LR image \\( I_{LR}\\uparrow \\) are again input to another flow estimator \\( Flow_2 \\) to compute multi-scale flow fields:\n$$ {V_2^3, V_2^2, V_2^1, V_2^0} = Flow_2(I_{LR‚Üë}, \\hat{I}_{REF}) $$ These multi-scale flows are used later in the synthesis network to refine alignment and reconstruct the final super-resolved image.\nthis two-stage alignment, coarse warping followed by multi-scale refinement‚Äîallows CrossNet++ to handle large parallax and depth variations, achieving more accurate correspondence and better alignment quality than the original CrossNet.\n3.3.2 Encoder Through the alignment module, we obtain four flow fields at different scales. The encoder receives the pre-aligned reference image $\\hat I_{REF}$ and the upsampled LR image $I_{LR‚Üë}$, then extracts their feature maps at four different scales.\nThe encoder has five convolutional layers with 64 filters of size ( 5 $\\times$ 5 ).\nThe first two layers (stride = 1) extract the feature map at scale 0. The next three layers (stride = 2) produce lower-resolution feature maps for scales 1 to 3. These operations are defined as: where $\\sigma$ is the ReLU activation, $*_1$ and $*_2$ denote convolutions with strides 1 and 2 respectively, and $F^i$ is the feature map at scale $i$.\n$$ F^0 = \\sigma(W^0 *_{1} I) $$ $$ F^i = \\sigma(W^i *_{2} F^{i-1}), \\\\ \\quad i = 1, 2, 3, $$ Unlike the original CrossNet, CrossNet++ uses a shared encoder for both $\\hat I_{REF}$ and $I_{LR‚Üë}$ instead of two separate encoders, which reduces about 0.41 M parameters while maintaining accuracy.\nThe resulting feature sets are:\n$$ {F^0_{LR}, F^1_{LR}, F^2_{LR}, F^3_{LR}} \\quad \\text{and} \\quad {F^0_{REF}, F^1_{REF}, F^2_{REF}, F^3_{REF}}. $$ Finally, each reference feature map $F^i_{REF}$ is warped using the multi-scale flow fields $V^i_2$ from to produce the aligned feature maps:\n$$ \\hat{F}^i_{REF} = Warp(F^i_{REF}, V^i_2), \\\\ \\quad i = 0, 1, 2, 3. $$ In short, the encoder extracts multi-scale feature maps for both LR and reference images using shared convolutional layers, then aligns the reference features to the LR features through warping with multi-scale flow fields, which provides precise, scale-consistent alignment for the next fusion step.\n3.3.3 Decoder After feature extraction and alignment, the decoder fuses the LR and reference feature maps and generates the final super-resolved image.\nIt follows a U-Net-like structure, which progressively upsamples the feature maps from coarse to fine scales.\nTo create the decoder features at scale $i$, the model concatenates:\nthe warped reference features $\\hat{F}^i_{REF}$, the LR image features $F^i_{LR}$¬†, and the decoder feature from the next coarser scale $F^{i+1}_{D}$ (if available). Then a deconvolution layer (stride 2, filter size 4 $\\times$ 4) is applied:\n$$ F^3_{D} = \\sigma(W^3_{D} *_{2} (F^3_{LR}, \\hat{F}^3_{REF})) $$ where $*_2$ is deconvolution with stride 2 and $\\sigma$ is the activation (ReLU).\n$$ F^i_{D} = \\sigma(W^i_{D} *_{2} (F^i_{LR}, \\hat{F}^i_{REF}, F^{i+1}_{D})), \\\\quad i = 2, 1, $$ After that, three more convolutional layers (filter sizes (5 $\\times$ 5), channels {64, 64, 3}) perform post-fusion to synthesize the final image $I_p$:\n$$ F^0_{D} / F_1 = \\sigma(W_1 *_{1} (F^0_{LR}, \\hat{F}^0_{REF}, F^1_{D})) $$ $$ F_2 = \\sigma(W_2 *_{1} F_1) $$ $$ I_p = \\sigma(W_p *_{1} F_2), $$ where $*_{1}$¬†means convolution with stride 1.\nThe decoder takes aligned multi-scale features from LR and reference images, fuses them step by step through deconvolutions and convolutions, and finally reconstructs the high-resolution output image $I_p$, the sharp, super-resolved result.\n3.4 Loss Function warping loss, landmark loss ‚Üí encourage flow estimator to generate precise flow.\nsuper-resolution loss ‚Üí is responsible for the final synthesized image.\n3.4.1 Warping Loss Used in the first-stage Flow Estimator to regularize the generated optical flow.\nIt ensures that the warped reference image $\\hat I_{REF}$ is close to the ground-truth HR image $I_{HR}$, assuming both share a similar viewpoint.\nThe loss minimizes pixel-wise intensity differences:\n$$ L_{warp} = \\frac{1}{2N} \\sum_{i,s,c} (\\hat{I}_{REF}(s, c) - I_{HR}(s, c))^2 $$ where $N$ is the total number of samples, $i$, $s$, and $c$ iterate over training samples, spatial locations and color channels respectively.\n3.4.2 Landmark Loss This loss provides directional geometric guidance for large-parallax cases.\nIt uses SIFT feature matching to find corresponding landmark pairs $(p, q)$ between the HR and reference images, and applies the flow field $V^0_1$ to warp these landmarks.\nThe warped landmark $\\hat{p}^j$ is computed as:\n$$ \\hat{p}^j = p^j + V^0_1[p^j] $$ and the landmark loss penalizes the distance between warped and target landmarks:\n$$ L_{lm} = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{m_i} | \\hat{p}^j - q^j |_2^2 $$ where $m_i$¬†is the number of landmark pairs in image $i$.\nThis term helps the flow estimator predict more accurate motion fields, especially when viewpoints differ greatly.\n3.4.3 Super-Resolution Loss This loss directly trains the model to synthesize the final super-resolved image $I_p$, comparing it with the ground-truth high-resolution image $I_{HR}$ using the Charbonnier penalty (a smooth $L_1$ loss):\n$$ L_{sr} = \\frac{1}{N} \\sum_{i,s,c} \\rho(I_{HR}(s, c) - I_p(s, c)) $$ $$ \\rho(x) = \\sqrt{x^2 + 0.001^2}. $$ 4 Experiment Flower dataset and LFVideo dataset\n14 $\\times$ 14 angular samples of size 376 $\\times$ 541. training and testing: central 8 $\\times$ 8 grid of angular samples top-left 320 $\\times$ 512 for training and testing training: 3243 images from Flower and 1080 images from LFVideo testing: 100 images from Flower and 270 images from LFVideo ","permalink":"/posts/crossnet++_cross-scale_large-parallax_warping_for/","summary":"\u003caside\u003e\n\u003ch2 id=\"crossnet-and-crossnet\"\u003e\u003cstrong\u003eCrossNet\u003c/strong\u003e and \u003cstrong\u003eCrossNet++\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eBoth are for \u003cstrong\u003eReference-Based Super-Resolution (RefSR),\u003c/strong\u003e using a \u003cstrong\u003elow-resolution (LR)\u003c/strong\u003e image and a \u003cstrong\u003ehigh-resolution (HR reference)\u003c/strong\u003e image to make a sharper, high-quality output.\u003c/p\u003e\n\u003cp\u003eThe performance of \u003cstrong\u003eCrossNet\u003c/strong\u003e drops with the increasing of perspective parallax, the improvement of \u003cstrong\u003eCrossNet++:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTwo-stage warping\u003c/strong\u003e ‚Üí improves alignment\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-supervised flow estimation\u003c/strong\u003e ‚Üí uses \u003cstrong\u003eFlowNet\u003c/strong\u003e to estimate motion between LR and Ref images\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCross-scale alignment\u003c/strong\u003e ‚Üí Aligns features at \u003cstrong\u003emultiple resolutions\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHybrid loss functions\u003c/strong\u003e ‚Üí warping + landmark + super-resolution loss\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReal-world performance\u003c/strong\u003e ‚Üí produces smoother, \u003cstrong\u003esharper\u003c/strong\u003e, and more realistic results, suitable for a variety of scenarios\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003ch1 id=\"abstraction\"\u003eAbstraction\u003c/h1\u003e\n\u003cp\u003eCrossNet++ focuses on \u003cstrong\u003ereference-based super-resolution (RefSR),\u003c/strong\u003e improving a low-resolution image using a high-resolution reference from another camera. This task is hard because of \u003cstrong\u003elarge scale differences (8√ó)\u003c/strong\u003e and \u003cstrong\u003ebig parallax (~10%)\u003c/strong\u003e between the two views.\u003c/p\u003e","title":"CrossNet++: Cross-Scale Large-Parallax Warping for"},{"content":"paper resource\nRWKV = a bridge between LSTM and Transformer (less computation and memory, parallel in training). Replace self-attention with a time-mixing mechanism that behaves like an LSTM in time but a Transformer in training and inference xLSTM = scale up the LSTM family to match or even surpass Transformer-level performance. Keep the recurrent spirit of LSTM, but redesign gates and memory (mLSTM) so it can train and scale efficiently like Transformers. Speciality: memory mixing, RWKV Abstract The paper revisits LSTMs, whose key innovations are the constant error carousel and gating mechanisms ‚Äî originally solved the vanishing-gradient problem and made long-term memory possible. Although Transformers later surpassed LSTMs thanks to their parallelizable self-attention, the authors ask whether LSTMs can be scaled up, like modern LLMs, to billions of parameters while overcoming their known limits.\nTo achieve this, they introduce:\nExponential gating ‚Äî a new gating function with improved normalization and stability. Modified memory structures: sLSTM ‚Äî uses scalar memory and scalar updates with new ‚Äúmemory mixing.‚Äù ‚Üí memory mixing mLSTM ‚Äî introduces matrix-based memory that supports full parallelization and a covariance-based update rule. A new memory architecture ‚Üí parallelization By stacking these enhanced cells into residual xLSTM blocks, they create architectures that combine the strengths of LSTMs and Transformers.\nExperiments show that xLSTMs can match or even outperform Transformers and State Space Models in both performance and scaling.\nüëâ Code: github.com/NX-AI/xlstm\n1 Introduction 1.1 LSTM $$ c_t = f_t c_{t-1} + i_t z_t, \\quad h_t = o_t \\psi(c_t) $$\nUpdate the cell state / long-term memory: $$ c_t = f_t c_{t-1} + i_t z_t $$\n$c_t$: Cell state, real-valued vector, the internal long-term memory after update $f_t$: Forget gate, values in (0, 1), decide how much of $c_{t-1}$ to keep $c_{t-1}$: Previous cell state, vector, carries long-term memory $i_t$: Input gate, values in (0, 1), decides how much new info to write $z_t$: Cell input / candidate memory, usually $\\tanh(\\cdot)$ output, the new content that could be added Produce the output / hidden state / short-term memory: $$ \\quad h_t = o_t \\psi(c_t) $$\n$h_t$: Hidden state, vector, output of the cell (short-term memory) $o_t$: Output gate, values in (0, 1), controls what part of memory is shown outside $\\psi(c_t)$: Activation function (often $\\tanh(c_t$)), squashes memory to bounded range Three Main Limitations of LSTMs Can‚Äôt revise stored information Once an LSTM stores something in its cell state, it struggles to update or replace it later. xLSTM fix: introduces exponential gating, allowing flexible updating of stored values. Limited storage capacity Traditional LSTMs store information in a single scalar cell state, forcing compression and loss of details. xLSTM fix: uses a matrix memory, which can hold richer, multi-dimensional information. No parallelization LSTM depends on sequential hidden-to-hidden connections, meaning each step waits for the previous one. xLSTM fix: changes the memory mixing structure to make computation parallelizable across time steps. 2 Extended LSTM Two main modifications: exponential gating and novel memory structures. Two variants mombined into xLSTM blocks, stacked with residual connections to build xLSTM architectures, both can have multiple memory cells and heads: sLSTM ‚Äì scalar memory, scalar update, memory mixing across cells. mLSTM ‚Äì matrix memory, covariance (outer product) update, fully parallelizable. 2.2 sLSTM sLSTM = LSTM + exponential gates + normalization state + stabilizer state + multiple memory cells.\nThe exponential gates $i_t$ and $f_t$ make it easier to amplify or reduce memory dynamically.\n‚Üí Helps sLSTM revise stored information better (a key limitation of classical LSTM).\nThe normalizer state $n_t$ keeps things numerically stable, so exponential growth doesn‚Äôt blow up.\nThe stabilizer state $m_t$ keeps their scale controlled, prevents numerical overflow during training.\nThe Multiple heads each with its own LSTM-like structure to compute its own $h_t$, then combined, just like multi-head attention in Transformers, allowing the network to learn different kinds of temporal patterns in parallel.\nNew Memory Mixing: In an sLSTM, each time step has multiple memory cells ‚Üí a vector computed by recurrent matrices R, each cell stores part of the long-term memory, we allow these memory cells to talk to each other.\nMemory mixing = different parts (dimensions) of the memory cells communicate and influence each other.\n2.3 mLSTM mLSTM = LSTM + exponential gates + normalization state + stabilizer state + multiple memory cells.\nmLSTM replaces the small one-number memory $c_t$ of LSTM with a key‚Äìvalue memory matrix, so it can store, search, and update information like attention, but still works as a recurrent network (RNN).\n$q_k, k_t, v_t$ ‚Üí same like query, key, value in transformer‚Ä¶ uses a matrix memory because it wants to store relationships between features (keys and values), not just single values like traditional LSTM. The normalizer state $n_t$ is the weighted sum of key vectors, keeps record of the strength of the gates. Multiple heads and multiple cells are equivalent as there is no memory mixing. 2.4 xLSTM Architecture 2.4.1 xLSTM Blocks Each block takes an input (sequence or features), passes it through an sLSTM or mLSTM cell, adds some non-linear layers (MLPs) and residual/skip connections, finally outputs a transformed sequence representation.\nPatterns are easier to separate after mapping into a higher-dimensional space. Like for better points classification, we can map each point from 2D ‚Üí 3D.\nWhen an xLSTM processes a sequence, it wants to distinguish different histories, for example:\n‚ÄúThe dog chased the cat‚Äù vs ‚ÄúThe cat chased the dog.‚Äù These sequences may look similar in lower dimensions (both use same words), but when we map them into a higher-dimensional representation, the model can more easily tell them apart.\nSo each xLSTM block:\nexpands data into a higher space (‚Äúup-projection‚Äù), applies non-linear transformations, and then compresses back (‚Äúdown-projection‚Äù). That makes it easier for the model to separate different contexts or meanings.\nType Memory type Recurrent connections Parallelization Up-proj position Storage capacity sLSTM Post up-projection Scalar memory (vector) ‚úÖ via matrices R ‚ùå¬†Sequential after LSTM Smaller mLSTM Pre up-projection Matrix memory ‚ùå¬†No recurrent matrices ‚úÖ parallelizable before LSTM Much larger 2.4.2 xLSTM Architecture Figure 1: The extended LSTM (xLSTM) family.\nFrom left to right:\nThe original LSTM memory cell with constant error carousel and gating. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule. mLSTM and sLSTM in residual blocks yield xLSTM blocks. Stacked xLSTM blocks give an xLSTM architecture. The constant error carousel is the additive update of the cell state $c_{t‚àí1}$ (green) by cell inputs $z_t$ and moderated by sigmoid gates (blue).\nThe gating mechanisms:\nForget gate decides what to erase. Input gate decides what to add. Output gate decides what to show. 4 Experiments LSTM and xLSTM models far outperform Transformers and State Space Models on tasks that need long-term memory and state tracking; xLSTM, especially when combining sLSTM + mLSTM, achieves the best all-around performance, showing that recurrent memory architectures still beat attention models for logical and structured reasoning.\nThe paper uses perplexity (ppl) as the main evaluation metric for language modeling. It measures how well the model predicts the next token in a text sequence.\nThe model is confident and accurate ‚Üí the model gives high probability to the correct next word ‚Üí it‚Äôs confident ‚Üí low perplexity. The model is confused and often wrong ‚Üí the model gives low probability ‚Üí it‚Äôs uncertain or wrong ‚Üí high perplexity. Scaling Laws Next token prediction perplexity of xLSTM, RWKV-4, Llama, and Mamba on the SlimPajama validation set when trained on 300B tokens from SlimPajama. Model sizes are 125M, 350M, 760M, and 1.3B. The scaling laws indicate that for larger models xLSTM will perform well too.\n5 Limitations sLSTM not parallelizable:\nIts memory mixing prevents full parallel execution. Custom CUDA version is faster, but still ~2√ó slower than mLSTM. mLSTM kernels not optimized:\nCurrent CUDA implementation is ~4√ó slower than FlashAttention. Could be improved with better GPU kernels. High computation cost:\nmLSTM processes (d \\times d) matrices, which increases compute load,\nthough it can still be parallelized using standard matrix ops.\nGate initialization sensitivity:\nForget-gate parameters must be tuned carefully for stability. Memory limits at long contexts:\nLarge matrix memory may overload at very long sequence lengths,\nbut works fine up to ~16k tokens.\nNot fully optimized yet:\nArchitecture and hyperparameters weren‚Äôt exhaustively tuned due to cost. More optimization could further boost performance. ","permalink":"/posts/xlstm_extended_long_short-term_memory/","summary":"\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2405.04517\"\u003epaper resource\u003c/a\u003e\u003c/p\u003e\n\u003caside\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRWKV\u003c/strong\u003e = a \u003cem\u003ebridge\u003c/em\u003e between LSTM and Transformer (less computation and memory, parallel in training). Replace self-attention with a time-mixing mechanism that behaves like an LSTM in time but a Transformer in training and inference\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003exLSTM\u003c/strong\u003e = s\u003cem\u003ecale up\u003c/em\u003e the LSTM family to match or even surpass Transformer-level performance. Keep the recurrent spirit of LSTM, but redesign gates and memory (mLSTM) so it can train and scale efficiently like Transformers.\n\u003cul\u003e\n\u003cli\u003eSpeciality: memory mixing, \u003cdel\u003eRWKV\u003c/del\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003eThe paper revisits \u003cstrong\u003eLSTMs\u003c/strong\u003e, whose key innovations are the \u003cstrong\u003econstant error carousel\u003c/strong\u003e and \u003cstrong\u003egating mechanisms\u003c/strong\u003e ‚Äî originally solved the vanishing-gradient problem and made long-term memory possible. Although \u003cstrong\u003eTransformers\u003c/strong\u003e later surpassed LSTMs thanks to their \u003cstrong\u003eparallelizable self-attention\u003c/strong\u003e, the authors ask whether LSTMs can be scaled up, like modern LLMs, to \u003cstrong\u003ebillions of parameters\u003c/strong\u003e while overcoming their known limits.\u003c/p\u003e","title":"xLSTM: Extended Long Short-Term Memory"},{"content":"Abstract Transformers are very powerful for language tasks and training is ****faster on GPUs because of parallization, but they use a lot of memory and computing power, especially when processing long text, their cost grows very fast (quadratically).\nRNNs, on the other hand, use less memory and computation cause they grow linearly but are slower to train and not as good at handling long sentences.\nThe new model RWKV mixes the good parts of both, it trains quickly and gets good performance like a Transformer and also runs efficiently like an RNN.\nMemory and computation:\nIn a Transformer, each token looks at every other token through self-attention. If you have N tokens in a sentence, it compares every pair, so total comparisons = N √ó N = N¬≤. That‚Äôs why Memory and computation both grow quadratically. In an RNN, the model reads tokens one by one, passing information step by step. So for N tokens, it just does N steps, the total cost = N (linear). 1 Introduction RWKV reformulates the attention mechanism with a variant of linear attention, replacing traditional dot-product token interaction with more effective channel-directed attention. This implementation, without approximation, offers the lowest computational and memory complexity.\n2 Background 2.1 Recurrent Neural Networks (RNNs) Popular RNN architectures such as LSTM and GRU. Although these RNNs can be factored into two linear blocks (W and U) and an RNN-specific block, the data dependency relying on previous time steps prohibits parallelizing these typical RNNs.\n$$ \\begin{aligned} f_t \u0026= \\sigma_g\\left(W_f x_t + U_f h_{t-1} + b_f\\right), \\\\ i_t \u0026= \\sigma_g\\left(W_i x_t + U_i h_{t-1} + b_i\\right), \\\\ o_t \u0026= \\sigma_g\\left(W_o x_t + U_o h_{t-1} + b_o\\right), \\\\ \\tilde{c}_t \u0026= \\sigma_c\\left(W_c x_t + U_c h_{t-1} + b_c\\right), \\\\ c_t \u0026= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t, \\\\ h_t \u0026= o_t \\odot \\sigma_h(c_t). \\end{aligned} $$ Model Depends on Parallelizable? RNN computed previous state ‚ùå No RWKV raw previous input ‚úÖ Yes (for training) 2.2 Transformers and AFT Standard Transformer self-attention Matrix form\n$$ \\text{Attn}{Attn}(Q,K,V)=\\text{softmax}(QK^\\top)V $$ Per token (t)\n$$ \\text{Attn}{Attn}(Q,K,V)_t=\\frac{\\sum_{i=1}^{T}\\exp(q_t^\\top k_i) \\odot v_i}{\\sum_{i=1}^{T}\\exp(q_t^\\top k_i)} $$ The weighted value equation in multi-head attention. Weight of token $i$ for query $t$ is $\\exp(q_t^\\top k_i)$. Output is a weighted average of $v_i$ AFT (Attention-Free Transformer) variant $$ \\text{Attn}{Attn}^{+}(W,K,V)_t=\\frac{\\sum_{i=1}^{t}\\exp(w_{t,i}+k_i)\\odot v_i}{\\sum_{i=1}^{t}\\exp(w_{t,i}+k_i)} $$ Replace $q_t^\\top k_i$ with (learned) position bias $w_{t,i}$ + a key score $k_i$. Causal: sum only $i\\le t$. Still a normalized weighted average, but weights depend on position via $w_{t,i}$. Variables:\ni: past token index, what we‚Äôre reading from memory t: current token index, what we‚Äôre generating now RWKV‚Äôs simplification Define the bias as a decay with distance:\n$$ w_{t,i}=-(t-i)w \\qquad w\\in(\\mathbb{R}{\\ge 0})^d $$ Per-channel vector $w$ ‚áí older tokens get weight $e^{-(t-i)w}\\le 1$. Now weights depend only on how far back a token is, not on $q_t$. This structure lets us keep running sums, so we don‚Äôt need all pairwise scores. Result:\nSame attention-like effect, but computed with O(T) time and O(1) memory per step (like an RNN), while still trainable in parallel across tokens (like a Transformer) by prefix-scan tricks.\nModel Has Query? How it computes weights Complexity Key idea Transformer ‚úÖ Yes all tokens attend to all O(N¬≤) Full pairwise attention, strong but heavy AFT ‚ùå No Uses position bias O(N) Removes query, uses learned positional weights RWKV ‚ùå No Adds time-decay weights O(N) AFT idea + RNN-style update (linear, efficient) 3 RWKV Four fundamental elements:\nR: The Receptance vector acts as the receiver of past information. W: The Weight signifies the positional weight decay vector, a trainable parameter within the model. K: The Key vector performs a role analogous to K in traditional attention mechanisms. V: The Value vector functions similarly to V in conventional attention processes. The difference between Time Mix and Channel Mix:\nTime Mix: Builds each token‚Äôs vector using time order and previous token info Channel Mix: Refines each token‚Äôs vector by mixing internal dimensions (features). It processes each token separately to mix and refine its feature channels. 3.1 Architecture The RWKV model is composed of stacked residual blocks. Each block consists of a time-mixing and a channel-mixing sub-block, embodying recurrent structures to leverage past information.\nThis model uses a unique attention-like score update process, which includes a time-dependent softmax operation improving numerical stability and mitigating vanishing gradients. It ensures that the gradient is propagated along the most relevant path. Additionally, layer normalization incorporated within the architecture aids in stabilizing the gradients, effectively addressing both vanishing and exploding gradient issues.\n3.1.1 Token Shift In this architecture, all linear projection vectors (R, K, V in time-mixing, and R‚Ä≤, K‚Ä≤ in channel- mixing) involved in computations are produced by linear interpolation between current and previous timestep inputs, facilitating a token shift.\nThe vectors for time-mixing computation are linear projections of linear combinations of the current and previous inputs of the block:\n$$ \\begin{aligned} r_t \u0026= W_r \\cdot \\left(\\mu_r \\odot x_t + (1 - \\mu_r) \\odot x_{t-1}\\right), \\\\ k_t \u0026= W_k \\cdot \\left(\\mu_k \\odot x_t + (1 - \\mu_k) \\odot x_{t-1}\\right), \\\\ v_t \u0026= W_v \\cdot \\left(\\mu_v \\odot x_t + (1 - \\mu_v) \\odot x_{t-1}\\right). \\end{aligned} $$ The channel-mixing inputs:\n$$ \\begin{aligned} r'_t \u0026= W'_{r} \\cdot \\left(\\mu'_{r} \\odot x_t + (1 - \\mu'_{r}) \\odot x_{t-1}\\right), \\\\ k'_t \u0026= W'_{k} \\cdot \\left(\\mu'_{k} \\odot x_t + (1 - \\mu'_{k}) \\odot x_{t-1}\\right). \\end{aligned} $$ 3.1.2 WKV Operator $$ wkv_t = \\frac{\\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} \\odot v_i + e^{u + k_t} \\odot v_t}{\\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} + e^{u + k_t}} $$ The difference of treating W between AFT and RWKV:\nPairwise matrix: Each token pair has its own weight Channel-wise vector: One decay weight per feature channel 3.1.3 Output Gating Time Mixing:\n$$ o_t = W_o \\cdot (\\sigma(r_t) \\odot wkv_t) $$ Channel Mixing:\n$$ o'_t = \\sigma(r'_t) \\odot (W'_v \\cdot \\max(k'_t, 0)^2) $$ 4 Trained Models and Computing Costs Adam optimizer, bfloat16 precision, the auxiliary loss introduced by PaLM‚Ä¶\n4.2 Scaling Laws Scaling laws in language models refer to the mathematical relationships that describe how the performance of a language model changes with respect to various factors.\nScaling laws are important for two primary reasons:\nthey allow us to make predictions and plans regarding the costs and performance of large models before they are trained via interpolation and extrapolation. the contexts in which they fail provides rich feedback on important areas for future research. Explain Interpolation and Extrapolation:\nInterpolation: predicting within the range of data you already know. For example, you trained models with 1B, 5B, and 10B parameters, then you interpolate to guess how a 7B model will perform. Extrapolation: predicting beyond the known range. For example, you trained up to 10B, and now you try to estimate performance of a 100B model. 5 Evaluation Evaluation direction and questions:\nCompetitiveness: Tests if RWKV performs as well as Transformers when both use the same computing power.\nLong Context: Tests if RWKV can handle very long texts better than Transformers,\nespecially when Transformers become too slow or costly for long sequences.\n6 Inference Experiments float32 precision, the HuggingFace Transformers,\n7 Future Work 1. Increase Model Expressivity\nImprove time-decay formulas. Explore better initialization of model states. Goal: more powerful representations without losing efficiency. 2. Improve Computational Efficiency\nUse parallel scan in $wkv_t$¬†step. Target complexity: $O(B \\log(T)d)$. 3. Apply to Encoder‚ÄìDecoder Architectures\nReplace cross-attention with RWKV mechanism. Useful for seq2seq and multimodal models. Boosts efficiency in both training and inference. 4. Use of Model State (Context)\nUsed for interpretability and predictability. Could enhance safety and control via prompt tuning. Modify hidden states to guide model behavior. 5. Larger Internal States\nImprove long-term memory and context understanding. Increase performance across various tasks. 8 Conclusion We introduced RWKV, a new approach to RNN models exploiting the potential of time-based mixing components. RWKV introduces several key strategies that allow it to capture locality and long-range dependencies while addressing limitations of current architectures by: (1) replacing the quadratic QK attention with a scalar formulation at linear cost, (2) reformulating recurrence and sequential inductive biases to enable efficient training parallelization and efficient inference, and (3) enhancing training dynamics using custom initializations.\nWe benchmark the proposed architecture in a wide variety of NLP tasks and show comparable performance to SoTA with reduced cost. Further experiments on expressivity, interpretability, and scaling showcase the model capabilities and draw parallels in behavior between RWKV and other LLMs.\nRWKV opens a new route for scalable and efficient architectures to model complex relationships in sequential data. While many alternatives to Transformers have been proposed with similar claims, ours is the first to back up those claims with pretrained models with tens of billions of parameters.\n9 Limitations 1. Performance Limitation (Memory Loss over Long Contexts)\nLinear attention is efficient but may lose fine details over long sequences. RWKV compresses history into a single vector, unlike Transformers that keep all token interactions. Its recurrent design limits ability to fully ‚Äúlook back‚Äù at distant tokens. 2. Dependence on Prompt Engineering\nRWKV relies more on well-designed prompts than Transformers. Poor prompts may cause information loss between prompt and continuation. ","permalink":"/posts/rwkv/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003eTransformers are very powerful for language tasks and training is ****faster on GPUs because of \u003cstrong\u003eparallization\u003c/strong\u003e, but they use \u003cstrong\u003ea lot of memory and computing power\u003c/strong\u003e, especially when processing long text, their cost grows \u003cstrong\u003every fast\u003c/strong\u003e (quadratically).\u003c/p\u003e\n\u003cp\u003eRNNs, on the other hand, use \u003cstrong\u003eless memory and computation\u003c/strong\u003e cause they grow linearly but are \u003cstrong\u003eslower to train\u003c/strong\u003e and not as good at handling long sentences.\u003c/p\u003e\n\u003cp\u003eThe new model \u003cstrong\u003eRWKV\u003c/strong\u003e mixes the good parts of both, it trains quickly and gets good performance like a Transformer and also runs efficiently like an RNN.\u003c/p\u003e","title":"RWKV: Reinventing RNNs for the Transformer Era"},{"content":"Abstract The game of Go:\nThe most challenging of classic games for AI, because:\nEnormous search space The difficulty of evaluating board positions and moves Concept Meaning Example in Go AI Solution Enormous search space Too many possible moves and future paths ‚Üí impossible to explore all At every turn, Go has ~250 legal moves; across 150 moves ‚Üí (250^{150}) possibilities Policy network narrows down the choices (reduces breadth of search) Hard-to-evaluate positions Even if you know the board, it‚Äôs hard to know who‚Äôs winning Humans can‚Äôt easily assign a numeric score to a mid-game position Value network predicts win probability (reduces depth of search) AlphaGo Imagine AlphaGo is a smart player who has:\nintuition ‚Üí from the policy network judgment ‚Üí from the value network planning ability ‚Üí from MCTS Integrating deep neural networks with Monte Carlo Tree Search (MCTS).\nThe main innovations include:\nTwo Neural Networks: Policy Network: Selects promising moves ‚Üí the probability of each move. Value Network: Evaluates board positions ‚Üí the likelihood of winning. Training Pipeline: Supervised Learning (SL) from expert human games to imitate professional play. Reinforcement Learning (RL) through self-play, improving beyond human strategies. Integration with MCTS: Combines the predictive power of neural networks with efficient search. Reduces: breadth (number of moves to consider) depth (number of steps to simulate) of search. MCTS It first adds all legal moves (children) under the current position in the tree. In every simulation, AlphaGo chooses one branch to go deeper into the tree (not all of them). It decides which one based on three main metrics: Policy Prior (P) ‚Üí the probability of the move from policy network Visit Count (N) ‚Üí how many times we‚Äôve already explored this move during simulations. Q-Value (Q) ‚Üí average win rate from past simulations Symbol Meaning Source Role in decision P(s,a) Policy prior (initial move probability) From policy network Guides initial exploration N(s,a) Number of times this move was explored Counted during simulations Balances exploration vs exploitation Q(s,a) Average predicted win rate (past experience) From value network results of simulations Exploitation: ‚Äúkeep doing what worked‚Äù Results:\nWithout search, AlphaGo already played at the level of strong Go programs. With the neural-network-guided MCTS, AlphaGo achieved a 99.8% win rate against other programs. It became the first program ever to defeat a human professional Go player (Fan Hui, European champion) by 5‚Äì0. Introduction Optimal value function $v^*(s)$ For any game state $s$, there exists a theoretical function that tells who will win if both players play perfectly. Computing this function exactly means searching through all possible sequences of moves. Search space explosion Total possibilities ‚âà $b^d$, where $b$: number of legal moves (breadth), $d$: game length (depth). For Go: ( $b$ ‚âà 250, $d$ ‚âà 150) ‚Üí bigggg number ‚Üí impossible to compute exhaustively. Reducing the search space ‚Äî two key principles: (1) Reduce depth using an approximate value function $v(s)$: Stop (truncate) deep search early. Use an approximate evaluator to predict how good a position is instead of exploring all future moves. This worked in chess, checkers, and Othello, but was believed to be impossible (‚Äúintractable‚Äù) for Go because Go‚Äôs positions are much more complex. (2) Reduce breadth using a policy $p(a|s)$: Instead of exploring all moves, only sample the most likely or promising ones. This narrows down which actions/moves to consider, saving enormous computation. Example: Monte Carlo rollouts: Simulate random games (using the policy) to estimate how good a position is. Maximum depth without branching at all, by sampling long sequences of actions for both players from a policy $p$. This method led to strong results in simpler games (backgammon, Scrabble), and weak amateur level in Go before AlphaGo. ‚ÄúSimulate‚Äù and ‚ÄúRoll out‚Äù basically mean the same thing in this context.\n‚ÄúSimulate‚Äù ‚Üí a general word: to play out an imaginary game in your head or computer. ‚ÄúRoll out‚Äù ‚Üí a more specific term from Monte Carlo methods, meaning ‚Äúplay random moves from the current position until the game ends.‚Äù So ‚Üí every rollout is one simulation of a complete (or partial) game.\nrollout = one simulated playthrough. Monte Carlo Rollout Monte Carlo rollout estimates how good a position is by:\nStarting from a given board position (s). Playing many simulated games to the end (using policy-guided moves ‚Üí reduce breadth). Recording each game‚Äôs result (+1 for win, ‚àí1 for loss). Averaging all outcomes to estimate the win probability for that position. $$ v(s) \\approx \\text{average(win/loss results from rollouts)} $$\nGoal:\nApproximate the value function $v(s)$, the expected chance of winning from position $s$.\nIt‚Äôs simple but inefficient ‚Äî great for small games, too slow and noisy for Go.\nMonte Carlo tree search MCTS uses Monte Carlo rollouts to estimate the value of each state. As more simulations are done, the search tree grows and values become more accurate. It can theoretically reach optimal play, but earlier Go programs used shallow trees and simple, hand-crafted policies or linear value functions (not deep learning). These older methods were limited because Go‚Äôs search space was too large. Training pipeline of AlphaGo Deep convolutional neural networks (CNNs) can represent board positions much better. So AlphaGo uses CNNs to reduce the search complexity in two ways: Evaluating positions with a value network ‚Üí replaces long rollouts (reduces search depth). Sampling moves with a policy network ‚Üí focuses on likely moves (reduces search breadth). Together, this lets AlphaGo explore much more efficiently than traditional MCTS. Supervised Learning (SL) Policy Network $p_\\sigma$: trained from human expert games. Fast Policy Network $p_\\pi$: used to quickly generate moves during rollouts. Reinforcement Learning (RL) Policy Network $p_\\rho$: improves the SL policy through self-play, optimizing for winning instead of just imitating humans. Value Network $v_\\theta$: predicts the winner from any board position based on self-play outcomes. Final AlphaGo system = combines policy + value networks inside MCTS for strong decision-making. Supervised learning of policy networks Panel(a): Better policy-network accuracy in predicting expert moves ‚Üí stronger actual gameplay performance.\nThis proves that imitation learning (supervised policy $p_œÉ$) already provides meaningful playing ability before any reinforcement learning or MCTS.\nFast Rollout Policy networks $p_\\pi(a|s)$\nA simpler and faster version of the policy network used during rollouts in MCTS. Uses a linear softmax model on small board-pattern features (not deep CNN). Much lower accuracy (24.2 %) but extremely fast takes only 2 ¬µs per move (vs. 3 ms for the full SL policy). Trained with the same supervised learning principle on human moves. Reinforcement learning of policy networks Structure of the policy network = SL policy network initial weights œÅ = œÉ Step What happens What‚Äôs learned Initialize Copy weights from SL policy (œÅ = œÉ) Start with human-like play Self-play Pick current p and an older version p Generate thousands of full games (self-play) Reward +1 for win, ‚àí1 for loss Label each move sequence, and collect experience (state, action, final reward) Update Update weights œÅ by SGD Policy network Repeat Thousands of games Stronger, self-improving policy Reinforcement learning of value networks Step What happens What‚Äôs learned Initialize Start from the trained RL policy network; use it to generate self-play games Provides realistic, high-level gameplay data Self-play RL policy network plays millions of games against itself Produce diverse board positions and their final outcomes (+1 win / ‚àí1 loss) Sampling Randomly select one position per game to form 30 M independent (state, outcome) pairs Avoids correlation between similar positions Labeling Each position (s) labeled with the final game result (z) Links every board state to its real win/loss outcome Training Train the value network (v_Œ∏(s)) by minimizing MSE Learns to predict winning probability directly from a position Evaluation Compare against Monte Carlo rollouts (pœÄ, pœÅ) Matches rollout accuracy with 15 000√ó less computation Result MSE ‚âà 0.23 (train/test), strong generalization Reliable position evaluation for use in MCTS Problem of naive approach of predicting game outcomes from data consisting of complete games:\nThe value network was first trained on all positions from the same human games. Consecutive positions were almost identical and had the same win/loss label. The network memorized whole games instead of learning real position evaluation ‚Üí overfitting (MSE = 0.37 test). Solution\nGenerate a new dataset: 30 million self-play games, take only one random position per game. Each sample is independent, so the network must learn general Go patterns, not memorize. Result: good generalization (MSE ‚âà 0.23) and accurate position evaluation. Searching with policy and value networks (MCTS) Panel Step What happens Which network helps a Selection Traverse the tree from root to a leaf by selecting the move with the highest combined score Q + u(P). Uses Q-values (average win) and policy priors P (from policy network). b Expansion When reaching a leaf (a position never explored before), expand it: generate its possible moves and initialize their prior probabilities using the policy network RL policy network c Evaluation Evaluate this new position in two ways: ‚ë† Value network (v_Œ∏(s)): predicts win probability instantly. ‚ë° Rollout with fast policy p_œÄ: quickly play random moves to the end, get final result (r). Value net + Fast policy d Backup Send the evaluation result (average of (v_Œ∏(s)) and (r)) back up the tree ‚Äî update each parent node‚Äôs Q-value (mean of all results from that branch). None directly (update step) The core idea Each possible move/edge (s, a) in the MCTS tree stores 3 key values:\nSymbol Meaning Source P(s,a) Prior probability ‚Äî how promising this move looks before searching From the policy network N(s,a) How many times this move has been tried From search statistics Q(s,a) Average win rate from playing move a at state s From past simulations Step 1: Selection ‚Äî choose which move to explore next At each step of a simulation, AlphaGo selects the move $a_t$ that maximizes:\n$$ a_t = \\arg\\max_a [Q(s_t, a) + u(s_t, a)] $$\nwhere the bonus term $u(s,a)$ encourages exploration:\n$$ u(s,a) \\propto \\frac{P(s,a)}{1 + N(s,a)} $$\n$Q(s,a)$: ‚ÄúHow good this move has proven so far.‚Äù $u(s,a)$: ‚ÄúHow much we should still explore this move.‚Äù ‚Üí Moves that are both good (high Q) and underexplored (low N) get priority.\nAs N increases, the bonus term shrinks ‚Äî the search gradually focuses on the best moves.\nStep 2: Expansion When the search reaches a leaf (a position not yet in the tree):\nThe policy network $p_\\sigma(a|s)$ outputs a probability for each legal move. Those values are stored as new P(s,a) priors for the new node. Initially $N(s,a) = 0$ $Q(s,a) = 0$ Now the tree has grown ‚Äî this new node represents a new possible future board.\nStep 3: Evaluation ‚Äî estimate how good the leaf is Each leaf position $s_L$ is evaluated in two ways:\nValue network $v_Œ∏(s_L)$: directly predicts win probability. Rollout result $z_L$: fast simulation (using the fast rollout policy $p_œÄ$) until the game ends +1 if win ‚àí1 if loss. Then AlphaGo combines the two results:\n$$ V(s_L) = (1 - Œª)v_Œ∏(s_L) + Œªz_L $$\n$Œª$ = mixing parameter (balances between value net and rollout). If $Œª$ = 0.5, both count equally. Step 4: Backup ‚Äî update the tree statistics The leaf‚Äôs evaluation $V(s_L)$ is propagated back up the tree:\nEvery move (edge) $(s, a)$ that was used to reach that leaf gets updated:\n$$ N(s,a) = \\sum_{i=1}^{n} 1(s,a,i) $$\n$$ Q(s,a) = \\frac{1}{N(s,a)} \\sum_{i=1}^{n} 1(s,a,i) V(s_L^i) $$\n$1(s,a,i)$ = 1 if that move was part of the i-th simulation, else 0. $V(s_L^i)$ = evaluation result from that simulation‚Äôs leaf. So, Q(s,a) becomes the average value of all evaluations ( $r$ and $vŒ∏$) in its subtree.\nStep 5: Final move decision After thousands of simulations, the root node has a set of moves with:\n$P(s_0, a)$: from policy network, $Q(s_0, a)$: average win rate, $N(s_0, a)$: visit counts. AlphaGo chooses the move with the highest visit count (N) ‚Äî the most explored and trusted move.\nWhy SL policy network performed better than RL policy network for MCTS?\nPolicy Behavior Effect in MCTS SL policy Mimics human experts ‚Üí gives a diverse set of good moves MCTS can explore several promising branches efficiently RL policy Optimized for winning ‚Üí focuses too narrowly on top 1‚Äì2 moves MCTS loses diversity ‚Üí gets less exploration benefit So, for MCTS‚Äôs exploration stage, a broader prior (SL policy) performs better.\nBut for value estimation, the RL value network is superior ‚Äî because it predicts winning chances more accurately.\nImplementation detail Evaluating policy \u0026amp; value networks takes much more compute than classical search. AlphaGo used: 40 search threads, 48 CPUs, 8 GPUs for parallel evaluation. The final system ran asynchronous multi-threaded search: CPUs handle the tree search logic, GPUs compute policy and value network evaluations in parallel. This allowed AlphaGo to efficiently combine deep learning with massive search.\nAll programs were allowed 5 s of computation time per move.\nDiscussion In this work we have developed a Go program, based on a combination of deep neural networks and tree search. We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.\nSelect those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network. Policy network ‚Üí ‚Äúprobability of choosing a move‚Äù\nValue network ‚Üí ‚Äúprobability of winning from a position‚Äù\n","permalink":"/posts/mastering-go-mcts/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eThe game of Go:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe most challenging of classic games for AI, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnormous search space\u003c/li\u003e\n\u003cli\u003eThe difficulty of evaluating board positions and moves\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eConcept\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n          \u003cth\u003eExample in Go\u003c/th\u003e\n          \u003cth\u003eAI Solution\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eEnormous search space\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eToo many possible moves and future paths ‚Üí impossible to explore all\u003c/td\u003e\n          \u003ctd\u003eAt every turn, Go has ~250 legal moves; across 150 moves ‚Üí (250^{150}) possibilities\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003ePolicy network\u003c/strong\u003e narrows down the choices (reduces \u003cem\u003ebreadth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eHard-to-evaluate positions\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eEven if you know the board, it‚Äôs hard to know who‚Äôs winning\u003c/td\u003e\n          \u003ctd\u003eHumans can‚Äôt easily assign a numeric score to a mid-game position\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eValue network\u003c/strong\u003e predicts win probability (reduces \u003cem\u003edepth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"alphago\"\u003e\u003cstrong\u003eAlphaGo\u003c/strong\u003e\u003c/h2\u003e\n\u003caside\u003e\n\u003cp\u003eImagine AlphaGo is a \u003cem\u003esmart player\u003c/em\u003e who has:\u003c/p\u003e","title":"Mastering the game of Go with MCTS and Deep Neural Networks"},{"content":" Chain of thought:\nA series of intermediate natural language reasoning steps that lead to the final output ‚Äî significantly improves the ability of large language models to perform complex reasoning. Chain-of-thought prompting:\nA simple and broadly applicable method for enhancing reasoning in language models. Improving performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Two simple methods to unlock the reasoning ability of LLMs Thinking in steps helps:\nWhen the model explains each step before the answer, it understands the problem better.\nLearning from examples:\nWhen we show a few examples with step-by-step answers, the model learns to do the same.\nFew-shot prompting means giving the model a few examples in the prompt to show it how to do a task before asking it to solve a new one.\nWhat this paper do Combine these two ideas help the language models think step by step to generate a clear and logical chain of ideas that shows how they reach the final answer. Given a prompt that consists of triples: \u0026lt;input, chain of thought, output\u0026gt; Why this method is important It doesn‚Äôt need a big training dataset. One model can do many different tasks without extra training. Greedy decoding: let the model choose the most likely next word each time\nResult It only works well for giant models, not smaller ones. It only works well for more-complicated problems, not the simple ones. Chain-of-thought prompting with big models gives results as good as or better than older methods that needed finetune for each task. Ablation study: It‚Äôs like testing which parts of your model really matter.\nRelated work Some methods make the input part of the prompt better ‚Äî for example, adding clearer instructions before the question. But this paper does something different (orthogonal): it improves the output part, by making the model generate reasoning steps (chain of thought) before giving the final answer. ","permalink":"/posts/chain-of-thought-prompting/","summary":"\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eChain of thought:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA series of intermediate \u003cstrong\u003enatural language reasoning steps\u003c/strong\u003e that lead to the final output ‚Äî significantly improves the ability of large language models to perform complex reasoning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eChain-of-thought prompting:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple and broadly applicable method for\n\u003cul\u003e\n\u003cli\u003eenhancing reasoning in language models.\u003c/li\u003e\n\u003cli\u003eImproving performance on a range of \u003cstrong\u003earithmetic\u003c/strong\u003e, \u003cstrong\u003ecommonsense\u003c/strong\u003e, and \u003cstrong\u003esymbolic\u003c/strong\u003e\nreasoning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003chr\u003e\n\u003ch2 id=\"two-simple-methods-to-unlock-the-reasoning-ability-of-llms\"\u003eTwo simple methods to unlock the reasoning ability of LLMs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eThinking in steps helps:\u003c/strong\u003e\u003c/p\u003e","title":"Chain-of-Thought Prompting"},{"content":"Abstract What‚Äôs the Reference-based Super-resolution (RefSR) Network:\nSuper-resolves a low-resolution (LR) image given an external high-resolution (HR) reference image The reference image and LR image share similar viewpoint but with significant resolution gap (8√ó). Solve the problem Existing RefSR methods work in a cascaded way such as patch matching followed by synthesis pipeline with two independently defined objective functions Divide the image into many small patches (like tiny squares), each patch is compared with a reference image to find its most similar region. But every patch makes its decision independently. ‚Üí Inter-patch misalignment Because of the small misalignments, the grid of patch boundaries in the final image shows. ‚Üí Grid artifacts Old methods trained two steps separately ‚Üí Inefficient training The challenge large-scale (8√ó) super-resolution problem the spatial resolution is increased by 8 times in each dimension (width and height). So the total number of pixels increases from 8√ó8 to 64√ó64. patch matching ‚Üí warping Structure:\nimage encoders extract multi-scale features from both the LR and the reference images cross-scale warping layers spatially aligns the reference feature map with the LR feature map warping module originated from spatial transformer network (STN) fusion decoder aggregates feature maps from both domains to synthesize the HR output Scale Resolution (relative) Example size What it focuses on Scale 0 √ó1 (full resolution) 512√ó512 Fine details (small shifts) Scale 1 √ó2 smaller 256√ó256 Medium motions Scale 2 √ó4 smaller 128√ó128 Larger motions Scale 3 √ó8 smaller 64√ó64 Very large motions Scale 4‚Äì5 √ó16, √ó32 smaller 32√ó32, 16√ó16 Extremely coarse view (too little detail) Result Using cross-scale warping, our network is able to perform spatial alignment at pixel-level in an end-to-end fashion, which improves the existing schemes both in precision (around 2dB-4dB) and efficiency (more than 100 times faster).\nspatial alignment at pixel-level ‚Üí precision and efficiency precision efficiency 1. Introduction The two critical issues in RefSR:\nImage correspondence between the two input images High resolution synthesis of the LR image. The ‚Äòpatch maching + synthesis‚Äô pipeline ‚Üí the end-to-end CrossNet ‚Üí results comparisons.\nüí° Flow estimator Input: feature maps from LR and Ref encoders.\nComputation:\nThe module compares these features (using convolutions) and learns to predict displacement vectors (optical flow).\nOutput: a flow map $F(x, y) = (\\Delta x, \\Delta y)$.\nUse: the warping layer applies this flow map to the reference feature map:\n$$ \\tilde{R}(x, y) = R(x + \\Delta x, y + \\Delta y) $$\nso the warped reference aligns with the LR image.\nNon-rigid deformation: when an object changes its shape or structure in the image (for example, bending, twisting, or changing due to different camera angles).\nRigid = only simple shifts, rotation, or scaling. Non-rigid = more complex distortions ‚Äî like bending, stretching, or perspective change. Grid artifacts: visible blocky or checker-like patterns that appear because the image was reconstructed from many small, rigid square patches that don‚Äôt align smoothly.\nGrid artifacts occur when an image is reconstructed from many small square patches that don‚Äôt align perfectly at their borders. The Laplacian is a mathematical operator that measures how much a pixel value differs from its surroundings.\nIn other words, it tells you where the image changes quickly ‚Äî that‚Äôs usually at edges or texture details.\n2. Related Work Multi-scale deep super resolution we employ MDSR as a sub-module for LR images feature extraction and RefSR synthesis.\nMDSR stands for Multi-scale Deep Super-Resolution Network Used for Feature extraction ‚Üí understanding what‚Äôs in the LR image RefSR synthesis ‚Üí combining LR and reference features to output the high-resolution result Warping and synthesis We follow such ‚Äúwarping and synthesis‚Äù pipeline. However, our approach is different from existing works in the following ways:\nOur approach performs multi-scale warping on feature domain at pixel-scale which accelerates the model convergence by allowing flow to be globally updated at higher scales. a novel fusion scheme is proposed for image synthesis. concatenation, linearly combining images 3. Approach 3.1 Fully Conv Cross-scale Alignment Module It is necessary to perform spatial alignment for reference image, since it is captured at different view points from LR image.\nCross-scale warping We propose cross-scale warping to perform non-rigid image transformation.\nOur proposed cross-scale warping operation considers a pixel-wise shift vector ( V ):\n$$ I_o = warp(y_{Ref}, V) $$\nwhich assigns a specific shift vector for each pixel location, so that it avoids the blocky and blurry artifacts.\nüí° Pixel-wise shift vector (V) ‚Üí patch matching\n( V ) represents a flow field, where each pixel gets its own small movement vector (Œîx, Œîy). A flow field is a map (like a vector field) that assigns a motion vector to every pixel in the image. So instead of moving the entire image or patch, CrossNet can move each pixel individually ‚Äî very flexible. The equation:\n$$ I_o = warp(y_{Ref}, V) $$\nmeans:\nThe output image ( $I_o$ ) is generated by warping the reference image ( $y_{Ref}$ ) according to the flow field ( $V$ ).\nEach pixel in ( $y_{Ref}$ ) is shifted by its corresponding vector in ( $V$ ).\nCross-scale flow estimator Purpose: Predict pixel-wise flow fields to align the upsampled LR image with the HR reference.\nModel: Based on FlowNetS, adapted for multi-scale correspondence.\nInputs:\n$I_{LR‚Üë}$ : LR image upsampled by MDSR (SISR) $I_{REF}$ : reference image Outputs: Multi-scale flow fields\n${V^{(3)}, V^{(2)}, V^{(1)}, V^{(0)}}$ (coarse ‚Üí fine).\nModification: √ó4 bilinear upsampling with ‚Üí two √ó2 upsampling modules + skip connections + deconvolution ‚Üí finer, smoother flow prediction.\nAdvantage:\nCaptures both large and small displacements; Enables accurate, non-rigid alignment Reduces warping artifacts. üí° How it works (coarse-to-fine refinement)\nThe coarse flow field (V¬≥) roughly aligns big structures. The next flow (V¬≤) refines alignment for medium details. The fine flows (V¬π, V‚Å∞) correct small local misalignments and textures. These flow fields are combined hierarchically ‚Äî like zooming in step-by-step to improve precision. 3.2 End-to-end Network Structure Network structure of CrossNet\nüí° Network:\na LR image encoder a reference image encoder a decoder ‚Üí U-Net LR image Encoder Goal: Extract multi-scale feature maps from the low-resolution (LR) image for alignment and fusion.\nStructure:\nUses a Single-Image SR (SISR) upsampling to enlarge the LR image first. Then applies 4 convolutional layers (5√ó5 filters, 64 channels). Each layer creates a feature map at a different scale (0 ‚Üí 3). Stride = 1 for the first layer, stride = 2 for deeper ones (downsampling by 2). Output:\nA set of multi-scale LR feature maps. $$ F_{LR}^{(0)}, F_{LR}^{(1)}, F_{LR}^{(2)}, F_{LR}^{(3)} $$\nActivation: ReLU (œÉ).\nReference image encoder Goal: Extract and align multi-scale reference features from the HR reference image.\nStructure:\nUses the same 4-scale encoder design as the LR encoder. Produces feature maps . $$ {F_{REF}^{(0)}, F_{REF}^{(1)}, F_{REF}^{(2)}, F_{REF}^{(3)}}¬†$$\nLR and reference encoders have different weights, allowing complementary feature learning. Alignment:\nEach reference feature map $F_{REF}^{(i)}$ is warped using the cross-scale flow $V^{(i)}$. This generates spatially aligned reference features $$ \\hat{F}{REF}^{(i)} = warp(F_{REF}^{(i)}, V^{(i)}) $$\nDecoder Goal: Fuse the low-resolution (LR) features and warped reference (Ref) features across multiple scales to reconstruct the super-resolved (SR) image. Structure Overview The decoder follows a U-Net‚Äìlike architecture. It performs multi-scale fusion and up-sampling using deconvolution layers. Each scale combines: The LR feature at that scale $F_{LR}^{(i)}$, The warped reference feature $\\hat{F}_{REF}^{(i)}$, The decoder feature from the next coarser scale $F_{D}^{(i+1)}$ (if available). üí° Equations (Eq. 6)\nFor the coarsest scale (i = 3):\n$$ F_{D}^{(3)} = \\sigma \\big( W_{D}^{(3)} \\star (F_{LR}^{(3)}, \\hat{F}{REF}^{(3)}) + b{D}^{(3)} \\big) $$\nFor finer scales (i = 2, 1, 0):\n$$ F_{D}^{(i)} = \\sigma \\big( W_{D}^{(i)} \\star (F_{LR}^{(i+1)}, \\hat{F}{REF}^{(i+1)}, F{D}^{(i+1)}) + b_{D}^{(i)} \\big) $$\nwhere:\n$\\star$ denotes the deconvolution operation (transposed convolution). $W_{D}^{(i)}$: deconvolution filters (size 4√ó4, 64 filters, stride 2). $\\sigma$: activation function (ReLU). $b_{D}^{(i)}$ : bias term. Thus, features are progressively upsampled and refined from coarse ‚Üí fine.\nüí° Post-Fusion (Eq. 7)\nAfter obtaining the final decoder feature map $F_{D}^{(0)}$,\nthree convolutional layers (filter size 5√ó5) are applied to refine and generate the SR image:\n$$ \\begin{aligned} F_1 \u0026amp;= \\sigma(W_1 * F_{D}^{(0)} + b_1), \\ F_2 \u0026amp;= \\sigma(W_2 * F_1 + b_2), \\ I_p \u0026amp;= \\sigma(W_p * F_2 + b_p), \\end{aligned} $$\nwhere:\n$W_1, W_2, W_p$: convolution filters with channel numbers {64, 64, 3}, $I_p$: final super-resolved output image. 3.3 Loss Function Goal: Train CrossNet to generate super-resolved (SR) outputs $I_p$ close to the ground-truth HR images $I_{HR}$.\nFormula:\n$$ L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{s} \\rho(I_{HR}^{(i)}(s) - I_{p}^{(i)}(s)) $$\nPenalty: Uses the Charbonnier loss\n$$ \\rho(x) = \\sqrt{x^2 + 0.001^2} $$\nA smooth, robust version of L1 loss that reduces the effect of outliers.\nVariables:\n$N$: number of training samples $s$: pixel (spatial location) $i$: training sample index 4. Experiment 4.1 Dataset Dataset: The representative Flower dataset and Light Field Video (LFVideo) dataset. Each light field image has: 376 √ó 541 spatial samples 14 √ó 14 angular samples Model training: Each light field image has: 320 √ó 512 spatial samples 8 √ó 8 angular samples Test generalization: Datasets: Stanford Light Field dataset Scene Light Field dataset During testing, they apply the big input images using a sliding window approach: Window size: 512√ó512 Stride: 256 4.2 Evaluation Training setup: Trained for 200K iterations on Flower and LFVideo datasets. Scale factors: √ó4 and √ó8 super-resolution. Learning rate: 1e-4 / 7e-5 ‚Üí decayed to 1e-5 / 7e-6 after 150K iterations. Optimizer: Adam (Œ≤‚ÇÅ = 0.9, Œ≤‚ÇÇ = 0.999). Comparisons: Competes with RefSR methods (SS-Net, PatchMatch) and SISR methods (SRCNN, VDSR, MDSR). Evaluation metrics: PSNR, SSIM, and IFC on √ó4 and √ó8 scales. Reference images from position (0,0); LR images from (1,1) and (7,7). Results: CrossNet achieves 2‚Äì4 dB PSNR gain over previous methods. CrossNet consistently outperforms the resting approaches under different disparities, datasets and scales. Quantitative evaluation of the sota SISR and RefSR algorithms, in terms of PSNR/SSIM/IFC for scale factors √ó4 and √ó8 respectively.\nMetric Meaning PSNR (Peak Signal-to-Noise Ratio) Measures reconstruction accuracy (higher = clearer, less error). SSIM (Structural Similarity Index) Measures structural similarity to the ground truth (higher = more visually similar). IFC (Information Fidelity Criterion) Evaluates how much visual information is preserved (higher = better detail). Generalization During training, apply a parallax augmentation procedure this means they randomly shift the reference image by ‚Äì15 to +15 pixels both horizontally and vertically. The purpose is to simulate different viewpoint disparities (parallax changes) make the model more robust to viewpoint variations. They initialize the model using parameters pre-trained on the LFVideo dataset, Then re-train on the Flower dataset for 200 K iterations to improve generalization. The initial learning rate is 7 √ó 10‚Åª‚Åµ, which decays by factors 0.5, 0.2, 0.1 at 50 K, 100 K, 150 K iterations. Table 2 and 3 show PSNR comparison results: Their re-trained model (CrossNet) outperforms PatchMatch [11] and SS-Net [2] on both Stanford and Scene Light Field datasets. The improvement is roughly +1.79 ‚Äì 2.50 dB (Stanford) and +2.84 dB (Scene LF dataset). Efficiency within 1 seconds machine: 8 Intel Xeon CPU (3.4 GHz) a GeForce GTX 1080 GPU 4.3 Discussion Flows at scale 0‚Äì3 were coherent (good). Flows at scale 4‚Äì5 were too noisy ‚Äî because those very small maps (like 32√ó32) lost too much information. Training setup:\nTrain both CrossNet and CrossNet-iw with the same procedure: Pre-train on LFVideo dataset Fine-tune on Flower dataset 200 K iterations total Additionally, CrossNet-iw pretraining: Pre-train only the flow estimator WS-SRNet using an image warping task for 100 K iterations. Train the whole network jointly for another 100 K iterations. FlowNetS+ adds extra upsampling layers\nIn the original FlowNetS: The final flow map is smaller than the input (maybe ¬º or ¬Ω resolution). This is fine for rough alignment but loses small motion details. In FlowNetS+: They add extra upsampling layers so the final flow map is finer (closer to full size). That‚Äôs why it aligns better ‚Äî it can describe tiny pixel movements more accurately. Downsampling = make smaller. Upsampling = make bigger.\n","permalink":"/posts/crossnet_an_end-to-end_reference-based_super_resol/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003eWhat‚Äôs the \u003cstrong\u003eReference-based Super-resolution (RefSR)\u003c/strong\u003e Network:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSuper-resolves a \u003cstrong\u003elow-resolution (LR)\u003c/strong\u003e image given an external \u003cstrong\u003ehigh-resolution (HR) reference image\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe reference image and LR image share similar viewpoint but with significant resolution gap (8√ó).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"solve-the-problem\"\u003eSolve the problem\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eExisting RefSR methods work in a cascaded way such as \u003cstrong\u003epatch matching\u003c/strong\u003e followed by \u003cstrong\u003esynthesis pipeline\u003c/strong\u003e with two independently defined objective functions\n\u003cul\u003e\n\u003cli\u003eDivide the image into many small \u003cstrong\u003epatches\u003c/strong\u003e (like tiny squares), each patch is compared with a \u003cstrong\u003ereference image\u003c/strong\u003e to find its most similar region.\u003c/li\u003e\n\u003cli\u003eBut every patch makes its decision independently. ‚Üí \u003cstrong\u003eInter-patch misalignment\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eBecause of the small misalignments, the \u003cstrong\u003egrid\u003c/strong\u003e of patch boundaries in the final image shows. ‚Üí \u003cstrong\u003eGrid artifacts\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eOld methods trained \u003cstrong\u003etwo steps separately ‚Üí Inefficient training\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThe challenge large-scale (8√ó) super-resolution problem\n\u003cul\u003e\n\u003cli\u003ethe \u003cstrong\u003espatial resolution is increased by 8 times\u003c/strong\u003e in each dimension (width and height).\u003c/li\u003e\n\u003cli\u003eSo the total number of pixels increases from 8√ó8 to 64√ó64.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003epatch matching ‚Üí warping\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStructure:\u003c/p\u003e","title":"CrossNet: An End-to-end Reference-based Super Resol"},{"content":"Video Source\nPage Source\nPratical Source\n1. Introduction Extension of Abstract.\nSequence Modeling: Recurrent language modelsExtension of Abstract.\nOperate step-by-step, processing one token or time step at a time. one input + hidden state ‚Üí one output + hidden state (update every time) e.g., RNNs, LSTMs Encoder-decoder architectures\nComposed of two RNNs (or other models): one to encode input into a representation another to decode it into the output sequence one input + hidden state ‚Üí ‚Ä¶ ‚Üí hidden state ‚Üí one output + hidden state ‚Üí ‚Ä¶ e.g., seq2seq Disadvantages about these two: Recurrent models preclude parallelization, so it‚Äôs not good.\nEncoder-decoder models have used attention to pass the stuff from encoder to decoder more effectively.\nAttention doesn‚Äôt use recurrence and entirely relies on an attention mechanism to draw glabal dependencies between input and output. 2. Background Corresponding Work:\nSpeak clearly about the corresponding papers, the connections between u and the papers, and the differences.\nReduce sequential computation: Typically use convolutional neural networks ‚Üí the number of operations grows in the distance between positions ‚Üí transfomer: a constant number of operations\nat the cost of reduced effective resolution due to averaging attention-weighted positions we counteract this bad effect with Multi-Head Attention Self-attention: compute a representation of the sequence based on different positions\nEnd-to-end memory networks: ‚úî Reccurrence attention mechanism\n‚ùå Sequence-aligned recurrence\nTransformer: The first transduction model relying entirely on self-attention to do encoder-decoder model. ( to compute representations of its input and output without using sequence-aligned RNNs or convolution)\ntransduction model refers to making predictions or inferences about specific instances based on the data at hand, without relying on a prior generalization across all possible examples, which is different from Inductive learning.\n3. Model Architecture The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and the decoder.\nEncoder: maps an input sequence of symbol representations x ‚Üí a sequence of continuous representations z. Decoder: given z, generates an output sequence of symbols one element at a time. At each step, the model is auto-regressive. Why Using LayerNorm not BatchNorm? LayerNorm normalizes across features of a single sample, suitable for variable-length sequences.\nBatchNorm normalizes across the batch, which can be inconsistent for sequence tasks.\n3.1 Encoder and Decoder Stacks Encoder N = 6 identical layers, each layer has two sub-layers.\nmulti-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\ndimension of output = 512\nDecoder N = 6 identical layers, each layer has three sub-layers.\nmasked multi-head attention over the output of the encoder stack multi-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\n3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output where the query, keys, values, and output are all vectors.\nThe output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n( Given a query, output is computed by similarity between query and keys, then different keys have different weights, next combine the values based on the weights. )\n3.2.1 Scaled Dot-Product Attention We compute the dot products of the query with all keys, divide each by length of query ‚àö, and apply a softmax function to obtain the weights on the values.\nWhy divide each by length of query ‚àö ?\nIt prevents the input to the softmax from becoming excessively large, ensuring that the softmax outputs remain well-distributed and numerically stable. The softmax function normalize the scores into probabilities, but it doesn\u0026rsquo;t address the problem of large input magnitudes directly. If the inputs to softmax are extremely large, the e^x can become numerically unstable, leading to issues in computation.\nn refers to the length of a sequence, the number of the words.\ndk refers to the length of the one word vector.\nm refers to the number of the target words.\ndv refers to the length of the one target word vector.\nWhat there is a function of the Mask?\nWhen computing the output, I only use the key-value pairs up to the current time and do not use any later key-value pairs.\n3.2.2 Multi-Head Attention Linear projection, just like lots of channels.\nLinearly project the queries, keys and values h times to low dimensions with different, learned linear projections.\nFinally, these are concatenated (stack) and once again projected.\nWe emply h = 8 parallel attention layers, or heads.\nSo for each layer or head, we make their dimension to 64 to make the total computational cost is similar to single-head attention.\n3.2.3 Application of Attention in our Model The Transformer uses multi-head attention in three different ways:\nIn ‚Äúencoder-decoder attention‚Äù layers, queries ‚Üí previous decoder layer keys and values ‚Üí the output of the encoder In ‚Äúencoder self-attention‚Äù layers, keys, values and queries all come from the previous layer in the encoder In ‚Äúdecoder self-attention‚Äù layers, keys, values and queries all come from the previous layer in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position 3.3 Position-wise Feed-Forward Networks The fully connected feed-forward network is applied to each position separately and identically.\nThe Multi-Head Attention has already get the location information, now we need to add more expression ability by adding non-linear.\nThe output and MLP patterns are the same. The difference are the input source.\n3.4 Embedding and Softmax Embedding convert the input tokens to vectors of dimension ( 512 ), then multiply those weights by\n$\\sqrt {d_{models}}$\nWhy multiply those weights by $\\sqrt {d_{models}}$ ?\nTo boosts the magnitude, making it more aligned with other components of the model. Because the initial value is between 0~1 by using normal distribution.\nWe update the L2 norm of the embeddings to 1 finally, so we need to ensure the value of each dimension not too small. The L2 norm value of a vector to 1 is best to express a word, because it only express direction.\nSoftmax convert the decoder output to predicted next-token probabilities.\n3.5 Positional Encoding The order change, but the values don‚Äôt change. So we add the sequential information to the input.\nWe use sine and cosine functions of different frequencies [-1, 1]\n4. Why Self-Attention Length of a word ‚Üí d Number of words ‚Üí n Self-Attention ‚Üí n words ‚úñÔ∏è every words need to multiply with n words and for each two words do d multiplying. Recurrent ‚Üí d-dimension vector multiply d‚úñÔ∏èd matrix, n times Convolutional ‚Üí k kernel_size, n words, d^2 input_channels ‚úñÔ∏è output_channels (Draw picture clear) Self-Attention (restricted) ‚Üí r the number of neighbors It seems like Self-Attention architecture has lots of advantages, but it needs more data and bigger model to train to achieve the same effect.\n5. Training 5.1 Training Data and Batching Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. ‚Üí So we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation.\n5.2 Hardware and Schedule 5.3 Optimizer 5.4 Regularization Residual Dropout Label Smoothing 6. Results $N:$ number of blocks\n$d_{model}:$ the length of a token vector\n$d_{ff}:$ Feed-Forward Full-Connected Layer Intermediate layer output size\n$h:$ the number of heads\n$d_k:$ the dimension of keys in a head\n$d_v :$ the dimension of values in a head\n$P_{drop}:$ dropout rate\n$\\epsilon_{ls}:$ Label Smoothing value, the learned label value\n$train steps:$ the number of batchs\n","permalink":"/posts/attention_is_all_you_need/","summary":"\u003cp\u003e\u003ca href=\"https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.1387.top_right_bar_window_history.content.click\u0026amp;vd_source=83d7a54445de4810b52e58e4864b4605\"\u003eVideo Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1706.03762\"\u003ePage Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=kCc8FmEb1nY\"\u003ePratical Source\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"1-introduction\"\u003e1. Introduction\u003c/h1\u003e\n\u003caside\u003e\n\u003cp\u003eExtension of Abstract.\u003c/p\u003e\n\u003c/aside\u003e\n\u003ch2 id=\"sequence-modeling\"\u003eSequence Modeling:\u003c/h2\u003e\n\u003cp\u003eRecurrent language modelsExtension of Abstract.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOperate step-by-step, processing one token or time step at a time.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state ‚Üí one output + hidden state (update every time)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., RNNs, LSTMs\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEncoder-decoder architectures\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eComposed of two RNNs (or other models):\n\u003cul\u003e\n\u003cli\u003eone to encode input into a representation\u003c/li\u003e\n\u003cli\u003eanother to decode it into the output sequence\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state ‚Üí ‚Ä¶ ‚Üí hidden state ‚Üí one output + hidden state ‚Üí ‚Ä¶\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., seq2seq\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"disadvantages-about-these-two\"\u003eDisadvantages about these two:\u003c/h2\u003e\n\u003cp\u003eRecurrent models preclude parallelization, so it‚Äôs not good.\u003c/p\u003e","title":"Attention is All You Need"}]