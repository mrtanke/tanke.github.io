[{"content":" CrossNet and CrossNet++ Both are for Reference-Based Super-Resolution (RefSR), using a low-resolution (LR) image and a high-resolution (HR reference) image to make a sharper, high-quality output.\nThe performance of CrossNet drops with the increasing of perspective parallax, the improvement of CrossNet++:\nTwo-stage warping ‚Üí improves alignment Self-supervised flow estimation ‚Üí uses FlowNet to estimate motion between LR and Ref images Cross-scale alignment ‚Üí Aligns features at multiple resolutions Hybrid loss functions ‚Üí warping + landmark + super-resolution loss Real-world performance ‚Üí produces smoother, sharper, and more realistic results, suitable for a variety of scenarios Abstraction CrossNet++ focuses on reference-based super-resolution (RefSR), improving a low-resolution image using a high-resolution reference from another camera. This task is hard because of large scale differences (8√ó) and big parallax (~10%) between the two views.\nTo solve this, CrossNet++ introduces an end-to-end two-stage network with:\nCross-scale warping modules, align images at multiple zoom levels to narrow down parallax, handle scale and parallax differences. Image encoder and fusion decoder, extract multi-scale features and combine them to reconstruct a high-quality super-resolved image. It uses new hybrid loss functions comprising warping loss, landmark loss and super-resolution loss to improve accuracy and stability by stabilizing the training of alignment module and helps to improve super-resolution performance.\ntwo-stage wrapping, hybrid loss\n1 Introduction The development of method:\npatch-matching + patch-synthesis + iteratively applying non-uniform warping Causes grid artifacts, incapable of handling the non-rigid image deformation Directly warping between the low and high-resolution images is inaccurate. Such iterative combination of patch matching and warping introduces heavy computational burden. The difference between rigid deformation and non-rigid deformation:\nRigid deformation = viewpoint change, like camera movement. Non-rigid deformation = object itself changes shape (face expression, fabric fold, petal bending). Grid artifacts = tiny square patterns caused by wrong image enlargement or alignment.\nwarping + synthesis It cannot effectively handle large-parallax cases that widely existed in real-world data. pre-warping + re-warping + synthesis CrossNet++ is a unified framework enabling fully end-to-end training which does not require pretraining the flow estimator. Two-stage pipeline: Two-stage cross-scale warping module. stage 1: Uses FlowNet to estimate motion (optical flow) between the low-resolution (LR) and reference (Ref) images without needing ground-truth flow (self-supervised). This produces a roughly aligned ‚Äúwarped-Ref‚Äù image. stage 2: Further refines alignment between the warped-Ref and LR image for more accurate warping. Hybrid loss: warping loss, landmark loss and super-resolution loss. warping loss: supervise the flow estimation implicitly. landmark loss: supervise the flow estimation explicitly. Without ground-truth flow = the model learns to estimate motion on its own, using only the images, not any pre-labeled motion data.\nInterpolation = predict inside known area Extrapolation = predict outside known area 2 Related Work 3 Preliminary of CrossNet 3.3 Network Structure 3.3.1 Alignment Module The alignment module aims to align the reference image $I_{REF}$ with the low-resolution image $I_{LR}$. CrossNet++ adopts a warping-based alignment using two-stage optical flow estimation.\nIn the first stage, a modified FlowNet (denoted as $Flow_1$) predicts the flow field between an upsampled LR image $I_{LR‚Üë}$ and the reference image $I_{REF}$:\n$$ V_1^0 = Flow_1(I_{LR‚Üë}, I_{REF}) $$ where $V_1^0$ represents the flow at scale 0 (the original image scale). The upsampled LR image $I_{LR‚Üë}$ is obtained via a single-image SR method:\n$$ I_{LR‚Üë} = SISR(I_{LR}) $$ Then, the reference image is spatially warped using this flow to produce the pre-aligned reference:\n$$ \\hat{I}_{REF} = Warp(I_{REF}, V_1^0) $$ In the second stage, the pre-aligned reference \\( \\hat{I}_{\\mathrm{REF}} \\) and the upsampled LR image \\( I_{LR}\\uparrow \\) are again input to another flow estimator \\( Flow_2 \\) to compute multi-scale flow fields:\n$$ {V_2^3, V_2^2, V_2^1, V_2^0} = Flow_2(I_{LR‚Üë}, \\hat{I}_{REF}) $$ These multi-scale flows are used later in the synthesis network to refine alignment and reconstruct the final super-resolved image.\nthis two-stage alignment, coarse warping followed by multi-scale refinement‚Äîallows CrossNet++ to handle large parallax and depth variations, achieving more accurate correspondence and better alignment quality than the original CrossNet.\n3.3.2 Encoder Through the alignment module, we obtain four flow fields at different scales. The encoder receives the pre-aligned reference image $\\hat I_{REF}$ and the upsampled LR image $I_{LR‚Üë}$, then extracts their feature maps at four different scales.\nThe encoder has five convolutional layers with 64 filters of size ( 5 $\\times$ 5 ).\nThe first two layers (stride = 1) extract the feature map at scale 0. The next three layers (stride = 2) produce lower-resolution feature maps for scales 1 to 3. These operations are defined as: where $\\sigma$ is the ReLU activation, $*_1$ and $*_2$ denote convolutions with strides 1 and 2 respectively, and $F^i$ is the feature map at scale $i$.\n$$ F^0 = \\sigma(W^0 *_{1} I) $$ $$ F^i = \\sigma(W^i *_{2} F^{i-1}), \\\\ \\quad i = 1, 2, 3, $$ Unlike the original CrossNet, CrossNet++ uses a shared encoder for both $\\hat I_{REF}$ and $I_{LR‚Üë}$ instead of two separate encoders, which reduces about 0.41 M parameters while maintaining accuracy.\nThe resulting feature sets are:\n$$ {F^0_{LR}, F^1_{LR}, F^2_{LR}, F^3_{LR}} \\quad \\text{and} \\quad {F^0_{REF}, F^1_{REF}, F^2_{REF}, F^3_{REF}}. $$ Finally, each reference feature map $F^i_{REF}$ is warped using the multi-scale flow fields $V^i_2$ from to produce the aligned feature maps:\n$$ \\hat{F}^i_{REF} = Warp(F^i_{REF}, V^i_2), \\\\ \\quad i = 0, 1, 2, 3. $$ In short, the encoder extracts multi-scale feature maps for both LR and reference images using shared convolutional layers, then aligns the reference features to the LR features through warping with multi-scale flow fields, which provides precise, scale-consistent alignment for the next fusion step.\n3.3.3 Decoder After feature extraction and alignment, the decoder fuses the LR and reference feature maps and generates the final super-resolved image.\nIt follows a U-Net-like structure, which progressively upsamples the feature maps from coarse to fine scales.\nTo create the decoder features at scale $i$, the model concatenates:\nthe warped reference features $\\hat{F}^i_{REF}$, the LR image features $F^i_{LR}$¬†, and the decoder feature from the next coarser scale $F^{i+1}_{D}$ (if available). Then a deconvolution layer (stride 2, filter size 4 $\\times$ 4) is applied:\n$$ F^3_{D} = \\sigma(W^3_{D} *_{2} (F^3_{LR}, \\hat{F}^3_{REF})) $$ where $*_2$ is deconvolution with stride 2 and $\\sigma$ is the activation (ReLU).\n$$ F^i_{D} = \\sigma(W^i_{D} *_{2} (F^i_{LR}, \\hat{F}^i_{REF}, F^{i+1}_{D})), \\\\quad i = 2, 1, $$ After that, three more convolutional layers (filter sizes (5 $\\times$ 5), channels {64, 64, 3}) perform post-fusion to synthesize the final image $I_p$:\n$$ F^0_{D} / F_1 = \\sigma(W_1 *_{1} (F^0_{LR}, \\hat{F}^0_{REF}, F^1_{D})) $$ $$ F_2 = \\sigma(W_2 *_{1} F_1) $$ $$ I_p = \\sigma(W_p *_{1} F_2), $$ where $*_{1}$¬†means convolution with stride 1.\nThe decoder takes aligned multi-scale features from LR and reference images, fuses them step by step through deconvolutions and convolutions, and finally reconstructs the high-resolution output image $I_p$, the sharp, super-resolved result.\n3.4 Loss Function warping loss, landmark loss ‚Üí encourage flow estimator to generate precise flow.\nsuper-resolution loss ‚Üí is responsible for the final synthesized image.\n3.4.1 Warping Loss Used in the first-stage Flow Estimator to regularize the generated optical flow.\nIt ensures that the warped reference image $\\hat I_{REF}$ is close to the ground-truth HR image $I_{HR}$, assuming both share a similar viewpoint.\nThe loss minimizes pixel-wise intensity differences:\n$$ L_{warp} = \\frac{1}{2N} \\sum_{i,s,c} (\\hat{I}_{REF}(s, c) - I_{HR}(s, c))^2 $$ where $N$ is the total number of samples, $i$, $s$, and $c$ iterate over training samples, spatial locations and color channels respectively.\n3.4.2 Landmark Loss This loss provides directional geometric guidance for large-parallax cases.\nIt uses SIFT feature matching to find corresponding landmark pairs $(p, q)$ between the HR and reference images, and applies the flow field $V^0_1$ to warp these landmarks.\nThe warped landmark $\\hat{p}^j$ is computed as:\n$$ \\hat{p}^j = p^j + V^0_1[p^j] $$ and the landmark loss penalizes the distance between warped and target landmarks:\n$$ L_{lm} = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{m_i} | \\hat{p}^j - q^j |_2^2 $$ where $m_i$¬†is the number of landmark pairs in image $i$.\nThis term helps the flow estimator predict more accurate motion fields, especially when viewpoints differ greatly.\n3.4.3 Super-Resolution Loss This loss directly trains the model to synthesize the final super-resolved image $I_p$, comparing it with the ground-truth high-resolution image $I_{HR}$ using the Charbonnier penalty (a smooth $L_1$ loss):\n$$ L_{sr} = \\frac{1}{N} \\sum_{i,s,c} \\rho(I_{HR}(s, c) - I_p(s, c)) $$ $$ \\rho(x) = \\sqrt{x^2 + 0.001^2}. $$ 4 Experiment Flower dataset and LFVideo dataset\n14 $\\times$ 14 angular samples of size 376 $\\times$ 541. training and testing: central 8 $\\times$ 8 grid of angular samples top-left 320 $\\times$ 512 for training and testing training: 3243 images from Flower and 1080 images from LFVideo testing: 100 images from Flower and 270 images from LFVideo ","permalink":"/posts/crossnet++_cross-scale_large-parallax_warping_for/","summary":"\u003caside\u003e\n\u003ch2 id=\"crossnet-and-crossnet\"\u003e\u003cstrong\u003eCrossNet\u003c/strong\u003e and \u003cstrong\u003eCrossNet++\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eBoth are for \u003cstrong\u003eReference-Based Super-Resolution (RefSR),\u003c/strong\u003e using a \u003cstrong\u003elow-resolution (LR)\u003c/strong\u003e image and a \u003cstrong\u003ehigh-resolution (HR reference)\u003c/strong\u003e image to make a sharper, high-quality output.\u003c/p\u003e\n\u003cp\u003eThe performance of \u003cstrong\u003eCrossNet\u003c/strong\u003e drops with the increasing of perspective parallax, the improvement of \u003cstrong\u003eCrossNet++:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTwo-stage warping\u003c/strong\u003e ‚Üí improves alignment\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-supervised flow estimation\u003c/strong\u003e ‚Üí uses \u003cstrong\u003eFlowNet\u003c/strong\u003e to estimate motion between LR and Ref images\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCross-scale alignment\u003c/strong\u003e ‚Üí Aligns features at \u003cstrong\u003emultiple resolutions\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHybrid loss functions\u003c/strong\u003e ‚Üí warping + landmark + super-resolution loss\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReal-world performance\u003c/strong\u003e ‚Üí produces smoother, \u003cstrong\u003esharper\u003c/strong\u003e, and more realistic results, suitable for a variety of scenarios\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003ch1 id=\"abstraction\"\u003eAbstraction\u003c/h1\u003e\n\u003cp\u003eCrossNet++ focuses on \u003cstrong\u003ereference-based super-resolution (RefSR),\u003c/strong\u003e improving a low-resolution image using a high-resolution reference from another camera. This task is hard because of \u003cstrong\u003elarge scale differences (8√ó)\u003c/strong\u003e and \u003cstrong\u003ebig parallax (~10%)\u003c/strong\u003e between the two views.\u003c/p\u003e","title":"CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution"},{"content":"paper resource\nRWKV = a bridge between LSTM and Transformer (less computation and memory, parallel in training). Replace self-attention with a time-mixing mechanism that behaves like an LSTM in time but a Transformer in training and inference xLSTM = scale up the LSTM family to match or even surpass Transformer-level performance. Keep the recurrent spirit of LSTM, but redesign gates and memory (mLSTM) so it can train and scale efficiently like Transformers. Speciality: memory mixing, RWKV Abstract The paper revisits LSTMs, whose key innovations are the constant error carousel and gating mechanisms ‚Äî originally solved the vanishing-gradient problem and made long-term memory possible. Although Transformers later surpassed LSTMs thanks to their parallelizable self-attention, the authors ask whether LSTMs can be scaled up, like modern LLMs, to billions of parameters while overcoming their known limits.\nTo achieve this, they introduce:\nExponential gating ‚Äî a new gating function with improved normalization and stability. Modified memory structures: sLSTM ‚Äî uses scalar memory and scalar updates with new ‚Äúmemory mixing.‚Äù ‚Üí memory mixing mLSTM ‚Äî introduces matrix-based memory that supports full parallelization and a covariance-based update rule. A new memory architecture ‚Üí parallelization By stacking these enhanced cells into residual xLSTM blocks, they create architectures that combine the strengths of LSTMs and Transformers.\nExperiments show that xLSTMs can match or even outperform Transformers and State Space Models in both performance and scaling.\nüëâ Code: github.com/NX-AI/xlstm\n1 Introduction 1.1 LSTM $$ c_t = f_t c_{t-1} + i_t z_t, \\quad h_t = o_t \\psi(c_t) $$\nUpdate the cell state / long-term memory: $$ c_t = f_t c_{t-1} + i_t z_t $$\n$c_t$: Cell state, real-valued vector, the internal long-term memory after update $f_t$: Forget gate, values in (0, 1), decide how much of $c_{t-1}$ to keep $c_{t-1}$: Previous cell state, vector, carries long-term memory $i_t$: Input gate, values in (0, 1), decides how much new info to write $z_t$: Cell input / candidate memory, usually $\\tanh(\\cdot)$ output, the new content that could be added Produce the output / hidden state / short-term memory: $$ \\quad h_t = o_t \\psi(c_t) $$\n$h_t$: Hidden state, vector, output of the cell (short-term memory) $o_t$: Output gate, values in (0, 1), controls what part of memory is shown outside $\\psi(c_t)$: Activation function (often $\\tanh(c_t$)), squashes memory to bounded range Three Main Limitations of LSTMs Can‚Äôt revise stored information Once an LSTM stores something in its cell state, it struggles to update or replace it later. xLSTM fix: introduces exponential gating, allowing flexible updating of stored values. Limited storage capacity Traditional LSTMs store information in a single scalar cell state, forcing compression and loss of details. xLSTM fix: uses a matrix memory, which can hold richer, multi-dimensional information. No parallelization LSTM depends on sequential hidden-to-hidden connections, meaning each step waits for the previous one. xLSTM fix: changes the memory mixing structure to make computation parallelizable across time steps. 2 Extended LSTM Two main modifications: exponential gating and novel memory structures. Two variants mombined into xLSTM blocks, stacked with residual connections to build xLSTM architectures, both can have multiple memory cells and heads: sLSTM ‚Äì scalar memory, scalar update, memory mixing across cells. mLSTM ‚Äì matrix memory, covariance (outer product) update, fully parallelizable. 2.2 sLSTM sLSTM = LSTM + exponential gates + normalization state + stabilizer state + multiple memory cells.\nThe exponential gates $i_t$ and $f_t$ make it easier to amplify or reduce memory dynamically.\n‚Üí Helps sLSTM revise stored information better (a key limitation of classical LSTM).\nThe normalizer state $n_t$ keeps things numerically stable, so exponential growth doesn‚Äôt blow up.\nThe stabilizer state $m_t$ keeps their scale controlled, prevents numerical overflow during training.\nThe Multiple heads each with its own LSTM-like structure to compute its own $h_t$, then combined, just like multi-head attention in Transformers, allowing the network to learn different kinds of temporal patterns in parallel.\nNew Memory Mixing: In an sLSTM, each time step has multiple memory cells ‚Üí a vector computed by recurrent matrices R, each cell stores part of the long-term memory, we allow these memory cells to talk to each other.\nMemory mixing = different parts (dimensions) of the memory cells communicate and influence each other.\n2.3 mLSTM mLSTM = LSTM + exponential gates + normalization state + stabilizer state + multiple memory cells.\nmLSTM replaces the small one-number memory $c_t$ of LSTM with a key‚Äìvalue memory matrix, so it can store, search, and update information like attention, but still works as a recurrent network (RNN).\n$q_k, k_t, v_t$ ‚Üí same like query, key, value in transformer‚Ä¶ uses a matrix memory because it wants to store relationships between features (keys and values), not just single values like traditional LSTM. The normalizer state $n_t$ is the weighted sum of key vectors, keeps record of the strength of the gates. Multiple heads and multiple cells are equivalent as there is no memory mixing. 2.4 xLSTM Architecture 2.4.1 xLSTM Blocks Each block takes an input (sequence or features), passes it through an sLSTM or mLSTM cell, adds some non-linear layers (MLPs) and residual/skip connections, finally outputs a transformed sequence representation.\nPatterns are easier to separate after mapping into a higher-dimensional space. Like for better points classification, we can map each point from 2D ‚Üí 3D.\nWhen an xLSTM processes a sequence, it wants to distinguish different histories, for example:\n‚ÄúThe dog chased the cat‚Äù vs ‚ÄúThe cat chased the dog.‚Äù These sequences may look similar in lower dimensions (both use same words), but when we map them into a higher-dimensional representation, the model can more easily tell them apart.\nSo each xLSTM block:\nexpands data into a higher space (‚Äúup-projection‚Äù), applies non-linear transformations, and then compresses back (‚Äúdown-projection‚Äù). That makes it easier for the model to separate different contexts or meanings.\nType Memory type Recurrent connections Parallelization Up-proj position Storage capacity sLSTM Post up-projection Scalar memory (vector) ‚úÖ via matrices R ‚ùå¬†Sequential after LSTM Smaller mLSTM Pre up-projection Matrix memory ‚ùå¬†No recurrent matrices ‚úÖ parallelizable before LSTM Much larger 2.4.2 xLSTM Architecture Figure 1: The extended LSTM (xLSTM) family.\nFrom left to right:\nThe original LSTM memory cell with constant error carousel and gating. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule. mLSTM and sLSTM in residual blocks yield xLSTM blocks. Stacked xLSTM blocks give an xLSTM architecture. The constant error carousel is the additive update of the cell state $c_{t‚àí1}$ (green) by cell inputs $z_t$ and moderated by sigmoid gates (blue).\nThe gating mechanisms:\nForget gate decides what to erase. Input gate decides what to add. Output gate decides what to show. 4 Experiments LSTM and xLSTM models far outperform Transformers and State Space Models on tasks that need long-term memory and state tracking; xLSTM, especially when combining sLSTM + mLSTM, achieves the best all-around performance, showing that recurrent memory architectures still beat attention models for logical and structured reasoning.\nThe paper uses perplexity (ppl) as the main evaluation metric for language modeling. It measures how well the model predicts the next token in a text sequence.\nThe model is confident and accurate ‚Üí the model gives high probability to the correct next word ‚Üí it‚Äôs confident ‚Üí low perplexity. The model is confused and often wrong ‚Üí the model gives low probability ‚Üí it‚Äôs uncertain or wrong ‚Üí high perplexity. Scaling Laws Next token prediction perplexity of xLSTM, RWKV-4, Llama, and Mamba on the SlimPajama validation set when trained on 300B tokens from SlimPajama. Model sizes are 125M, 350M, 760M, and 1.3B. The scaling laws indicate that for larger models xLSTM will perform well too.\n5 Limitations sLSTM not parallelizable:\nIts memory mixing prevents full parallel execution. Custom CUDA version is faster, but still ~2√ó slower than mLSTM. mLSTM kernels not optimized:\nCurrent CUDA implementation is ~4√ó slower than FlashAttention. Could be improved with better GPU kernels. High computation cost:\nmLSTM processes (d \\times d) matrices, which increases compute load,\nthough it can still be parallelized using standard matrix ops.\nGate initialization sensitivity:\nForget-gate parameters must be tuned carefully for stability. Memory limits at long contexts:\nLarge matrix memory may overload at very long sequence lengths,\nbut works fine up to ~16k tokens.\nNot fully optimized yet:\nArchitecture and hyperparameters weren‚Äôt exhaustively tuned due to cost. More optimization could further boost performance. ","permalink":"/posts/xlstm_extended_long_short-term_memory/","summary":"\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2405.04517\"\u003epaper resource\u003c/a\u003e\u003c/p\u003e\n\u003caside\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRWKV\u003c/strong\u003e = a \u003cem\u003ebridge\u003c/em\u003e between LSTM and Transformer (less computation and memory, parallel in training). Replace self-attention with a time-mixing mechanism that behaves like an LSTM in time but a Transformer in training and inference\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003exLSTM\u003c/strong\u003e = s\u003cem\u003ecale up\u003c/em\u003e the LSTM family to match or even surpass Transformer-level performance. Keep the recurrent spirit of LSTM, but redesign gates and memory (mLSTM) so it can train and scale efficiently like Transformers.\n\u003cul\u003e\n\u003cli\u003eSpeciality: memory mixing, \u003cdel\u003eRWKV\u003c/del\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003eThe paper revisits \u003cstrong\u003eLSTMs\u003c/strong\u003e, whose key innovations are the \u003cstrong\u003econstant error carousel\u003c/strong\u003e and \u003cstrong\u003egating mechanisms\u003c/strong\u003e ‚Äî originally solved the vanishing-gradient problem and made long-term memory possible. Although \u003cstrong\u003eTransformers\u003c/strong\u003e later surpassed LSTMs thanks to their \u003cstrong\u003eparallelizable self-attention\u003c/strong\u003e, the authors ask whether LSTMs can be scaled up, like modern LLMs, to \u003cstrong\u003ebillions of parameters\u003c/strong\u003e while overcoming their known limits.\u003c/p\u003e","title":"xLSTM: Extended Long Short-Term Memory"},{"content":"Abstract Transformers are very powerful for language tasks and training is ****faster on GPUs because of parallization, but they use a lot of memory and computing power, especially when processing long text, their cost grows very fast (quadratically).\nRNNs, on the other hand, use less memory and computation cause they grow linearly but are slower to train and not as good at handling long sentences.\nThe new model RWKV mixes the good parts of both, it trains quickly and gets good performance like a Transformer and also runs efficiently like an RNN.\nMemory and computation:\nIn a Transformer, each token looks at every other token through self-attention. If you have N tokens in a sentence, it compares every pair, so total comparisons = N √ó N = N¬≤. That‚Äôs why Memory and computation both grow quadratically. In an RNN, the model reads tokens one by one, passing information step by step. So for N tokens, it just does N steps, the total cost = N (linear). 1 Introduction RWKV reformulates the attention mechanism with a variant of linear attention, replacing traditional dot-product token interaction with more effective channel-directed attention. This implementation, without approximation, offers the lowest computational and memory complexity.\n2 Background 2.1 Recurrent Neural Networks (RNNs) Popular RNN architectures such as LSTM and GRU. Although these RNNs can be factored into two linear blocks (W and U) and an RNN-specific block, the data dependency relying on previous time steps prohibits parallelizing these typical RNNs.\n$$ \\begin{aligned} f_t \u0026= \\sigma_g\\left(W_f x_t + U_f h_{t-1} + b_f\\right), \\\\ i_t \u0026= \\sigma_g\\left(W_i x_t + U_i h_{t-1} + b_i\\right), \\\\ o_t \u0026= \\sigma_g\\left(W_o x_t + U_o h_{t-1} + b_o\\right), \\\\ \\tilde{c}_t \u0026= \\sigma_c\\left(W_c x_t + U_c h_{t-1} + b_c\\right), \\\\ c_t \u0026= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t, \\\\ h_t \u0026= o_t \\odot \\sigma_h(c_t). \\end{aligned} $$ Model Depends on Parallelizable? RNN computed previous state ‚ùå No RWKV raw previous input ‚úÖ Yes (for training) 2.2 Transformers and AFT Standard Transformer self-attention Matrix form\n$$ \\text{Attn}{Attn}(Q,K,V)=\\text{softmax}(QK^\\top)V $$ Per token (t)\n$$ \\text{Attn}{Attn}(Q,K,V)_t=\\frac{\\sum_{i=1}^{T}\\exp(q_t^\\top k_i) \\odot v_i}{\\sum_{i=1}^{T}\\exp(q_t^\\top k_i)} $$ The weighted value equation in multi-head attention. Weight of token $i$ for query $t$ is $\\exp(q_t^\\top k_i)$. Output is a weighted average of $v_i$ AFT (Attention-Free Transformer) variant $$ \\text{Attn}{Attn}^{+}(W,K,V)_t=\\frac{\\sum_{i=1}^{t}\\exp(w_{t,i}+k_i)\\odot v_i}{\\sum_{i=1}^{t}\\exp(w_{t,i}+k_i)} $$ Replace $q_t^\\top k_i$ with (learned) position bias $w_{t,i}$ + a key score $k_i$. Causal: sum only $i\\le t$. Still a normalized weighted average, but weights depend on position via $w_{t,i}$. Variables:\ni: past token index, what we‚Äôre reading from memory t: current token index, what we‚Äôre generating now RWKV‚Äôs simplification Define the bias as a decay with distance:\n$$ w_{t,i}=-(t-i)w \\qquad w\\in(\\mathbb{R}{\\ge 0})^d $$ Per-channel vector $w$ ‚áí older tokens get weight $e^{-(t-i)w}\\le 1$. Now weights depend only on how far back a token is, not on $q_t$. This structure lets us keep running sums, so we don‚Äôt need all pairwise scores. Result:\nSame attention-like effect, but computed with O(T) time and O(1) memory per step (like an RNN), while still trainable in parallel across tokens (like a Transformer) by prefix-scan tricks.\nModel Has Query? How it computes weights Complexity Key idea Transformer ‚úÖ Yes all tokens attend to all O(N¬≤) Full pairwise attention, strong but heavy AFT ‚ùå No Uses position bias O(N) Removes query, uses learned positional weights RWKV ‚ùå No Adds time-decay weights O(N) AFT idea + RNN-style update (linear, efficient) 3 RWKV Four fundamental elements:\nR: The Receptance vector acts as the receiver of past information. W: The Weight signifies the positional weight decay vector, a trainable parameter within the model. K: The Key vector performs a role analogous to K in traditional attention mechanisms. V: The Value vector functions similarly to V in conventional attention processes. The difference between Time Mix and Channel Mix:\nTime Mix: Builds each token‚Äôs vector using time order and previous token info Channel Mix: Refines each token‚Äôs vector by mixing internal dimensions (features). It processes each token separately to mix and refine its feature channels. 3.1 Architecture The RWKV model is composed of stacked residual blocks. Each block consists of a time-mixing and a channel-mixing sub-block, embodying recurrent structures to leverage past information.\nThis model uses a unique attention-like score update process, which includes a time-dependent softmax operation improving numerical stability and mitigating vanishing gradients. It ensures that the gradient is propagated along the most relevant path. Additionally, layer normalization incorporated within the architecture aids in stabilizing the gradients, effectively addressing both vanishing and exploding gradient issues.\n3.1.1 Token Shift In this architecture, all linear projection vectors (R, K, V in time-mixing, and R‚Ä≤, K‚Ä≤ in channel- mixing) involved in computations are produced by linear interpolation between current and previous timestep inputs, facilitating a token shift.\nThe vectors for time-mixing computation are linear projections of linear combinations of the current and previous inputs of the block:\n$$ \\begin{aligned} r_t \u0026= W_r \\cdot \\left(\\mu_r \\odot x_t + (1 - \\mu_r) \\odot x_{t-1}\\right), \\\\ k_t \u0026= W_k \\cdot \\left(\\mu_k \\odot x_t + (1 - \\mu_k) \\odot x_{t-1}\\right), \\\\ v_t \u0026= W_v \\cdot \\left(\\mu_v \\odot x_t + (1 - \\mu_v) \\odot x_{t-1}\\right). \\end{aligned} $$ The channel-mixing inputs:\n$$ \\begin{aligned} r'_t \u0026= W'_{r} \\cdot \\left(\\mu'_{r} \\odot x_t + (1 - \\mu'_{r}) \\odot x_{t-1}\\right), \\\\ k'_t \u0026= W'_{k} \\cdot \\left(\\mu'_{k} \\odot x_t + (1 - \\mu'_{k}) \\odot x_{t-1}\\right). \\end{aligned} $$ 3.1.2 WKV Operator $$ wkv_t = \\frac{\\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} \\odot v_i + e^{u + k_t} \\odot v_t}{\\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} + e^{u + k_t}} $$ The difference of treating W between AFT and RWKV:\nPairwise matrix: Each token pair has its own weight Channel-wise vector: One decay weight per feature channel 3.1.3 Output Gating Time Mixing:\n$$ o_t = W_o \\cdot (\\sigma(r_t) \\odot wkv_t) $$ Channel Mixing:\n$$ o'_t = \\sigma(r'_t) \\odot (W'_v \\cdot \\max(k'_t, 0)^2) $$ 4 Trained Models and Computing Costs Adam optimizer, bfloat16 precision, the auxiliary loss introduced by PaLM‚Ä¶\n4.2 Scaling Laws Scaling laws in language models refer to the mathematical relationships that describe how the performance of a language model changes with respect to various factors.\nScaling laws are important for two primary reasons:\nthey allow us to make predictions and plans regarding the costs and performance of large models before they are trained via interpolation and extrapolation. the contexts in which they fail provides rich feedback on important areas for future research. Explain Interpolation and Extrapolation:\nInterpolation: predicting within the range of data you already know. For example, you trained models with 1B, 5B, and 10B parameters, then you interpolate to guess how a 7B model will perform. Extrapolation: predicting beyond the known range. For example, you trained up to 10B, and now you try to estimate performance of a 100B model. 5 Evaluation Evaluation direction and questions:\nCompetitiveness: Tests if RWKV performs as well as Transformers when both use the same computing power.\nLong Context: Tests if RWKV can handle very long texts better than Transformers,\nespecially when Transformers become too slow or costly for long sequences.\n6 Inference Experiments float32 precision, the HuggingFace Transformers,\n7 Future Work 1. Increase Model Expressivity\nImprove time-decay formulas. Explore better initialization of model states. Goal: more powerful representations without losing efficiency. 2. Improve Computational Efficiency\nUse parallel scan in $wkv_t$¬†step. Target complexity: $O(B \\log(T)d)$. 3. Apply to Encoder‚ÄìDecoder Architectures\nReplace cross-attention with RWKV mechanism. Useful for seq2seq and multimodal models. Boosts efficiency in both training and inference. 4. Use of Model State (Context)\nUsed for interpretability and predictability. Could enhance safety and control via prompt tuning. Modify hidden states to guide model behavior. 5. Larger Internal States\nImprove long-term memory and context understanding. Increase performance across various tasks. 8 Conclusion We introduced RWKV, a new approach to RNN models exploiting the potential of time-based mixing components. RWKV introduces several key strategies that allow it to capture locality and long-range dependencies while addressing limitations of current architectures by: (1) replacing the quadratic QK attention with a scalar formulation at linear cost, (2) reformulating recurrence and sequential inductive biases to enable efficient training parallelization and efficient inference, and (3) enhancing training dynamics using custom initializations.\nWe benchmark the proposed architecture in a wide variety of NLP tasks and show comparable performance to SoTA with reduced cost. Further experiments on expressivity, interpretability, and scaling showcase the model capabilities and draw parallels in behavior between RWKV and other LLMs.\nRWKV opens a new route for scalable and efficient architectures to model complex relationships in sequential data. While many alternatives to Transformers have been proposed with similar claims, ours is the first to back up those claims with pretrained models with tens of billions of parameters.\n9 Limitations 1. Performance Limitation (Memory Loss over Long Contexts)\nLinear attention is efficient but may lose fine details over long sequences. RWKV compresses history into a single vector, unlike Transformers that keep all token interactions. Its recurrent design limits ability to fully ‚Äúlook back‚Äù at distant tokens. 2. Dependence on Prompt Engineering\nRWKV relies more on well-designed prompts than Transformers. Poor prompts may cause information loss between prompt and continuation. ","permalink":"/posts/rwkv/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003eTransformers are very powerful for language tasks and training is ****faster on GPUs because of \u003cstrong\u003eparallization\u003c/strong\u003e, but they use \u003cstrong\u003ea lot of memory and computing power\u003c/strong\u003e, especially when processing long text, their cost grows \u003cstrong\u003every fast\u003c/strong\u003e (quadratically).\u003c/p\u003e\n\u003cp\u003eRNNs, on the other hand, use \u003cstrong\u003eless memory and computation\u003c/strong\u003e cause they grow linearly but are \u003cstrong\u003eslower to train\u003c/strong\u003e and not as good at handling long sentences.\u003c/p\u003e\n\u003cp\u003eThe new model \u003cstrong\u003eRWKV\u003c/strong\u003e mixes the good parts of both, it trains quickly and gets good performance like a Transformer and also runs efficiently like an RNN.\u003c/p\u003e","title":"RWKV: Reinventing RNNs for the Transformer Era"},{"content":"Abstract The game of Go:\nThe most challenging of classic games for AI, because:\nEnormous search space The difficulty of evaluating board positions and moves Concept Meaning Example in Go AI Solution Enormous search space Too many possible moves and future paths ‚Üí impossible to explore all At every turn, Go has ~250 legal moves; across 150 moves ‚Üí (250^{150}) possibilities Policy network narrows down the choices (reduces breadth of search) Hard-to-evaluate positions Even if you know the board, it‚Äôs hard to know who‚Äôs winning Humans can‚Äôt easily assign a numeric score to a mid-game position Value network predicts win probability (reduces depth of search) AlphaGo Imagine AlphaGo is a smart player who has:\nintuition ‚Üí from the policy network judgment ‚Üí from the value network planning ability ‚Üí from MCTS Integrating deep neural networks with Monte Carlo Tree Search (MCTS).\nThe main innovations include:\nTwo Neural Networks: Policy Network: Selects promising moves ‚Üí the probability of each move. Value Network: Evaluates board positions ‚Üí the likelihood of winning. Training Pipeline: Supervised Learning (SL) from expert human games to imitate professional play. Reinforcement Learning (RL) through self-play, improving beyond human strategies. Integration with MCTS: Combines the predictive power of neural networks with efficient search. Reduces: breadth (number of moves to consider) depth (number of steps to simulate) of search. MCTS It first adds all legal moves (children) under the current position in the tree. In every simulation, AlphaGo chooses one branch to go deeper into the tree (not all of them). It decides which one based on three main metrics: Policy Prior (P) ‚Üí the probability of the move from policy network Visit Count (N) ‚Üí how many times we‚Äôve already explored this move during simulations. Q-Value (Q) ‚Üí average win rate from past simulations Symbol Meaning Source Role in decision P(s,a) Policy prior (initial move probability) From policy network Guides initial exploration N(s,a) Number of times this move was explored Counted during simulations Balances exploration vs exploitation Q(s,a) Average predicted win rate (past experience) From value network results of simulations Exploitation: ‚Äúkeep doing what worked‚Äù Results:\nWithout search, AlphaGo already played at the level of strong Go programs. With the neural-network-guided MCTS, AlphaGo achieved a 99.8% win rate against other programs. It became the first program ever to defeat a human professional Go player (Fan Hui, European champion) by 5‚Äì0. Introduction Optimal value function $v^*(s)$ For any game state $s$, there exists a theoretical function that tells who will win if both players play perfectly. Computing this function exactly means searching through all possible sequences of moves. Search space explosion Total possibilities ‚âà $b^d$, where $b$: number of legal moves (breadth), $d$: game length (depth). For Go: ( $b$ ‚âà 250, $d$ ‚âà 150) ‚Üí bigggg number ‚Üí impossible to compute exhaustively. Reducing the search space ‚Äî two key principles: (1) Reduce depth using an approximate value function $v(s)$: Stop (truncate) deep search early. Use an approximate evaluator to predict how good a position is instead of exploring all future moves. This worked in chess, checkers, and Othello, but was believed to be impossible (‚Äúintractable‚Äù) for Go because Go‚Äôs positions are much more complex. (2) Reduce breadth using a policy $p(a|s)$: Instead of exploring all moves, only sample the most likely or promising ones. This narrows down which actions/moves to consider, saving enormous computation. Example: Monte Carlo rollouts: Simulate random games (using the policy) to estimate how good a position is. Maximum depth without branching at all, by sampling long sequences of actions for both players from a policy $p$. This method led to strong results in simpler games (backgammon, Scrabble), and weak amateur level in Go before AlphaGo. ‚ÄúSimulate‚Äù and ‚ÄúRoll out‚Äù basically mean the same thing in this context.\n‚ÄúSimulate‚Äù ‚Üí a general word: to play out an imaginary game in your head or computer. ‚ÄúRoll out‚Äù ‚Üí a more specific term from Monte Carlo methods, meaning ‚Äúplay random moves from the current position until the game ends.‚Äù So ‚Üí every rollout is one simulation of a complete (or partial) game.\nrollout = one simulated playthrough. Monte Carlo Rollout Monte Carlo rollout estimates how good a position is by:\nStarting from a given board position (s). Playing many simulated games to the end (using policy-guided moves ‚Üí reduce breadth). Recording each game‚Äôs result (+1 for win, ‚àí1 for loss). Averaging all outcomes to estimate the win probability for that position. $$ v(s) \\approx \\text{average(win/loss results from rollouts)} $$\nGoal:\nApproximate the value function $v(s)$, the expected chance of winning from position $s$.\nIt‚Äôs simple but inefficient ‚Äî great for small games, too slow and noisy for Go.\nMonte Carlo tree search MCTS uses Monte Carlo rollouts to estimate the value of each state. As more simulations are done, the search tree grows and values become more accurate. It can theoretically reach optimal play, but earlier Go programs used shallow trees and simple, hand-crafted policies or linear value functions (not deep learning). These older methods were limited because Go‚Äôs search space was too large. Training pipeline of AlphaGo Deep convolutional neural networks (CNNs) can represent board positions much better. So AlphaGo uses CNNs to reduce the search complexity in two ways: Evaluating positions with a value network ‚Üí replaces long rollouts (reduces search depth). Sampling moves with a policy network ‚Üí focuses on likely moves (reduces search breadth). Together, this lets AlphaGo explore much more efficiently than traditional MCTS. Supervised Learning (SL) Policy Network $p_\\sigma$: trained from human expert games. Fast Policy Network $p_\\pi$: used to quickly generate moves during rollouts. Reinforcement Learning (RL) Policy Network $p_\\rho$: improves the SL policy through self-play, optimizing for winning instead of just imitating humans. Value Network $v_\\theta$: predicts the winner from any board position based on self-play outcomes. Final AlphaGo system = combines policy + value networks inside MCTS for strong decision-making. Supervised learning of policy networks Panel(a): Better policy-network accuracy in predicting expert moves ‚Üí stronger actual gameplay performance.\nThis proves that imitation learning (supervised policy $p_œÉ$) already provides meaningful playing ability before any reinforcement learning or MCTS.\nFast Rollout Policy networks $p_\\pi(a|s)$\nA simpler and faster version of the policy network used during rollouts in MCTS. Uses a linear softmax model on small board-pattern features (not deep CNN). Much lower accuracy (24.2 %) but extremely fast takes only 2 ¬µs per move (vs. 3 ms for the full SL policy). Trained with the same supervised learning principle on human moves. Reinforcement learning of policy networks Structure of the policy network = SL policy network initial weights œÅ = œÉ Step What happens What‚Äôs learned Initialize Copy weights from SL policy (œÅ = œÉ) Start with human-like play Self-play Pick current p and an older version p Generate thousands of full games (self-play) Reward +1 for win, ‚àí1 for loss Label each move sequence, and collect experience (state, action, final reward) Update Update weights œÅ by SGD Policy network Repeat Thousands of games Stronger, self-improving policy Reinforcement learning of value networks Step What happens What‚Äôs learned Initialize Start from the trained RL policy network; use it to generate self-play games Provides realistic, high-level gameplay data Self-play RL policy network plays millions of games against itself Produce diverse board positions and their final outcomes (+1 win / ‚àí1 loss) Sampling Randomly select one position per game to form 30 M independent (state, outcome) pairs Avoids correlation between similar positions Labeling Each position (s) labeled with the final game result (z) Links every board state to its real win/loss outcome Training Train the value network (v_Œ∏(s)) by minimizing MSE Learns to predict winning probability directly from a position Evaluation Compare against Monte Carlo rollouts (pœÄ, pœÅ) Matches rollout accuracy with 15 000√ó less computation Result MSE ‚âà 0.23 (train/test), strong generalization Reliable position evaluation for use in MCTS Problem of naive approach of predicting game outcomes from data consisting of complete games:\nThe value network was first trained on all positions from the same human games. Consecutive positions were almost identical and had the same win/loss label. The network memorized whole games instead of learning real position evaluation ‚Üí overfitting (MSE = 0.37 test). Solution\nGenerate a new dataset: 30 million self-play games, take only one random position per game. Each sample is independent, so the network must learn general Go patterns, not memorize. Result: good generalization (MSE ‚âà 0.23) and accurate position evaluation. Searching with policy and value networks (MCTS) Panel Step What happens Which network helps a Selection Traverse the tree from root to a leaf by selecting the move with the highest combined score Q + u(P). Uses Q-values (average win) and policy priors P (from policy network). b Expansion When reaching a leaf (a position never explored before), expand it: generate its possible moves and initialize their prior probabilities using the policy network RL policy network c Evaluation Evaluate this new position in two ways: ‚ë† Value network (v_Œ∏(s)): predicts win probability instantly. ‚ë° Rollout with fast policy p_œÄ: quickly play random moves to the end, get final result (r). Value net + Fast policy d Backup Send the evaluation result (average of (v_Œ∏(s)) and (r)) back up the tree ‚Äî update each parent node‚Äôs Q-value (mean of all results from that branch). None directly (update step) The core idea Each possible move/edge (s, a) in the MCTS tree stores 3 key values:\nSymbol Meaning Source P(s,a) Prior probability ‚Äî how promising this move looks before searching From the policy network N(s,a) How many times this move has been tried From search statistics Q(s,a) Average win rate from playing move a at state s From past simulations Step 1: Selection ‚Äî choose which move to explore next At each step of a simulation, AlphaGo selects the move $a_t$ that maximizes:\n$$ a_t = \\arg\\max_a [Q(s_t, a) + u(s_t, a)] $$\nwhere the bonus term $u(s,a)$ encourages exploration:\n$$ u(s,a) \\propto \\frac{P(s,a)}{1 + N(s,a)} $$\n$Q(s,a)$: ‚ÄúHow good this move has proven so far.‚Äù $u(s,a)$: ‚ÄúHow much we should still explore this move.‚Äù ‚Üí Moves that are both good (high Q) and underexplored (low N) get priority.\nAs N increases, the bonus term shrinks ‚Äî the search gradually focuses on the best moves.\nStep 2: Expansion When the search reaches a leaf (a position not yet in the tree):\nThe policy network $p_\\sigma(a|s)$ outputs a probability for each legal move. Those values are stored as new P(s,a) priors for the new node. Initially $N(s,a) = 0$ $Q(s,a) = 0$ Now the tree has grown ‚Äî this new node represents a new possible future board.\nStep 3: Evaluation ‚Äî estimate how good the leaf is Each leaf position $s_L$ is evaluated in two ways:\nValue network $v_Œ∏(s_L)$: directly predicts win probability. Rollout result $z_L$: fast simulation (using the fast rollout policy $p_œÄ$) until the game ends +1 if win ‚àí1 if loss. Then AlphaGo combines the two results:\n$$ V(s_L) = (1 - Œª)v_Œ∏(s_L) + Œªz_L $$\n$Œª$ = mixing parameter (balances between value net and rollout). If $Œª$ = 0.5, both count equally. Step 4: Backup ‚Äî update the tree statistics The leaf‚Äôs evaluation $V(s_L)$ is propagated back up the tree:\nEvery move (edge) $(s, a)$ that was used to reach that leaf gets updated:\n$$ N(s,a) = \\sum_{i=1}^{n} 1(s,a,i) $$\n$$ Q(s,a) = \\frac{1}{N(s,a)} \\sum_{i=1}^{n} 1(s,a,i) V(s_L^i) $$\n$1(s,a,i)$ = 1 if that move was part of the i-th simulation, else 0. $V(s_L^i)$ = evaluation result from that simulation‚Äôs leaf. So, Q(s,a) becomes the average value of all evaluations ( $r$ and $vŒ∏$) in its subtree.\nStep 5: Final move decision After thousands of simulations, the root node has a set of moves with:\n$P(s_0, a)$: from policy network, $Q(s_0, a)$: average win rate, $N(s_0, a)$: visit counts. AlphaGo chooses the move with the highest visit count (N) ‚Äî the most explored and trusted move.\nWhy SL policy network performed better than RL policy network for MCTS?\nPolicy Behavior Effect in MCTS SL policy Mimics human experts ‚Üí gives a diverse set of good moves MCTS can explore several promising branches efficiently RL policy Optimized for winning ‚Üí focuses too narrowly on top 1‚Äì2 moves MCTS loses diversity ‚Üí gets less exploration benefit So, for MCTS‚Äôs exploration stage, a broader prior (SL policy) performs better.\nBut for value estimation, the RL value network is superior ‚Äî because it predicts winning chances more accurately.\nImplementation detail Evaluating policy \u0026amp; value networks takes much more compute than classical search. AlphaGo used: 40 search threads, 48 CPUs, 8 GPUs for parallel evaluation. The final system ran asynchronous multi-threaded search: CPUs handle the tree search logic, GPUs compute policy and value network evaluations in parallel. This allowed AlphaGo to efficiently combine deep learning with massive search.\nAll programs were allowed 5 s of computation time per move.\nDiscussion In this work we have developed a Go program, based on a combination of deep neural networks and tree search. We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.\nSelect those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network. Policy network ‚Üí ‚Äúprobability of choosing a move‚Äù\nValue network ‚Üí ‚Äúprobability of winning from a position‚Äù\n","permalink":"/posts/mastering-go-mcts/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eThe game of Go:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe most challenging of classic games for AI, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnormous search space\u003c/li\u003e\n\u003cli\u003eThe difficulty of evaluating board positions and moves\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eConcept\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n          \u003cth\u003eExample in Go\u003c/th\u003e\n          \u003cth\u003eAI Solution\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eEnormous search space\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eToo many possible moves and future paths ‚Üí impossible to explore all\u003c/td\u003e\n          \u003ctd\u003eAt every turn, Go has ~250 legal moves; across 150 moves ‚Üí (250^{150}) possibilities\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003ePolicy network\u003c/strong\u003e narrows down the choices (reduces \u003cem\u003ebreadth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eHard-to-evaluate positions\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eEven if you know the board, it‚Äôs hard to know who‚Äôs winning\u003c/td\u003e\n          \u003ctd\u003eHumans can‚Äôt easily assign a numeric score to a mid-game position\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eValue network\u003c/strong\u003e predicts win probability (reduces \u003cem\u003edepth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"alphago\"\u003e\u003cstrong\u003eAlphaGo\u003c/strong\u003e\u003c/h2\u003e\n\u003caside\u003e\n\u003cp\u003eImagine AlphaGo is a \u003cem\u003esmart player\u003c/em\u003e who has:\u003c/p\u003e","title":"Mastering the game of Go with MCTS and Deep Neural Networks"},{"content":"Abstract What‚Äôs the Reference-based Super-resolution (RefSR) Network:\nSuper-resolves a low-resolution (LR) image given an external high-resolution (HR) reference image The reference image and LR image share similar viewpoint but with significant resolution gap (8√ó). Solve the problem Existing RefSR methods work in a cascaded way such as patch matching followed by synthesis pipeline with two independently defined objective functions Divide the image into many small patches (like tiny squares), each patch is compared with a reference image to find its most similar region. But every patch makes its decision independently. ‚Üí Inter-patch misalignment Because of the small misalignments, the grid of patch boundaries in the final image shows. ‚Üí Grid artifacts Old methods trained two steps separately ‚Üí Inefficient training The challenge large-scale (8√ó) super-resolution problem the spatial resolution is increased by 8 times in each dimension (width and height). So the total number of pixels increases from 8√ó8 to 64√ó64. patch matching ‚Üí warping Structure:\nimage encoders extract multi-scale features from both the LR and the reference images cross-scale warping layers spatially aligns the reference feature map with the LR feature map warping module originated from spatial transformer network (STN) fusion decoder aggregates feature maps from both domains to synthesize the HR output Scale Resolution (relative) Example size What it focuses on Scale 0 √ó1 (full resolution) 512√ó512 Fine details (small shifts) Scale 1 √ó2 smaller 256√ó256 Medium motions Scale 2 √ó4 smaller 128√ó128 Larger motions Scale 3 √ó8 smaller 64√ó64 Very large motions Scale 4‚Äì5 √ó16, √ó32 smaller 32√ó32, 16√ó16 Extremely coarse view (too little detail) Result Using cross-scale warping, our network is able to perform spatial alignment at pixel-level in an end-to-end fashion, which improves the existing schemes both in precision (around 2dB-4dB) and efficiency (more than 100 times faster).\nspatial alignment at pixel-level ‚Üí precision and efficiency precision efficiency 1. Introduction The two critical issues in RefSR:\nImage correspondence between the two input images High resolution synthesis of the LR image. The ‚Äòpatch maching + synthesis‚Äô pipeline ‚Üí the end-to-end CrossNet ‚Üí results comparisons.\nüí° Flow estimator Input: feature maps from LR and Ref encoders.\nComputation:\nThe module compares these features (using convolutions) and learns to predict displacement vectors (optical flow).\nOutput: a flow map $F(x, y) = (\\Delta x, \\Delta y)$.\nUse: the warping layer applies this flow map to the reference feature map:\n$$ \\tilde{R}(x, y) = R(x + \\Delta x, y + \\Delta y) $$\nso the warped reference aligns with the LR image.\nNon-rigid deformation: when an object changes its shape or structure in the image (for example, bending, twisting, or changing due to different camera angles).\nRigid = only simple shifts, rotation, or scaling. Non-rigid = more complex distortions ‚Äî like bending, stretching, or perspective change. Grid artifacts: visible blocky or checker-like patterns that appear because the image was reconstructed from many small, rigid square patches that don‚Äôt align smoothly.\nGrid artifacts occur when an image is reconstructed from many small square patches that don‚Äôt align perfectly at their borders. The Laplacian is a mathematical operator that measures how much a pixel value differs from its surroundings.\nIn other words, it tells you where the image changes quickly ‚Äî that‚Äôs usually at edges or texture details.\n2. Related Work Multi-scale deep super resolution we employ MDSR as a sub-module for LR images feature extraction and RefSR synthesis.\nMDSR stands for Multi-scale Deep Super-Resolution Network Used for Feature extraction ‚Üí understanding what‚Äôs in the LR image RefSR synthesis ‚Üí combining LR and reference features to output the high-resolution result Warping and synthesis We follow such ‚Äúwarping and synthesis‚Äù pipeline. However, our approach is different from existing works in the following ways:\nOur approach performs multi-scale warping on feature domain at pixel-scale which accelerates the model convergence by allowing flow to be globally updated at higher scales. a novel fusion scheme is proposed for image synthesis. concatenation, linearly combining images 3. Approach 3.1 Fully Conv Cross-scale Alignment Module It is necessary to perform spatial alignment for reference image, since it is captured at different view points from LR image.\nCross-scale warping We propose cross-scale warping to perform non-rigid image transformation.\nOur proposed cross-scale warping operation considers a pixel-wise shift vector ( V ):\n$$ I_o = warp(y_{Ref}, V) $$\nwhich assigns a specific shift vector for each pixel location, so that it avoids the blocky and blurry artifacts.\nüí° Pixel-wise shift vector (V) ‚Üí patch matching\n( V ) represents a flow field, where each pixel gets its own small movement vector (Œîx, Œîy). A flow field is a map (like a vector field) that assigns a motion vector to every pixel in the image. So instead of moving the entire image or patch, CrossNet can move each pixel individually ‚Äî very flexible. The equation:\n$$ I_o = warp(y_{Ref}, V) $$\nmeans:\nThe output image ( $I_o$ ) is generated by warping the reference image ( $y_{Ref}$ ) according to the flow field ( $V$ ).\nEach pixel in ( $y_{Ref}$ ) is shifted by its corresponding vector in ( $V$ ).\nCross-scale flow estimator Purpose: Predict pixel-wise flow fields to align the upsampled LR image with the HR reference.\nModel: Based on FlowNetS, adapted for multi-scale correspondence.\nInputs:\n$I_{LR‚Üë}$ : LR image upsampled by MDSR (SISR) $I_{REF}$ : reference image Outputs: Multi-scale flow fields\n${V^{(3)}, V^{(2)}, V^{(1)}, V^{(0)}}$ (coarse ‚Üí fine).\nModification: √ó4 bilinear upsampling with ‚Üí two √ó2 upsampling modules + skip connections + deconvolution ‚Üí finer, smoother flow prediction.\nAdvantage:\nCaptures both large and small displacements; Enables accurate, non-rigid alignment Reduces warping artifacts. üí° How it works (coarse-to-fine refinement)\nThe coarse flow field (V¬≥) roughly aligns big structures. The next flow (V¬≤) refines alignment for medium details. The fine flows (V¬π, V‚Å∞) correct small local misalignments and textures. These flow fields are combined hierarchically ‚Äî like zooming in step-by-step to improve precision. 3.2 End-to-end Network Structure Network structure of CrossNet\nüí° Network:\na LR image encoder a reference image encoder a decoder ‚Üí U-Net LR image Encoder Goal: Extract multi-scale feature maps from the low-resolution (LR) image for alignment and fusion.\nStructure:\nUses a Single-Image SR (SISR) upsampling to enlarge the LR image first. Then applies 4 convolutional layers (5√ó5 filters, 64 channels). Each layer creates a feature map at a different scale (0 ‚Üí 3). Stride = 1 for the first layer, stride = 2 for deeper ones (downsampling by 2). Output:\nA set of multi-scale LR feature maps. $$ F_{LR}^{(0)}, F_{LR}^{(1)}, F_{LR}^{(2)}, F_{LR}^{(3)} $$\nActivation: ReLU (œÉ).\nReference image encoder Goal: Extract and align multi-scale reference features from the HR reference image.\nStructure:\nUses the same 4-scale encoder design as the LR encoder. Produces feature maps . $$ {F_{REF}^{(0)}, F_{REF}^{(1)}, F_{REF}^{(2)}, F_{REF}^{(3)}}¬†$$\nLR and reference encoders have different weights, allowing complementary feature learning. Alignment:\nEach reference feature map $F_{REF}^{(i)}$ is warped using the cross-scale flow $V^{(i)}$. This generates spatially aligned reference features $$ \\hat{F}{REF}^{(i)} = warp(F_{REF}^{(i)}, V^{(i)}) $$\nDecoder Goal: Fuse the low-resolution (LR) features and warped reference (Ref) features across multiple scales to reconstruct the super-resolved (SR) image. Structure Overview The decoder follows a U-Net‚Äìlike architecture. It performs multi-scale fusion and up-sampling using deconvolution layers. Each scale combines: The LR feature at that scale $F_{LR}^{(i)}$, The warped reference feature $\\hat{F}_{REF}^{(i)}$, The decoder feature from the next coarser scale $F_{D}^{(i+1)}$ (if available). üí° Equations (Eq. 6)\nFor the coarsest scale (i = 3):\n$$ F_{D}^{(3)} = \\sigma \\big( W_{D}^{(3)} \\star (F_{LR}^{(3)}, \\hat{F}{REF}^{(3)}) + b{D}^{(3)} \\big) $$\nFor finer scales (i = 2, 1, 0):\n$$ F_{D}^{(i)} = \\sigma \\big( W_{D}^{(i)} \\star (F_{LR}^{(i+1)}, \\hat{F}{REF}^{(i+1)}, F{D}^{(i+1)}) + b_{D}^{(i)} \\big) $$\nwhere:\n$\\star$ denotes the deconvolution operation (transposed convolution). $W_{D}^{(i)}$: deconvolution filters (size 4√ó4, 64 filters, stride 2). $\\sigma$: activation function (ReLU). $b_{D}^{(i)}$ : bias term. Thus, features are progressively upsampled and refined from coarse ‚Üí fine.\nüí° Post-Fusion (Eq. 7)\nAfter obtaining the final decoder feature map $F_{D}^{(0)}$,\nthree convolutional layers (filter size 5√ó5) are applied to refine and generate the SR image:\n$$ \\begin{aligned} F_1 \u0026amp;= \\sigma(W_1 * F_{D}^{(0)} + b_1), \\ F_2 \u0026amp;= \\sigma(W_2 * F_1 + b_2), \\ I_p \u0026amp;= \\sigma(W_p * F_2 + b_p), \\end{aligned} $$\nwhere:\n$W_1, W_2, W_p$: convolution filters with channel numbers {64, 64, 3}, $I_p$: final super-resolved output image. 3.3 Loss Function Goal: Train CrossNet to generate super-resolved (SR) outputs $I_p$ close to the ground-truth HR images $I_{HR}$.\nFormula:\n$$ L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{s} \\rho(I_{HR}^{(i)}(s) - I_{p}^{(i)}(s)) $$\nPenalty: Uses the Charbonnier loss\n$$ \\rho(x) = \\sqrt{x^2 + 0.001^2} $$\nA smooth, robust version of L1 loss that reduces the effect of outliers.\nVariables:\n$N$: number of training samples $s$: pixel (spatial location) $i$: training sample index 4. Experiment 4.1 Dataset Dataset: The representative Flower dataset and Light Field Video (LFVideo) dataset. Each light field image has: 376 √ó 541 spatial samples 14 √ó 14 angular samples Model training: Each light field image has: 320 √ó 512 spatial samples 8 √ó 8 angular samples Test generalization: Datasets: Stanford Light Field dataset Scene Light Field dataset During testing, they apply the big input images using a sliding window approach: Window size: 512√ó512 Stride: 256 4.2 Evaluation Training setup: Trained for 200K iterations on Flower and LFVideo datasets. Scale factors: √ó4 and √ó8 super-resolution. Learning rate: 1e-4 / 7e-5 ‚Üí decayed to 1e-5 / 7e-6 after 150K iterations. Optimizer: Adam (Œ≤‚ÇÅ = 0.9, Œ≤‚ÇÇ = 0.999). Comparisons: Competes with RefSR methods (SS-Net, PatchMatch) and SISR methods (SRCNN, VDSR, MDSR). Evaluation metrics: PSNR, SSIM, and IFC on √ó4 and √ó8 scales. Reference images from position (0,0); LR images from (1,1) and (7,7). Results: CrossNet achieves 2‚Äì4 dB PSNR gain over previous methods. CrossNet consistently outperforms the resting approaches under different disparities, datasets and scales. Quantitative evaluation of the sota SISR and RefSR algorithms, in terms of PSNR/SSIM/IFC for scale factors √ó4 and √ó8 respectively.\nMetric Meaning PSNR (Peak Signal-to-Noise Ratio) Measures reconstruction accuracy (higher = clearer, less error). SSIM (Structural Similarity Index) Measures structural similarity to the ground truth (higher = more visually similar). IFC (Information Fidelity Criterion) Evaluates how much visual information is preserved (higher = better detail). Generalization During training, apply a parallax augmentation procedure this means they randomly shift the reference image by ‚Äì15 to +15 pixels both horizontally and vertically. The purpose is to simulate different viewpoint disparities (parallax changes) make the model more robust to viewpoint variations. They initialize the model using parameters pre-trained on the LFVideo dataset, Then re-train on the Flower dataset for 200 K iterations to improve generalization. The initial learning rate is 7 √ó 10‚Åª‚Åµ, which decays by factors 0.5, 0.2, 0.1 at 50 K, 100 K, 150 K iterations. Table 2 and 3 show PSNR comparison results: Their re-trained model (CrossNet) outperforms PatchMatch [11] and SS-Net [2] on both Stanford and Scene Light Field datasets. The improvement is roughly +1.79 ‚Äì 2.50 dB (Stanford) and +2.84 dB (Scene LF dataset). Efficiency within 1 seconds machine: 8 Intel Xeon CPU (3.4 GHz) a GeForce GTX 1080 GPU 4.3 Discussion Flows at scale 0‚Äì3 were coherent (good). Flows at scale 4‚Äì5 were too noisy ‚Äî because those very small maps (like 32√ó32) lost too much information. Training setup:\nTrain both CrossNet and CrossNet-iw with the same procedure: Pre-train on LFVideo dataset Fine-tune on Flower dataset 200 K iterations total Additionally, CrossNet-iw pretraining: Pre-train only the flow estimator WS-SRNet using an image warping task for 100 K iterations. Train the whole network jointly for another 100 K iterations. FlowNetS+ adds extra upsampling layers\nIn the original FlowNetS: The final flow map is smaller than the input (maybe ¬º or ¬Ω resolution). This is fine for rough alignment but loses small motion details. In FlowNetS+: They add extra upsampling layers so the final flow map is finer (closer to full size). That‚Äôs why it aligns better ‚Äî it can describe tiny pixel movements more accurately. Downsampling = make smaller. Upsampling = make bigger.\n","permalink":"/posts/crossnet_an_end-to-end_reference-based_super_resol/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003eWhat‚Äôs the \u003cstrong\u003eReference-based Super-resolution (RefSR)\u003c/strong\u003e Network:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSuper-resolves a \u003cstrong\u003elow-resolution (LR)\u003c/strong\u003e image given an external \u003cstrong\u003ehigh-resolution (HR) reference image\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe reference image and LR image share similar viewpoint but with significant resolution gap (8√ó).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"solve-the-problem\"\u003eSolve the problem\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eExisting RefSR methods work in a cascaded way such as \u003cstrong\u003epatch matching\u003c/strong\u003e followed by \u003cstrong\u003esynthesis pipeline\u003c/strong\u003e with two independently defined objective functions\n\u003cul\u003e\n\u003cli\u003eDivide the image into many small \u003cstrong\u003epatches\u003c/strong\u003e (like tiny squares), each patch is compared with a \u003cstrong\u003ereference image\u003c/strong\u003e to find its most similar region.\u003c/li\u003e\n\u003cli\u003eBut every patch makes its decision independently. ‚Üí \u003cstrong\u003eInter-patch misalignment\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eBecause of the small misalignments, the \u003cstrong\u003egrid\u003c/strong\u003e of patch boundaries in the final image shows. ‚Üí \u003cstrong\u003eGrid artifacts\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eOld methods trained \u003cstrong\u003etwo steps separately ‚Üí Inefficient training\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThe challenge large-scale (8√ó) super-resolution problem\n\u003cul\u003e\n\u003cli\u003ethe \u003cstrong\u003espatial resolution is increased by 8 times\u003c/strong\u003e in each dimension (width and height).\u003c/li\u003e\n\u003cli\u003eSo the total number of pixels increases from 8√ó8 to 64√ó64.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003epatch matching ‚Üí warping\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStructure:\u003c/p\u003e","title":"CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping"},{"content":" Chain of thought:\nA series of intermediate natural language reasoning steps that lead to the final output ‚Äî significantly improves the ability of large language models to perform complex reasoning. Chain-of-thought prompting:\nA simple and broadly applicable method for enhancing reasoning in language models. Improving performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Two simple methods to unlock the reasoning ability of LLMs Thinking in steps helps:\nWhen the model explains each step before the answer, it understands the problem better.\nLearning from examples:\nWhen we show a few examples with step-by-step answers, the model learns to do the same.\nFew-shot prompting means giving the model a few examples in the prompt to show it how to do a task before asking it to solve a new one.\nWhat this paper do Combine these two ideas help the language models think step by step to generate a clear and logical chain of ideas that shows how they reach the final answer. Given a prompt that consists of triples: \u0026lt;input, chain of thought, output\u0026gt; Why this method is important It doesn‚Äôt need a big training dataset. One model can do many different tasks without extra training. Greedy decoding: let the model choose the most likely next word each time\nResult It only works well for giant models, not smaller ones. It only works well for more-complicated problems, not the simple ones. Chain-of-thought prompting with big models gives results as good as or better than older methods that needed finetune for each task. Ablation study: It‚Äôs like testing which parts of your model really matter.\nRelated work Some methods make the input part of the prompt better ‚Äî for example, adding clearer instructions before the question. But this paper does something different (orthogonal): it improves the output part, by making the model generate reasoning steps (chain of thought) before giving the final answer. ","permalink":"/posts/chain-of-thought-prompting/","summary":"\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eChain of thought:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA series of intermediate \u003cstrong\u003enatural language reasoning steps\u003c/strong\u003e that lead to the final output ‚Äî significantly improves the ability of large language models to perform complex reasoning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eChain-of-thought prompting:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple and broadly applicable method for\n\u003cul\u003e\n\u003cli\u003eenhancing reasoning in language models.\u003c/li\u003e\n\u003cli\u003eImproving performance on a range of \u003cstrong\u003earithmetic\u003c/strong\u003e, \u003cstrong\u003ecommonsense\u003c/strong\u003e, and \u003cstrong\u003esymbolic\u003c/strong\u003e\nreasoning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003chr\u003e\n\u003ch2 id=\"two-simple-methods-to-unlock-the-reasoning-ability-of-llms\"\u003eTwo simple methods to unlock the reasoning ability of LLMs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eThinking in steps helps:\u003c/strong\u003e\u003c/p\u003e","title":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"},{"content":"Initial Impression üëâüèº The light field refres to the representation of a scene. Unlike the tranditional 2D image, it adds an extra dimention: the direction of light. This means each image captures not only the length and width, but also the direction of light.\nTraditional image: 2D spatially (width, height), but each pixel carries a 3-value vector (RGB, 3 channels for color). Light filed(width, heidht, X-direction, Y-direction): 4D images, but each pixel carries a 3-value vector. As a result, light field data is massive and challenging to process.\nTo address it, researchers have turned into machine learning to efficiently handle and enhance light field imaging.\nThe paper mentions 4 main areas that AI can helps:\nDepth estimation Reconstruction Compression Quelity evaluation Abstract üëâüèº Content:\nBackground Motivation Research focus Paper content the existing learning-based solutions frameworks evaluation methods datasets future research directions Background Traditional cameras capture only 2D images. Light field imaging records the direction of light rays, enabling realistic and immersive 3D-like representations. Motivation Light field data provides richer visual information but comes with large data volume and high computational cost. Machine learning and deep networks offer efficient and intelligent ways to process this data. Research Focus Learning-based methods are applied to:\nDepth estimation Reconstruction and super-resolution Compression and quality enhancement Paper Goals The paper surveys existing learning-based techniques, summarizes the most promising frameworks, reviews current datasets, and evaluation methods and outlook for future research directions.\n1. Introduction üëâüèº Content:\nConcept and potential of light field imaging Market growth and application fields Technical challenges and processing tasks Shift toward learning-based solutions Purpose and structure of the paper Concept and Potential Light field imaging is a promising 3D imaging technology that records light rays traveling through every point in space and in every direction. Unlike 2D photography, it captures angular information, providing a sense of depth, realism, and immersion. This enables photo-realistic rendering and supports 6-DoF (Degrees of Freedom) experiences for next-generation immersive media, broadcasting, and gaming. Market and Applications The light field market is expanding rapidly, driven by glasses-free 3D displays and multi-view visualization systems. It supports a range of applications such as virtual reality, augmented reality, 3D reconstruction, and computational photography. Challenges and Processing Tasks High-dimensional light field data introduces issues like data redundancy, storage complexity, and inter-view correlation. Essential processing tasks include: Spatial and angular super-resolution to enhance image quality Compression algorithms for efficient data storage and transmission Depth estimation for 3D scene reconstruction These tasks are more complex than traditional 2D image processing due to added angular dimensions. Shift Toward Learning-Based Solutions Traditional geometry-based methods struggle with large datasets and occlusion problems. Deep learning and data-driven frameworks now dominate, improving efficiency and performance in reconstruction, compression, and depth estimation. Learning frameworks enable automation and scalability, addressing the computational challenges of high-dimensional data. Purpose and Structure of the Paper This review provides a comprehensive overview of learning-based solutions for light field imaging. It discusses: Fundamentals of light field imaging and data acquisition Key processing tasks and related learning frameworks Benchmark datasets and evaluation methods Current challenges and future research directions The goal is to summarize progress, identify open issues, and provide a roadmap for future research in learning-based light field processing. 2. Light field imaging background üëâüèº Content:\nLight field fundamentals Light field acquisition Light field visualization 2.1 Light Field Fundamentals üëâüèº Content:\nConcept of the Plenoptic Function Dimensional Reduction Light Field Representation Concept of the Plenoptic Function To reproduce realistic 3D scenes, cameras must capture light from many viewpoints.\nA light field describes all light rays in 3D space ‚Äî their positions, directions, colors, and intensity.\nThe complete description is given by the plenoptic function, a 7-dimensional (7D) function:\n$$ P(Œ∏, œÜ, Œª, œÑ, V_x, V_y, V_z) $$\nwhich includes direction, wavelength, time, and position of every ray in space.\nDimensional Reduction Capturing the full 7D plenoptic function is practically impossible. Under constant lighting and static scenes, wavelength (Œª) and time (œÑ) can be ignored. This simplifies the representation to a 5D function: $$ P(Œ∏, œÜ, V_x, V_y, V_z) $$\nThe 5D form forms the foundation for Neural Radiance Fields (NeRF), while further simplification leads to a 4D light field representation. Light Field Representation Two-Plane Parameterization\nThe 4D light field assumes light rays travel in straight lines. Each ray is defined by its intersection with two parallel planes: (u, v) ‚Üí spatial coordinates on the image/focal plane (Œ©) (s, t) ‚Üí coordinates on the camera plane (Œ†) The resulting function: $$ P(u, v, s, t) $$\ndefines the light field in terms of spatial and angular information. This two-plane parameterization makes light fields easier to store, process, and compress using 2D image arrays. A 4D light field can be expressed as an array of 2D images indexed by (u, v) for spatial coordinates and (s, t) for different viewpoints. This enables the use of standard 2D codecs for compression and native 2D algorithms for processing. Epipolar Plane Image (EPI)\nEpipolar Plane Image (EPI), a 2D slice of the light field showing spatial‚Äìangular relationships. Looks like a single 2D picture made by stacking the same pixel row from all those camera views. Coordinates ‚Üí (u, s) or (v, t) (only one spatial + one view direction) In the EPI, each object in the scene becomes a slanted line: If an object is close, its line is steeper (because it moves more between views). If it‚Äôs far, its line is flatter (moves less). Lines in the EPI correspond to scene depth ‚Äî analyzing their slopes allows depth estimation and 3D reconstruction. Epipolar Plane Image (EPI)\n2.2 Light Field Acquisition üëâüèº Acquisition methods:\nSingle-Camera Systems Multi-Camera Arrays Other Methods Computer-generated light fields Handheld or SLAM-based systems LiDAR-assisted capture Overview Light fields can be acquired by single plenoptic cameras, camera arrays, or synthetic and LiDAR-based systems, each balancing data density, resolution, and complexity.\nSingle-Camera Systems Plenoptic (lenslet-based) cameras use microlens arrays to capture dense angular information in one shot. Examples: Raytrix and Lytro cameras. Lytro ‚Üí The Lytro camera captures a 4-D light field by recording many tiny viewpoint images through its microlens array ‚Äî those images form the SAI stack. Multi-Camera Arrays Arrays of monocular cameras (planar or spherical) capture scenes from different viewpoints. The camera layout defines the angular sampling and overall field of view. Other Methods Computer-generated light fields provide accurate depth maps for benchmarking. Handheld or SLAM-based systems reconstruct light fields from multiple frames. LiDAR-assisted capture combines sensors and cameras for precise, automated 3D data. 2.3 Light field visualization üëâüèº Content:\nGoal and use cases Visualization approaches Display principles and challenges Perceptual limitations Goal and Use Cases The main goal is to provide a true 3D visual experience, essential for immersive and interactive applications. Visualization may be passive (no interaction) or active (viewer can move or rotate objects). Used in areas like medical imaging, VR/AR, and 3D display systems. Visualization Approaches Based on the 4D light-field representation combining spatial and angular data. Passive use cases render fixed views; active use cases synthesize new views via view interpolation. (View interpolation means creating new viewpoints images that were never actually captured by a real camera, but are mathematically generated from nearby real ones. Rendering quality strongly influences perceptual realism. Display Principles and Challenges Two key properties: Angular resolution (baseline between views) the baseline = the distance between two camera viewpoints. Small baseline ‚Üí cameras are close together Wide baseline ‚Üí cameras are far apart Spatial resolution (image detail) 3D displays must reproduce directional light rays accurately to recreate depth and realism. Capturing dense samples and rendering multiple view angles remain computationally demanding. Perceptual Limitations Standard 2D displays lack realistic cues like vergence and accommodation, limiting immersion. Light-field displays address this by replicating real light rays to the viewer‚Äôs eyes but face: Finite ray sampling Vergence‚Äìaccommodation conflict Restricted field of view (FoV) Horizontal- or vertical-parallax-only designs causing viewing discomfort. 3. Learning‚Äëbased light field processing üëâüèº Content:\nDepth estimation Autoencoders Stereo matching and refinement End‚Äëto‚Äëend feature extraction and disparity regression Light field reconstruction Spatial super‚Äëresolution Single‚Äëview super‚Äëresolution and refinement End‚Äëto‚Äëend residual learning Angular super‚Äëresolution EPI super‚Äëresolution Depth estimation and warping Multi‚Äëplane image generation Spatio‚Äëangular reconstruction Neural scene representation Compression Learning‚Äëbased view synthesis on the decoder side Learning‚Äëbased view synthesis on the encoder and decoder sides End‚Äëto‚Äëend light field compression architecture Other light field imaging applications This section summarizes the most prominent light field processing tasks studied in the literature and highlights the learning-based imaging techniques deployed for each processing task.\nTerms and explanation:\nTerm Simple meaning Example / Analogy EPI volume Stack of many light-field slices showing how points move across views Like stacking many thin image strips to form a 3D block Stereo view Two or more photos of the same scene from different angles Like your left and right eye views SAI stack Group of small 2D images from a light field camera Like a grid of mini photos from slightly different directions Depth map Image showing distance of each pixel (white = near, black = far) Like a 3D scanner‚Äôs output Disparity volume 3D data showing pixel shifts between views Large shift ‚Üí close object, small shift ‚Üí far object Details about the difference between Light Field representation:\nTerm Relation to Two-Plane Parameterization Simple meaning EPI A slice through the Two-Plane representation. Shows pixel movement (as slanted lines) between multiple views ‚Äî used for depth estimation. Stereo view it‚Äôs a simpler / smaller subset of the Two-Plane model (only 2 views). Like taking only the left and right images from the light field. SAI stack it‚Äôs directly sampled from the Two-Plane model (a grid of (s, t) views, each with its own (u, v) image). Many small 2D images from slightly different viewpoints ‚Äî a practical way to store the light field. Relationship between Disparity volume and Depth map:\nConcept Type What it represents Used for Disparity volume 3D data (x, y, disparity) All possible matching shifts between views Intermediate step (to find the best match) Depth map 2D image (x, y), 3D data (x, y, depth) Actual distance of each pixel Final result Disparity and depth are inversely related:\n$$ \\text{Depth} = \\frac{f \\times B}{\\text{Disparity}} $$\nwhere:\n(f) = focal length of the camera, (B) = distance between two camera views (baseline). So:\nLarge disparity (big shift) ‚Üí close object. Small disparity (tiny shift) ‚Üí far object.\nDepth map\n3.1 Depth estimation üëâüèº Three main approaches:\nAutoencoders Stereo matching and refinement End-to-end feature extraction and disparity regression Depth estimation model architecture\nOverview Goal: Estimate the distance of each pixel from the camera to recover the scene‚Äôs 3D structure. Light field imaging enables capturing a scene from multiple viewpoints so depth information is implicitly encoded in the light field representation and can be acquired by computing the inter-view pixel disparity information. disparity volume ‚Üí depth map Main challenges: occlusions, non-Lambertian surfaces, and texture-less regions, which make accurate estimation difficult. A Lambertian surface is an ideal matte surface ‚Äî it reflects light equally in all directions. That means no matter where you look from, the brightness of that point stays the same. Recent progress focuses on learning-based approaches, achieving higher accuracy than traditional geometry-based methods. Autoencoders Take an EPI volume ‚Üí compress it (encoder) ‚Üí learn the hidden features ‚Üí expand it (decoder).\nClassical method:\nHeber \u0026amp; Pock: five-block CNN estimating line orientation in EPIs; Orientation refers to the direction each camera is facing. later extended to a U-shaped encoder‚Äìdecoder, Further to U-shaped encoder‚Äìdecoder with skip connections and 3D filters. Step Explanation Input Horizontal and vertical EPI volumes (from the light field). Each EPI shows slanted lines ‚Äî the slope encodes depth. Model A 5-block CNN that scans small ‚Äúwindows‚Äù (patches) of the EPI. Each CNN layer extracts features and estimates the orientation (slope) of the EPI lines. Later versions evolved into a U-shaped encoder‚Äìdecoder (U-Net) for better reconstruction. Output A depth map, where every pixel‚Äôs value = estimated distance from the camera. Alperovich et al.:\nan autoencoder that encodes horizontal and vertical EPI stacks simultaneously using six stages of residual blocks to improve robustness. Then, the compressed representation is expanded using three decoder pathways to address the disparity, diffusion, and specularity estimation problems. Step Explanation Input Combined horizontal + vertical EPI stacks from the light field. Model An autoencoder with: 6 residual blocks (for stronger feature extraction) in the encoder, and 3 decoder branches (to handle disparity, diffusion, specularity). It compresses the light field into a latent code, then reconstructs richer outputs. Output A disparity volume ‚Äî a 3D array where each pixel position stores multiple disparity hypotheses (possible shifts). From this volume, a final depth map can be derived later. Analysis:\nPros: captures compact depth features, handles EPI geometry directly. Cons: computationally heavy; limited to 2D EPI slices, less effective in occluded regions. Stereo Matching and Refinement Computes disparity between SAIs using neural stereo-matching networks.\nTypical pipeline:\nCoarse disparity estimation via networks like FlowNet 2.0 or encoder‚Äìdecoder CNNs. Refinement using residual or occlusion-aware learning to correct depth errors. Examples:\nRogge et al. (belief propagation + residual refinement);\nGuo et al. (encoder‚Äìdecoder concatenation of SAIs).\nAnalysis:\nPros: exploits full 4D light-field correlations, good for complex geometry. Cons: high computation cost; sensitive to reflections and non-Lambertian surfaces. End-to-End Feature Extraction and Disparity Regression Fully end-to-end CNNs learn features and regress depth directly.\nMethods:\nEpinet: Horizontal, vertical, and diagonal SAI stacks ‚Üí Multi-stream CNN feature extraction ‚Üí Regression network ‚Üí Depth map Leistner et al.: Vertical \u0026amp; horizontal SAI stacks ‚Üí Siamese U-Net ‚Üí Autoencoder regression module ‚Üí Classification + regression fusion ‚Üí Depth map Two-stream CNN: Horizontal \u0026amp; vertical EPIs ‚Üí Multi-scale feature extraction (four convolutional stages) ‚Üí Feature concatenation ‚Üí Multi-label regression ‚Üí Depth map Zhu et al.: Focal stacks + center view + EPIs ‚Üí Hybrid feature extraction ‚Üí Fully connected + softmax layers ‚Üí Pixel-wise disparity classification ‚Üí Depth map Tsai et al.: Multi-view SAIs ‚Üí Residual blocks + spatial pyramid pooling ‚Üí Cost volume construction + attention module ‚Üí Disparity regression ‚Üí Depth map Multi-scale Cost-Volume Method: Shifted SAI feature maps (multi-disparity levels) ‚Üí 4D cost volume (low memory footprint) ‚Üí Multi-scale feature extraction ‚Üí Regression ‚Üí Depth map Analysis:\nPros: highest accuracy, unified optimization of feature + disparity learning. Cons: large data/training demand; reduced performance on wide-baseline scenes. üëâüèº Summary:\nDepth estimation for the wide baseline scenario, with an acceptable trade-off between accuracy and computation, is still an open research problem.\n3.2 Light field reconstruction üëâüèº Content:\nSpatial super‚Äëresolution Single‚Äëview super‚Äëresolution and refinement End‚Äëto‚Äëend residual learning Angular super‚Äëresolution EPI super‚Äëresolution Depth estimation and warping Multi‚Äëplane image generation Spatio‚Äëangular reconstruction Neural scene representation To enable higher spatial and angular resolutions, the development of light field reconstruction/ super-resolution (SR) methods has gained significant attention.\nSpatial SR = make each image (each view) sharper ‚Äî more pixels, more detail. Angular SR = make more viewpoints ‚Äî fill in missing views between existing ones. üí° Spatial super-resolution (SR) focuses on improving the static spatial resolution ‚Äî that is, the sharpness, clarity, and detail of each individual sub-aperture image (still view).\nAngular consistency ensures smooth and coherent transitions between different viewpoints, maintaining stable motion perception and correct 3D geometry when the view changes or the scene is refocused.\nLight-field SR must improve both:\nSpatial resolution ‚Üí each view looks higher spatial resolution. Angular consistency ‚Üí all views agree about geometry and depth. System Spatial resolution (image sharpness) Angular resolution (number of viewpoints) Baseline Notes Plenoptic camera Low High (dense) Narrow Compact but blurry Camera rig High Low (sparse) Wide Sharp but heavy and complex Spatial super‚Äëresolution Examples of architectures for the spatial, angular, and spatio-angular super-resolution (SR) frameworks.\na ‚Üí Single-view SR using single image super-resolution (SISR) network and inter-view enhancement, b ‚Üí end-to-end residual learning, c ‚Üí warping and residual learning for refinement, d ‚Üí multi-plane image generation, e ‚Üí residual learning using 4D CNNs and refinement, **** f ‚Üí GAN-based method Single‚Äëview super‚Äëresolution and refinement\nStep Explanation Input (SAI stack LR) SAI stack LR: all the sub-aperture images captured by the light-field camera. low-resolution ‚Äî each view is blurry and lacks details (because each microlens only captures a small number of pixels). For example, 9√ó9¬†SAIs¬†of¬†60√ó60¬†pixels. SISR Single-Image Super-Resolution: This block applies a 2D super-resolution network (e.g., VDSR) to each SAI individually to ****improve the spatial resolution. HR(init)LF High-Resolution (initial) Light Field: Each image looks sharper, but the views may no longer be perfectly aligned (angular inconsistency). EPI EPIs (Epipolar Plane Images) are extracted. EPIs capture the geometric relationships between views ‚Äî how pixels shift across viewpoints (slanted lines). Refinement module This is a learning network (often a CNN) that analyzes the EPIs to find and correct misalignments between neighboring SAIs. Making sure all views agree in depth and structure. Residual addition Combine sharpness from the first stage and consistency from the second stage. Output (HR LF) High-Resolution Light Field: The final output light field has: High spatial resolution (sharp individual views), and High angular consistency (smooth geometry across views). For example, 9√ó9¬†SAIs¬†of¬†240√ó240¬†pixels. End‚Äëto‚Äëend residual learning\nStep Explanation Input (SAI stack) SAI stacks (Horizontal, Vertical, Diagonal, Center View), Center view ‚Üí the middle image, used as a geometric reference Feature Extraction Each SAI stack is passed through a feature extraction CNN. Extract features from each SAI direction, like ‚Äúwhat the object looks like‚Äù and ‚Äúhow it moves between views.‚Äù Feature Integration \u0026amp; Processing The feature maps from all directions (horizontal, vertical, diagonal, etc.) are merged or fused here. Upsampling Network Increases the spatial resolution (i.e., number of pixels). Output (HR LF) The final output is a high-resolution light field ‚Äî a grid of SAIs that are: Spatially sharper and Angularly consistent. Upsampling means making an image larger ‚Äî that is, increasing its resolution by creating more pixels.\nAngular super‚Äëresolution Angular Super-Resolution (SR) means synthesizing new in-between views to make the light field smoother and more complete. (view synthesis)\nEPI super‚Äëresolution\nAll of these models have the same basic structure:\nStart from a blurry / low-angular-resolution light field (few viewpoints). Use deep networks to predict high-frequency details (the fine geometry and textures missing in the low version). Reconstruct the high-angular-resolution (HR) light field, which includes the new views. Step Explanation Input (SAI stack LR) SAI stack LR: all the sub-aperture images captured by the light-field camera. low-resolution ‚Äî each view is blurry and lacks details (because each microlens only captures a small number of pixels). For example, 9√ó9¬†SAIs¬†of¬†60√ó60¬†pixels. SISR Single-Image Super-Resolution: This block applies a 2D super-resolution network (e.g., VDSR) to each SAI individually to ****improve the spatial resolution. HR(init)LF High-Resolution (initial) Light Field: Each image looks sharper, but the views may no longer be perfectly aligned (angular inconsistency). EPI EPIs (Epipolar Plane Images) are extracted. EPIs capture the geometric relationships between views ‚Äî how pixels shift across viewpoints (slanted lines). Refinement module This is a learning network (often a CNN) that analyzes the EPIs to find and correct misalignments between neighboring SAIs. Making sure all views agree in depth and structure. Residual addition Combine sharpness from the first stage and consistency from the second stage. Output (HR LF) High-Resolution Light Field: The final output light field has: High spatial resolution (sharp individual views), and High angular consistency (smooth geometry across views). For example, 9√ó9¬†SAIs¬†of¬†240√ó240¬†pixels. Depth estimation and warping\nDisparity is the shift of the same object‚Äôs position between two different views. Warping = using disparity to reposition pixels. Step Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Depth Estimator Predicts a depth map (distance information) for each SAI or for the entire light field. It learns how far each pixel is from the camera by analyzing geometric cues across views. (wraps_by_an_amount_that_depends_on_its_depth) Warping Module Uses the estimated depth maps to warp (geometrically align) all SAIs toward a common viewpoint ‚Äî usually the center view. ‚ÄúWarping‚Äù means shifting pixels according to their depth so that corresponding points from different views line up. Refinement Module Fine-tunes the warped images using a CNN. Corrects small errors from imperfect depth estimation or warping. Output (HR LF) The final reconstructed light field: All SAIs are now high-resolution (sharp textures). The views are geometrically aligned (consistent depth perception). Some basics:\nConcept Meaning Why it matters Depth map Tells how far each pixel is Needed to compute correct pixel shift Disparity The actual shift caused by viewpoint change Derived from depth Warping Moves pixels according to disparity Aligns all views Without depth All pixels shift equally ‚Üí wrong alignment Causes blur and ghosting Multi‚Äëplane image generation (MPI generation)\nStep Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Warping Module The Warping Module aligns all SAIs to a reference view (usually the center view) using a range of hypothetical depth planes. Plane-sweep volume After warping, stacks all these reprojected images together ‚Äî forming a plane-sweep volume. Create a 3D data structure that encodes how well each depth hypothesis aligns across views. No explicit depth estimator ‚Äî depth is implicitly encoded in the plane-sweep volume. 3D CNN The 3D convolutional neural network processes the plane-sweep volume to analyze spatial and depth correlations. Multi-plane image A multi-plane image (MPI) ‚Äî a set of 2D images, each representing a scene layer at a specific depth, with color + transparency (Œ±) values. Blending Module Combines (blends) all the multi-plane images (depth layers) into a single coherent high-resolution light field. Output (HR LF) The final light field: All SAIs are now high-resolution and geometrically aligned. Spatio‚Äëangular reconstruction Residual learning using 4D CNNs and refinement\nStep Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Warping Module The Warping Module aligns all SAIs to a reference view (usually the center view) using a range of hypothetical depth planes. 4D CNN A convolutional network that operates directly on the 4D light-field volume. It jointly learns: Spatial features (edges, textures inside each SAI) and Angular features (parallax). The 4D CNN predicts the missing high-frequency details for all views simultaneously. Residual Connection This ‚Äúresidual learning‚Äù means the 4D CNN only learns the difference (the missing fine details) rather than reconstructing the entire image from scratch. Refinement Module Polish the HR LF and ensure angular consistency. Output (HR LF) Output: HR LFThe final output = high-resolution light field ‚Üí same number of views as the input, ‚Üí each view sharper (higher spatial resolution) and smoothly aligned (angular consistency). GAN-based method\nStep Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Generator The generator learns to upsample the SAIs and restore missing details (edges, textures, angular consistency). GAN Discriminator It is trained on real HR light fields (ground truth) and fake HR light fields (from the generator). Its job is to classify them as real or fake. Force the generator to create results that are indistinguishable from real data ‚Äî not just pixel-wise accurate but also visually realistic (better textures, depth edges, lighting). Output (HR LF) The generator alone can take any new LR light field and output an HR version that: has high spatial detail, maintains angular consistency, and looks visually realistic (not over-smoothed). üí° Summary: Limitations of LF Reconstruction Techniques: Early reconstruction methods Slow to run. Trained for fixed view sampling patterns ‚Üí hard to generalize to new setups. Single-view SR methods (Fig. 3a) Each sub-aperture image (SAI) is processed separately. Causes geometric inconsistency between views because inter-view information isn‚Äôt used. Depth-based \u0026amp; warping methods (Fig. 3c) Work better for wide-baseline cases (larger view spacing). Depend heavily on accurate depth maps ‚Üí errors lead to tearing, ghosting, and problems with non-Lambertian (reflective) surfaces. MPI-based methods (Fig. 3d) Memory-hungry and slow to train (often need multiple GPUs and days of training). Model size grows with number of depth planes (larger depth budget ‚Üí bigger model). Can assign wrong opacity to layers ‚Üí causes blurry reconstructions. 4D CNN methods (Fig. 3e) Produce high-quality results, but have high computational cost due to expensive 4D convolutions. GAN-based methods (Fig. 3f) Need large training datasets. Training can suffer from instability and mode collapse (generator producing limited or repetitive outputs). Neural scene representation Researchers started using neural networks to represent 3D scenes which replaces 2D pictures.\nFrom image-based to neural 3D representations\nEarlier works used image-based rendering (combine nearby views). Recent advances use neural networks to represent and render 3D scenes directly. 3D representations can be: Explicit: meshes, voxels, point clouds. Implicit: continuous functions learned by networks + differentiable ray marching. NeRF ‚Äî the core idea\nNeural Radiance Fields (NeRF) represent a scene using an MLP (multi-layer perceptron). The network takes 5D coordinates ‚Üí (x, y, z, Œ∏, œÜ): spatial position + viewing direction. It outputs: Density (geometry) Color (view-dependent radiance). Rendering is done by volume rendering along camera rays. üí° How NeRF learns the scene:\nFeed the network 2D photos of the same scene taken from different camera angles, and you must know exactly where each camera was (its pose = position + orientation). ‚Üí So NeRF knows which pixel in which image corresponds to which ray in 3D space. NeRF renders its current guess (density and color) of those images using its internal 3D representation. It compares them to the real photos (pixel by pixel). It updates its weights using backpropagation to minimize the difference. To render one image (for a camera view):\nFor each pixel, shoot a ray through the 3D scene.\nSample many points along that ray (like tiny steps through space).\nAt each point, ask the network for its color and density.\nCombine (accumulate) all samples along the ray using a differentiable volume rendering formula:\n$$ C = \\sum_i T_i (1 - e^{-\\sigma_i \\Delta_i}) c_i $$\nwhere:\nC = final pixel color, $\\sigma_i$ = density, $c_i$ = color, $T_i$ = how much light passes through before reaching this point. This process is known as differentiable ray marching.\nSo the process works like this:\nCollect multiple 2D photos of the same scene, taken from different camera angles (with known position and orientation: x, y, z, Œ∏, œÜ). For each pixel in each photo, sample many 3D points along the corresponding camera ray. For each sampled point, use the network to predict its color and density (or depth). Aggregate the information from all sampled points to estimate the final color of the pixel. Compare the predicted image to the real photo, pixel by pixel, and update the model based on the error. NeRF uses a coarse network (for rough geometry) and a fine network (for detailed structure). Analysis: Pros: realistic view synthesis. Cons: very slow training and rendering. Methods to improve speed and efficiency\nSeveral approaches speed up NeRF by changing how the scene is represented, they replaced the big neural network with simpler structures:\nMethod Representation Key idea Pros / Cons Voxel-based NeRFs (Fridovich et al.) Sparse voxel grid Store opacity + spherical harmonic coefficients Faster, but memory-heavy TensoRF 4D tensors (low-rank decomposition) Factorize radiance field into compact tensor components Efficient, compact SNeRG Sparse 3D voxel grid with color \u0026amp; features Encodes view-dependent effects Fast rendering; still large memory Octree NeRF (Yu et al.) Octree structure (adaptive voxels) Sample dense regions more finely Faster, but higher memory cost NeX Extended MPI (Multi-Plane Image) Model color as function of viewing angle using spherical bases Better view-dependent rendering Ray-space embedding 4D ray embedding ‚Üí latent space Compact feature embedding Memory efficient but slower rendering KiloNeRF Thousands of tiny MLPs Divide scene into grid cells, each with a small network Great speed, coarse structure 3D Gaussian representation Continuous Gaussians Skip empty-space computation Near real-time rendering Instant-NGP (M√ºller et al.) Hash-based encoding Store features in hash table for fast lookup Very fast, compact, GPU-friendly Methods to improve quality\nVariant Key idea Benefit Mip-NeRF Multi-scale cones for anti-aliased rendering Handles different resolutions \u0026amp; reduces aliasing NeRF++ Two networks: near-field \u0026amp; far-field (spherical) Better for unbounded, complex scenes Mip-NeRF 360 Extends Mip-NeRF for unbounded scenes; uses nonlinear parameterization \u0026amp; regularization High-quality large-scene rendering Trade-offs and current challenges:\nFast methods (gaussian) ‚Üí train quickly, but lose visual quality. High-quality methods (NeRF, Mip-NeRF 360) ‚Üí photorealistic but slow to train. Explicit methods also can‚Äôt be optimized directly with gradients ‚Üí need to convert trained implicit NeRFs into their format (adds complexity). Real-time + high-quality rendering remains an open research challenge. 3.3 Compression üí° Three main strategies:\nLearning-based view synthesis on the decoder side; Learning-based view synthesis on the encoder and decoder sides; End-to-end light field compression architecture; An efficient codec should be able to explore:\nnot only the spatial and angular redundancies independently (as two-dimensional data), but also the combined spatial‚Äìangular redundancy (4D data). The key idea of this compression architecture is bitrate saving by sparsely encoding the views.\nLearning-based view synthesis on the decoder side Step Explanation Input light fields These are the original dense light-field data, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints. Key SAIs selection Instead of sending all the views, we select a few key SAIs. These key SAIs contain enough angular and spatial information to reconstruct the missing ones later. Encoder The encoder compresses these key SAIs into a bitstream (binary data) for transmission or storage. Bitstream This is the compressed data that‚Äôs transmitted or saved. It contains only the encoded information of the key SAIs (no non-key views). Decoder The decoder reconstructs the key SAIs from the bitstream. Learning-based view synthesis This is a deep neural network trained to synthesize new views (non-key SAIs) from the nearby key SAIs. Generate all non-key views ‚Üí fill in the gaps to recreate the full light field. Decoded light fields Produce and output the complete high-quality light field (same size as the original) after combining Decoded key SAIs and Generated non-key SAIs. Learning-based view synthesis on the encoder and decoder sides Idea:\nEncode only a few key views (the main ones), Use deep learning to predict or reconstruct the missing views (non-key views), Send only the residual errors to refine those predictions. Step Explanation Input light fields These are the original dense light-field data, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints. Key SAIs and Non-key SAIs The system splits the light field into: Key SAIs: a few representative images selected for transmission (e.g., every 3rd or 4th view). Non-key SAIs: the remaining views that will be predicted rather than transmitted. Encoder Takes two inputs: Key SAIs and Residuals (for the non-key SAIs), compresses both into a compact bitstream (binary data) for transmission or storage. Learning-based view synthesis (encoder side) This neural network predicts the non-key SAIs from the available key SAIs. so the encoder can calculate the prediction error (residual) ‚Üí the difference between the predicted and real non-key SAIs is computed as a residual. Residue The encoder subtracts the predicted non-key SAI (from the network) from the actual non-key SAI. only encode the difference, not the entire image ‚Äî saves lots of bitrate. Bitstream The data sent or stored ‚Äî contains compressed key SAIs + residuals. Decoder Receives and decompresses the bitstream. Reconstructs the key SAIs first. Then passes them to the learning-based view synthesis network to generate predicted non-key SAIs. Learning-based view synthesis (decoder side) Same (or similar) neural network as on the encoder side. It uses the decoded key SAIs to synthesize (predict) the non-key SAIs. Then it adds the residual (decoded correction data) to refine those synthesized views. Addition (+) and output The synthesized non-key SAIs are added to the decoded residuals ‚Üí final accurate non-key views. Combine key + non-key SAIs ‚Üí get the fully decoded light field. üí° Su et al. ‚Üí\nInstead of treating every pixel separately, they group light rays that belong to the same 3D point in the scene. After grouping rays, some nearby super-rays may still be very similar. So they merge them into larger super-rays to save even more space. Compress each super-ray using a 4D Discrete Cosine Transform (DCT). a light field varies in both: Spatial dimensions (x, y) ‚Äî inside each image, Angular dimensions (u, v) ‚Äî across different viewpoints. Better captures 4D spatial‚Äìangular redundancy but is more complex. End-to-end light field compression architecture Step Explanation Input light fields These are the original dense light-field data, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints. Encoder This is the main compression network.It takes the input light field and learns to represent it using fewer numbers (features). Bitstream The bitstream is the final compressed data produced by the encoder. It contains the quantized latent features. Decoder The decoder takes the bitstream and reconstructs (decodes) the light field. It performs the reverse of the encoder: expands the compact features back into full-resolution sub-aperture images. Decoded Light Fields These are the reconstructed sub-aperture images (SAIs).Ideally, they look almost identical to the input views, with small errors due to compression. End-to-end schemes are gaining more attention due to their effectiveness in image compression.\nüí° View synthesis drawbacks can be circumvented by neural representations that achieve a level of detail that is challenging for traditional methods.\n3.4 Other light field imaging applications Light-field (LF) images are more powerful than normal 2D photos because they capture depth, focus, and parallax ‚Äî this allows better performance in many computer vision tasks.\nDeep learning methods are now being used to apply light-field data to various new areas.\nSaliency Detection (SOD) Goal: detect which objects or regions attract human attention. LF advantage: provides both spatial and angular information, giving richer clues about object boundaries and depth. Typical model: encoder‚Äìdecoder two-stream networks: One stream uses all-in-focus (center) images, The other stream uses focal stacks or multi-view features. A comprehensive review compares deep LF-SOD models with standard RGB-D models. Face Recognition Goal: identify faces more accurately using multi-view data from light fields. LF advantage: combines intra-view (within one image) and inter-view (across multiple angles) features. Methods: VGG features with LSTM layers to model view changes. Capsule networks with a pose matrix to handle viewpoint shifts. Datasets introduced: LFFW (Light Field Faces in the Wild), LFFC (Light Field Face Constrained) ‚Äî for benchmarking LF face recognition. Light Field Microscopy Goal: use LF imaging to capture and reconstruct 3D biological structures quickly. LF advantage: captures 3D spatial information in one camera shot ‚Üí instant 3D imaging. Deep learning usage: improves speed and quality of reconstructions. Methods: Encoder‚Äìdecoder networks convert 2D LF inputs to 3D volume data. Networks with 2D and 3D residual blocks enhance reconstruction quality. Convolutional sparse coding (CSC) networks use EPIs as input for fast neuron localization. Applications: real-time visualization of cardiovascular or neuronal activity. The network uses EPIs as inputs and generates sparse codes, representing depth data, as outputs. Other applications Image classification ‚Äî improves feature learning using angular cues. Low-light imaging ‚Äî LF data helps reconstruct clear images in dark conditions. Overall, these works show that learning-based light-field imaging provides richer 3D understanding and better accuracy than normal 2D or RGB-D methods.\n4. Datasets and quality assessment Datasets Characteristics of the light field datasets used to benchmark the light field imaging systems\nQuality Assessment Light field imaging algorithms are typically evaluated using quantitative methods by comparing generated data to a ground-truth. Due to the diversity of light field acquisition procedures, distortions, and rendering processes, light field quality assessment remains a challenging task. A recent focus has been on developing more accurate objective algorithms that extract features from both spatial and angular domains for light field quality assessment. Metrics can be classified into three categories based on the availability of the reference image: full-reference (FR), reduced-reference (RR), no-reference (NR). Developing NR metrics are gaining more attention due to their success in improving accuracy. Current learning-based light field algorithms are still only evaluated using conventional PSNR and SSIM methods. In this context, the IEEE established a new standard called ‚ÄôIEEE P3333.1.4‚Äô, which defines metrics and provides recommended practices for light field quality assessment. A standardization activity, namely ‚ÄôJPEG Pleno Quality Assessment‚Äô, was recently initiated within the JPEG committee aiming to explore the most promising subjective quality assessment practices as well as the objective methodologies for plenoptic modalities in the context of multiple use cases. Summary of the objective quality assessment methods for light fields\n5. Discussion, challenges and perspectives While parallel to light fields, other plenoptic modalities like point cloud and holography have also been developed. Even though point clouds and holographic content processing and compression have advanced significantly in recent years, these content types may eventually need to be converted to light field views for visualization on the display. Light fields provide more comprehensive information when it comes to capturing scenes. Light fields capture not only the 3D objects but also the entire scene information, which can be essential in many applications like autonomous driving, that require accurate 3D recreation of the vehicle surroundings. Recent advances in using deep learning for spatio-angular reconstruction and the emergence of the NeRF-based approaches. More recent methods, such as 3D Gaussian splatting, show improvements in the quality‚Äìspeed trade-off. Enhancing the quality‚Äìspeed trade-off could enable new use cases such as real-time telepresence and robotic tasks with fewer views for reconstruction. Two neural scene representations of light fields: an explicit representation based on multiple SAIs an implicit neural representation encodes light fields as parameters of an MLP. Advances in deep learning frameworks are expected to significantly improve the performance of light field processing algorithms and solve the existing challenges. Better depth estimation or depth-free approaches are critical. Advanced methods are needed to efficiently exploit the huge amount of redundant information about the light rays in the same scene that conveys angular and spatial information Now targeting the creation of a learning-based coding standard to provide competitive compression efficiency compared to state-of-the-art light field coding solutions. The evaluation of light field imaging systems has several shortfalls related to the content and assessment approaches available. Advanced methods are needed to efficiently exploit the huge amount of redundant information about the light rays in the same scene that conveys angular and spatial information. It is essential to provide more comprehensive light field datasets from both the quantitative and content diversity perspectives. The assessment of plenoptic image quality also faces various challenges because of the variety of quality aspects and complexity of the content when compared to the assessment of 2D images. JPEG has begun developing a light field quality assessment standard, defining a framework with subjective quality assessment protocols and objective quality assessment procedures for lossy decoding of light field data within the context of multiple use cases. The IEEE is also developing a standard called ‚ÄúP3333.1.4‚ÄîRecommended Practice for the Quality Assessment of light field Imaging‚Äù that targets to establish methods of quality assessment of light field imaging based on psychophysical studies 6. Conclusions üëâüèº Content:\nCore Focus Progress and trends Challenges Future outlook Core Focus Main Tasks Reviewed:\nDepth estimation, reconstruction, super-resolution, and compression. Other Tasks:\nMicroscopy, saliency, face recognition, refocusing, and relighting also benefit from learning-based methods. Progress and Trends Deep Learning Integration:\nAI frameworks now appear in almost every stage of light field processing. Growth Drivers:\nBetter capture and display hardware and larger datasets will accelerate progress. Challenges Limited Realism:\nCurrent systems have narrow Field of View (FoV) and Depth of Field (DoF); Still far from true 6-DoF free-view exploration. Data Burden:\nExpanding datasets increase computational cost and reduce processing efficiency. Future Outlook Compression Evolution:\nLearning-based image compression is expected to greatly improve light field storage and transmission, making real-world applications more feasible. ","permalink":"/posts/learningbased_light_field_imaging/","summary":"\u003ch1 id=\"initial-impression\"\u003eInitial Impression\u003c/h1\u003e\n\u003caside\u003e\nüëâüèº\n\u003cp\u003eThe light field refres to the representation of a scene. Unlike the tranditional 2D image, it adds an extra dimention: the direction of light. This means each image captures not only the length and width, but also the direction of light.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTraditional image: \u003cstrong\u003e2D spatially\u003c/strong\u003e (width, height), but each pixel carries a \u003cstrong\u003e3-value vector\u003c/strong\u003e (RGB, 3 channels for color).\u003c/li\u003e\n\u003cli\u003eLight filed(width, heidht, X-direction, Y-direction): \u003cstrong\u003e4D images\u003c/strong\u003e, but each pixel carries a \u003cstrong\u003e3-value vector.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs a result, light field data is massive and challenging to process.\u003c/p\u003e","title":"Learning‚Äëbased light field imaging"},{"content":"1. Abstract Background:\nsummarization ‚Üí Traditional RAG works well for specific questions (‚ÄúWhen was Company X founded?‚Äù), but it struggles with broad, global ones (‚ÄúWhat are the main ideas in all these documents?‚Äù). scalability ‚Üí (Such questions need summarization of the whole dataset, not just retrieving a few passages ‚Äî that‚Äôs called query-focused summarization (QFS).) Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. we need to combine scalability and summarization: combines knowledge graph generation and query-focused summarization GraphRAG,\na graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text.\nBuild a graph index in two stages:\nderive an entity knowledge graph from the source documents. a knowledge graph (nodes = entities, edges = relationships) pre-generate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user.\n1. Introduction GraphRAG,\nuses an LLM to construct a knowledge graph a knowledge graph, nodes correspond to key entities in the corpus and edges represent relationships between those entities. it partitions the graph into a hierarchy of communities of closely related entities, before using an LLM to generate community-level summaries. GraphRAG answers queries through map-reduce processing of community summaries. In the map step ‚Üí the summaries are used to provide partial answers to the query independently and in parallel, In the reduce step ‚Üí the partial answers are combined and used to generate a final global answer. GraphRAG contrasts with vector RAG (text embeddings) in its ability to answer queries that require global sensemaking over the entire data corpus.\n2. Background Adaptive Benchmarking ‚Üí the process of dynamically generating evaluation benchmarks tailored to specific domains or use cases.\nGenerating test questions based on the current knowledge base. Measuring how well the model adapts when the corpus changes. Evaluating both retrieval and generation quality together. 3. Methods The high-level data flow of the GraphRAG approach and pipeline:\nCommunity detection is used to partition the graph index into groups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time.\nnum of duplicates ‚Üí edge weights claims ‚Üí similarity Entities \u0026amp; Relationships ‚Üí Knowledge Graph\nComponent Purpose Typical Technique (as described or implied) LLM extraction Identify entities/relations/claims Prompt-based, few-shot examples Entity matching Merge identical names Exact string match (default), fuzzy possible Graph construction Store nodes/edges Simple adjacency list or NetworkX graph Edge weighting Track frequency of relationships Count duplicates Aggregation \u0026amp; summarization Produce node/edge descriptions LLM summarization Community detection Find clusters Leiden algorithm (modularity optimization) For a given community level, the global answer to any user query is generated as follows:\nPrepare community summaries. Community summaries are randomly shuffled and divided into chunks of pre-specified token size. This ensures relevant information is distributed across chunks, rather than concentrated (and potentially lost) in a single context window. Map community answers. Intermediate answers are generated in parallel. The LLM is also asked to generate a score between 0-100 indicating how helpful the generated answer is in answering the target question. Answers with score 0 are filtered out. Reduce to global answer. Intermediate community answers are sorted in descending order of helpfulness score and iteratively added into a new context window until the token limit is reached. This final context is used to generate the global answer returned to the user. ","permalink":"/posts/from_local_to_global_a_graphrag_approach_to_query-/","summary":"\u003ch1 id=\"1-abstract\"\u003e1. Abstract\u003c/h1\u003e\n\u003cp\u003eBackground:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003esummarization ‚Üí\u003c/strong\u003e Traditional RAG works well for \u003cem\u003especific\u003c/em\u003e questions (‚ÄúWhen was Company X founded?‚Äù), but it struggles with \u003cem\u003ebroad\u003c/em\u003e, \u003cem\u003eglobal\u003c/em\u003e ones (‚ÄúWhat are the main ideas in all these documents?‚Äù).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003escalability ‚Üí\u003c/strong\u003e (Such questions need \u003cstrong\u003esummarization of the whole dataset\u003c/strong\u003e, not just retrieving a few passages ‚Äî that‚Äôs called \u003cstrong\u003equery-focused summarization\u003c/strong\u003e (QFS).) Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems.\u003c/li\u003e\n\u003cli\u003ewe need to combine \u003cstrong\u003escalability\u003c/strong\u003e and \u003cstrong\u003esummarization:\u003c/strong\u003e combines knowledge graph generation and query-focused summarization\u003c/li\u003e\n\u003c/ol\u003e\n\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eGraphRAG\u003c/strong\u003e,\u003c/p\u003e","title":"From Local to Global A GraphRAG Approach to Query-"},{"content":"Abstract Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. pre-trained models non-parametric memory differentiable access mechanism - In soft differentiable access mechanism, we don‚Äôt discard any chunks. - In Hard retrieval (standard RAG), the retriever picks the top-k passages We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. pre-trained models ‚Üí seq2seq model non-parametric memory ‚Üí a dense vector index of Wikipedia differentiable access mechanism ‚Üí a pre-trained neural retriever 1. Prompt (question) arrives. 2. Seq2seq encoder turns it into query vector q. 3. Retriever compares q to all memory keys k_i (Wikipedia passage vectors). 4. Compute similarity scores s_i = q ‚ãÖ k_i. 5. Apply softmax ‚Üí attention weights Œ±_i. 6. Read vector r = Œ£ Œ±_i v_i (weighted mixture of passage info). 7. Feed r (plus q) into seq2seq decoder ‚Üí generate answer token by token. 8. Gradients flow through Œ±_i ‚Üí retriever learns to attend to more relevant chunks. text chunk ‚Üí retriever encoder ‚Üí key/value ‚Üí FAISS index ‚Üí query embedding ‚Üí top-k retrieval ‚Üí generator We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. same retrieved passages ‚Üí RAG-Sequence different passages per token ‚Üí RAG-Token It‚Äôs often used for knowledge-intensive tasks, not free-form story generation. Discussion We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining.\nThis is one of RAG‚Äôs biggest advantages over standard language models: - You can update its knowledge base without retraining its parameters. The retriever learns the mapping ‚Üí parametric The index just holds the results ‚Üí non-parametric Retriever model A neural network that encodes queries and documents into vectors. Parametric ‚Äî it has learnable weights (parameters) Retrieval index (memory) The database of all document embeddings (keys + values) Non-parametric ‚Äî stored outside the model‚Äôs paras Index = structure that accelerates similarity search using ANN methods (cluster-and-search) (ANN -\u0026gt; Approximate nearest neighbor). Key = pre-computed document embedding; Value = original text (encoded later when used). Hard retrieval = pick top-k texts ‚Üí concatenate ‚Üí generator sees text. Soft retrieval = mix all embeddings by attention ‚Üí generator sees one read vector. Generator (e.g., BART or T5): a Transformer-based seq2seq model. 1. Introduction RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\nWe can make an anology: - RAG\u0026#39;s retriever is like encoder, because it summarizes what info the model should pay attention to before generation. - RAG\u0026#39;s generator is like decoder, because it generate the sequence token-by-token. Steps The retriever (Dense Passage Retriever, henceforth DPR) provides latent documents conditioned on the input, The seq2seq model (BART) then conditions on these latent documents together with the input to generate the output. 2. Methods Our models leverage 2 components:\na retriever $p_Œ∑(z|x)$ a generator $p_Œ∏(y_i|x, z, y_{1:i‚àí1})$ x = the query (e.g., a question or a sentence you want to search with) z = a text passage (a possible relevant document) Œ∑ = the parameters of the model/retriever **p(z‚à£x) = the prob that passage z is relevant to the query x** --- y1:i-1 = the previous i-1 tokens z = the retrieved passage x = the original input **pŒ∏(yi|x, z, y1:i‚àí1) = the prob that generating token yi, give three inputs.** We propose 2 models (based on the average of the latent documents in different ways to produce a distribution over generated text) :\nRAG-Token ‚Üí can predict each target token based on a different doc/chunk. RAG-Sequence ‚Üí the model uses the same doc/chunk to predict each target token. 2.1 Models RAG-Sequence Model: The RAG-Sequence model uses the same retrieved doc/chunk to generate the complete seq.\nRAG-Token Model: we can draw a different latent document for each target token and marginalize accordingly.\n2.2 Retriever: DPR We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. We refer to the document index as the non-parametric memory.\nDPR (Dense Passage Retriever): a bi-encoder architecture: d(z) = a dense representation of a document produced by a BERT document encoder. q(x) = a query representation produced by a query encoder, also based on BERT. MIPS (Maximum Inner Product Search) ‚Üí The operation of finding top-k documents by inner product between query and every docs. 2.3 Generator: BART We use BART-large, a pre-trained seq2seq transformer with 400M parameters. We simply concatenate the input x and the retrieved content z.\nBART combines the strengths of BERT and GPT: - BERT: bidirectional understanding (encoder) - GPT: left-to-right generation (decoder) 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved.\nUpdating the document encoder **BERTd** during training is costly as it requires the document index to be periodically updated as **REALM** does during pre-training. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder **BERTq** and the **BART generator**. BERTd = document encoder BERTq = query encoder REALM = Retrieval-Enhanced Adaptive Language Model update the doc encoder required re-encoding all documents every few steps ‚Äî which made it extremely slow and hard to scale. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate $arg max_y p(y|x)$.\nRAG-Token Model: standard beam search RAG-Sequence Model: Thorough Decoding or Fast Decoding An autoregressive model = predicts the next token based on all previous tokens. - Thorough Decoding = Generate and score candidate answers for every retrieved document, then combine their probabilities - most accurate but slow. - Fast Decoding = Only score candidates that were actually generated during beam search, skipping others ‚Äî much faster but approximate. RAG-Token Âú®ÁîüÊàêËøáÁ®ã‰∏≠ÔºåÊ®°Âûã‰ºöÂèÇËÄÉÊØè‰∏™ chunk ‰∏ãÁöÑÊù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏ÉÔºö$p_\\theta(y_i \\mid x, z, y_{1:i-1})$ Êù•È¢ÑÊµã‰∏ã‰∏Ä‰∏™ token ÁöÑÂèØËÉΩÊÄß„ÄÇ ÁÑ∂ÂêéÊ†πÊçÆÊ£ÄÁ¥¢Âô®ÁªôÂá∫ÁöÑÊØè‰∏™ chunk ÁöÑÊùÉÈáç $p_\\eta(z|x)$ÔºåÂØπËøô‰∫õÂàÜÂ∏ÉËøõË°åÂä†ÊùÉËûçÂêàÔºåÂæóÂà∞‰∏Ä‰∏™ÁªºÂêàÁöÑ‰∏ã‰∏ÄËØçÊ¶ÇÁéáÂàÜÂ∏ÉÔºö$p\u0026rsquo;(y_i \\mid x, y_{1:i-1}) = \\sum_z p_\\eta(z|x),p_\\theta(y_i \\mid x, z, y_{1:i-1})$ Ê®°Âûã‰ªéËøô‰∏™ËûçÂêàÂàÜÂ∏É‰∏≠ÈÄâÂá∫Ê¶ÇÁéáÊúÄÈ´òÁöÑ tokenÔºåÂÜçÂ∞ÜÂÖ∂Âä†ÂÖ•Âà∞Â∑≤ÁîüÊàêÁöÑÂ∫èÂàó‰∏≠„ÄÇ ÈáçÂ§çËØ•Ê≠•È™§ÔºåÁõ¥Âà∞ÁîüÊàêÂÆåÊï¥Âè•Â≠ê„ÄÇ üí° This makes generation very fast, but because it can borrow inconsistent or partially incorrect evidence from different chunks, the final sentence may contain blended or wrong facts, even though the decoding itself is efficient.\nRAG-Sequence (Thorough Decoding) ÂÖàÂú®ÊØè‰∏™ chunk ‰∏ãÁã¨Á´ãËøêË°å beam searchÔºåÂæóÂà∞Ê¶ÇÁéáÊúÄÈ´òÁöÑÂÄôÈÄâÂè•Â≠êÔºõ ÁÑ∂ÂêéÂ∞ÜËøô‰∫õÂÄôÈÄâÂè•ÂàÜÂà´Âú®ÂÖ∂‰ªñ chunk ‰∏äÈáçÊñ∞ËÆ°ÁÆóÁîüÊàêÊ¶ÇÁéá $p_\\theta(y|x,z)$Ôºà‰ΩøÁî® teacher forcing Âº∫Âà∂ÁîüÊàêÔºâÔºå ÊúÄÂêéÊ†πÊçÆÊØè‰∏™ chunk ÁöÑÊ£ÄÁ¥¢ÊùÉÈáç $p_\\eta(z|x)$ ÂØπÂè•Â≠êÊ¶ÇÁéáËøõË°åÂä†ÊùÉÊ±ÇÂíåÔºö$p(y|x) = \\sum_z p_\\eta(z|x),p_\\theta(y|x,z)$ ÊúÄÁªàÈÄâÂá∫Êï¥‰ΩìÊ¶ÇÁéáÊúÄÈ´òÁöÑÂè•Â≠ê‰Ωú‰∏∫ËæìÂá∫„ÄÇ üí° This ‚Äúglobal reconsideration‚Äù allows the model to filter out wrong or inconsistent sentences and select the most related one overall. However, because it must compute the probability of every candidate on every chunk, the process is extremely slow.\nRAG-Sequence (Fast Decoding) ÂÖàÂú®ÊØè‰∏™ chunk ‰∏ãÁîüÊàêÊ¶ÇÁéáÊúÄÈ´òÁöÑÂÄôÈÄâÂè•Â≠êÔºå ‰ΩÜÂè™Âú®ÁîüÊàêËøáËØ•Âè•Â≠êÁöÑ chunk‰∏äËÆ°ÁÆóÊ¶ÇÁéáÔºå Êú™ÁîüÊàêËØ•Âè•Â≠êÁöÑ chunk Áõ¥Êé•ÂøΩÁï•ÔºàËÆ§‰∏∫Ê¶ÇÁéá‚âà0ÔºâÔºå ÂÜçËøõË°åÂêåÊ†∑ÁöÑÂä†ÊùÉÊ±ÇÂíå„ÄÇ üí° This method is a trade-off between the two. It still generates separate sentences for each chunk, but it skips the expensive re-evaluation on other chunks‚Äîonly using the chunks that actually produced each sentence.\nAs a result, it‚Äôs much faster than thorough decoding while keeping almost the same accuracy, though it‚Äôs still slower than RAG-Token.\nComparison Item RAG-Token RAG-Sequence (Thorough) RAG-Sequence (Fast) Fusion Timing Dynamically fuses predictions from all chunks at each token Uses a fixed chunk for the whole sentence, then re-evaluates globally Uses a fixed chunk for the whole sentence, then re-evaluates locally Fusion Granularity Token-level Sentence-level Sentence-level Decoding Method Single beam search Multiple beam searches + full re-evaluation Multiple beam searches + partial re-evaluation Cross-chunk Generation ‚úÖ Allowed ‚ùå Not allowed ‚ùå Not allowed Accuracy Medium Highest High Speed Fast Slow Faster Typical Usage Common for online inference Mainly theoretical analysis / small-scale experiments Practical trade-off in real applications Probability Computation Sum across chunks at each token Sum across chunks after full sentence generation Sum across chunks after full sentence generation Core Idea Fuse multiple chunk predictions at every step Generate each sentence independently, then globally combine Generate each sentence independently, then combine locally Key Characteristics Each word leverages all chunks ‚Äî very fast but may produce inconsistent sentences Theoretically most accurate but computationally slow Approximate yet efficient ‚Äî widely used in practice 3. Experiments For all experiments:\nNon-parametric knowledge source: the December 2018 dump Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M docs. Build a single MIPS index using FAISS with a Hierarchical Navigable Small World approximation for fast retrieval. During training:\nWe retrieve the top k documents for each query. We consider k ‚àà {5, 10} for training and set k for test time using dev data. 3.1 Open-domain Question Answering Compare with:\nThe extractive QA paradigm ‚Äì extracts short answer spans directly from retrieved documents, relying mainly on non-parametric knowledge. The Closed-Book QA approaches ‚Äì generate answers without retrieval, depending only on parametric knowledge stored in the model. Consider four popular open-domain QA datasets:\nNatural Questions (NQ) TriviaQA (TQA) WebQuestions (WQ) CuratedTrec (CT) (CT and WQ are small; models are initialized from the NQ-trained RAG model.)\nEvaluate:\nPerformance is measured using Exact Match (EM) ‚Äì a metric that checks whether the generated answer exactly matches the reference answer. üí° Focus: finding facts from retrieval, not writing sentences.\n3.2 Abstractive Question Answering Evaluate:\nThe MSMARCO NLG v2.1 task, which tests RAG‚Äôs ability to generate free-form, natural language answers in a knowledge-intensive setting. Setup:\nEach example includes a question, ten gold retrieved passages, and a full-sentence human-written answer. RAG ignores the supplied passages and treats MSMARCO as an open-domain QA task (retrieving from Wikipedia instead). Note:\nSome questions cannot be answered correctly without the gold passages (e.g., ‚ÄúWhat is the weather in Volcano, CA?‚Äù). In such cases, RAG relies on its parametric knowledge to generate reasonable responses. üí° Focus: natural, fluent language generation (NLG).\n3.3 Jeopardy Question Generation Task:\nGiven an answer entity, generate a factual Jeopardy-style question (reverse QA). Dataset:\nSearchQA, with 100K train / 14K dev / 27K test examples. Compare:\nRAG vs BART (baseline model). Evaluate:\nQ-BLEU-1 metric (favors entity matching and factual accuracy). Human evaluation on two criteria: Factuality ‚Äî whether the question is factually correct. Specificity ‚Äî whether the question is closely related to the given answer. üí° Focus: evaluate RAG‚Äôs generation abilities in a non-QA setting.\n3.4 Fact Verification Task:\nGiven a claim, classify whether it is supported, refuted, or not enough info using evidence from Wikipedia. Dataset:\nFEVER benchmark. Method:\nMap each class label to a single output token, treating the task as sequence classification. RAG trains without supervision on retrieved evidence, learning retrieval and reasoning jointly. Evaluate:\nReport label accuracy for both: 3-way classification: supports / refutes / not enough info 2-way classification: supports / refutes Purpose:\nTest RAG‚Äôs capability for reasoning-based classification, not just text generation. üí° Focus: reasoning and classification with retrieval (not generation)\n4. Results Open-domain Question Answering Abstractive Question Answering Jeopardy Question Generation Fact Verification Table 1 \u0026amp; 2 üí° Table 1:\nTo show that RAG outperforms previous retrieval-based QA systems (like DPR and REALM) and even large closed-book models (like T5), Proving that retrieval + generation can achieve state-of-the-art results without re-rankers or extractive readers. Table 2:\nTo demonstrate that RAG generalizes beyond simple QA: it performs strongly on abstractive answer generation (MSMARCO), question generation (Jeopardy), and fact verification (FEVER) showing it works for both generation and classification tasks, even without gold evidence Table 3 Table 4 \u0026amp; 5 üí° Table 4:\nTo verify through human judgment that RAG‚Äôs generated questions are more factual and more specific than those from BART, confirming that retrieval grounding improves accuracy and relevance in text generation. Table 5:\nTo measure linguistic diversity of generated text ‚Äî showing that RAG‚Äôs outputs are less repetitive and more varied (closer to human text) than BART‚Äôs, thanks to diverse retrieved contexts. Factuality ‚Üí Is the question factually correct? Specificity ‚Üí Does the question precisely match its given answer (not too generic)? Table 6 ‚ÄúAblation‚Äù means removing or changing a part of the model to test how much it matters.\nüí° Table 6 shows that RAG‚Äôs learned dense retriever is essential\nreplacing it with BM25 or freezing it significantly hurts performance. proving that jointly learned retrieval is key for strong open-domain generation and QA results. Figure 2 The heatmap (right) shows which retrieved document (y-axis) the model relies on when generating each token (x-axis) of a sentence.\nThe heatmap shows a dark blue cell at (Doc 2, ‚ÄúSun‚Äù), which means Doc 2 ‚Äî the one containing ‚ÄúThe Sun Also Rises‚Äù ‚Äî is strongly influencing this token. (The model correctly ‚Äúlooks up‚Äù the document that mentions that book.)\nAfter that, the dark blue (posterior weight) flattens ‚Äî it spreads out across documents. That means: once the model has started generating ‚ÄúThe Sun‚Ä¶‚Äù, it can finish ‚ÄúAlso Rises‚Äù without continuing to depend on that document.\nüí° After seeing one or two key words from retrieval chunks (non-parametric memory), the generator‚Äôs parametric knowledge is enough to recall and complete the title.\nThe non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory.\n","permalink":"/posts/retrieval-augmented_generation_for_knowledge-inten/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003ePre-trained models\u003c/strong\u003e with a \u003cstrong\u003edifferentiable access mechanism\u003c/strong\u003e to \u003cstrong\u003eexplicit non-parametric memory\u003c/strong\u003e have so far been only investigated for extractive downstream tasks.\n\u003cul\u003e\n\u003cli\u003epre-trained models\u003c/li\u003e\n\u003cli\u003enon-parametric memory\u003c/li\u003e\n\u003cli\u003edifferentiable access mechanism\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-html\" data-lang=\"html\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e- In soft differentiable access mechanism, we don‚Äôt discard any chunks.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e- In Hard retrieval (standard RAG), the retriever picks the top-k passages\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\n\u003cli\u003eWe introduce RAG models where the parametric memory is \u003cstrong\u003ea pre-trained seq2seq model\u003c/strong\u003e and the non-parametric memory is \u003cstrong\u003ea dense vector index of Wikipedia\u003c/strong\u003e, accessed with \u003cstrong\u003ea pre-trained neural retriever\u003c/strong\u003e.\n\u003cul\u003e\n\u003cli\u003epre-trained models ‚Üí seq2seq model\u003c/li\u003e\n\u003cli\u003enon-parametric memory ‚Üí a dense vector index of Wikipedia\u003c/li\u003e\n\u003cli\u003edifferentiable access mechanism ‚Üí a pre-trained neural retriever\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-html\" data-lang=\"html\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e1. Prompt (question) arrives.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e2. Seq2seq encoder turns it into query vector q.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e3. Retriever compares q to all memory keys k_i (Wikipedia passage vectors).\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e4. Compute similarity scores s_i = q ‚ãÖ k_i.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e5. Apply softmax ‚Üí attention weights Œ±_i.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e6. Read vector r = Œ£ Œ±_i v_i  (weighted mixture of passage info).\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e7. Feed r (plus q) into seq2seq decoder ‚Üí generate answer token by token.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e8. Gradients flow through Œ±_i ‚Üí retriever learns to attend to more relevant chunks.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003etext chunk ‚Üí retriever encoder ‚Üí key/value ‚Üí FAISS index ‚Üí query embedding ‚Üí top-k retrieval ‚Üí generator\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\n\u003cli\u003eWe compare two RAG formulations, one which conditions on the \u003cstrong\u003esame retrieved passages\u003c/strong\u003e across the whole generated sequence, and another which can use \u003cstrong\u003edifferent passages\u003c/strong\u003e per token.\n\u003cul\u003e\n\u003cli\u003esame retrieved passages ‚Üí RAG-Sequence\u003c/li\u003e\n\u003cli\u003edifferent passages per token ‚Üí RAG-Token\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-html\" data-lang=\"html\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eIt‚Äôs often used for knowledge-intensive tasks, not free-form story generation.\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch1 id=\"discussion\"\u003eDiscussion\u003c/h1\u003e\n\u003cp\u003eWe conducted an thorough investigation of the learned retrieval component, validating\nits effectiveness, and we illustrated how the retrieval index can be \u003cstrong\u003ehot-swapped\u003c/strong\u003e to update the model without requiring any retraining.\u003c/p\u003e","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"content":"Von Neumann model The von Neumann model was an ‚Äúefficient bridge between software and hardware‚Äù because:\nHardware designers could build chips to execute it efficiently. Software developers could write high-level programs that compile into this model. Thus, the von Neumann model is the connecting bridge that enables programs from the diverse and chaotic world of software to run efficientby on machines from the diverse and chaotic world of hardware.\nBulk-synchronous parallel (BSP) model it is a viable candidate for the role of bridging model.\nValiant wants parallel simulations to be almost as fast as ideal ones.\nThe extra cost should be only a small constant factor, not growing with processor count. He tries to avoid efficiency loss that scales with log(p). The model should work well for any number of processors, from a few to millions. Features of BSP model A major feature of the BSP model is that it provides this option with optimal efficiency (i.e., within constant factors) provided the programmer writes programs with sufficient parallel slackness.\nBSP can automatically manage communication and memory efficiently if the program exposes\nenough parallel work. If there‚Äôs enough parallelism (many tasks per processor), the model achieves near-optimal performance with little manual tuning.\n","permalink":"/posts/a_bridging_model_for_parallel_computation/","summary":"\u003ch2 id=\"von-neumann-model\"\u003eVon Neumann model\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003evon Neumann model\u003c/strong\u003e was an \u003cem\u003e‚Äúefficient bridge between software and hardware‚Äù\u003c/em\u003e because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHardware designers\u003c/strong\u003e could build chips to execute it efficiently.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware developers\u003c/strong\u003e could write high-level programs that compile into this model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThus, the \u003cstrong\u003evon Neumann model\u003c/strong\u003e is the connecting bridge that enables programs from\nthe diverse and chaotic world of software to run efficientby on machines from the diverse and chaotic world of hardware.\u003c/p\u003e\n\u003ch2 id=\"bulk-synchronous-parallel-bsp-model\"\u003eBulk-synchronous parallel (BSP) model\u003c/h2\u003e\n\u003cp\u003eit is a viable candidate for the role of \u003cstrong\u003ebridging model\u003c/strong\u003e.\u003c/p\u003e","title":"A Bridging Model for Parallel Computation"},{"content":"Video Source\nPage Source\nPratical Source\n1. Introduction Extension of Abstract.\nSequence Modeling: Recurrent language modelsExtension of Abstract.\nOperate step-by-step, processing one token or time step at a time. one input + hidden state ‚Üí one output + hidden state (update every time) e.g., RNNs, LSTMs Encoder-decoder architectures\nComposed of two RNNs (or other models): one to encode input into a representation another to decode it into the output sequence one input + hidden state ‚Üí ‚Ä¶ ‚Üí hidden state ‚Üí one output + hidden state ‚Üí ‚Ä¶ e.g., seq2seq Disadvantages about these two: Recurrent models preclude parallelization, so it‚Äôs not good.\nEncoder-decoder models have used attention to pass the stuff from encoder to decoder more effectively.\nAttention doesn‚Äôt use recurrence and entirely relies on an attention mechanism to draw glabal dependencies between input and output. 2. Background Corresponding Work:\nSpeak clearly about the corresponding papers, the connections between u and the papers, and the differences.\nReduce sequential computation: Typically use convolutional neural networks ‚Üí the number of operations grows in the distance between positions ‚Üí transfomer: a constant number of operations\nat the cost of reduced effective resolution due to averaging attention-weighted positions we counteract this bad effect with Multi-Head Attention Self-attention: compute a representation of the sequence based on different positions\nEnd-to-end memory networks: ‚úî Reccurrence attention mechanism\n‚ùå Sequence-aligned recurrence\nTransformer: The first transduction model relying entirely on self-attention to do encoder-decoder model. ( to compute representations of its input and output without using sequence-aligned RNNs or convolution)\ntransduction model refers to making predictions or inferences about specific instances based on the data at hand, without relying on a prior generalization across all possible examples, which is different from Inductive learning.\n3. Model Architecture The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and the decoder.\nEncoder: maps an input sequence of symbol representations x ‚Üí a sequence of continuous representations z. Decoder: given z, generates an output sequence of symbols one element at a time. At each step, the model is auto-regressive. Why Using LayerNorm not BatchNorm? LayerNorm normalizes across features of a single sample, suitable for variable-length sequences.\nBatchNorm normalizes across the batch, which can be inconsistent for sequence tasks.\n3.1 Encoder and Decoder Stacks Encoder N = 6 identical layers, each layer has two sub-layers.\nmulti-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\ndimension of output = 512\nDecoder N = 6 identical layers, each layer has three sub-layers.\nmasked multi-head attention over the output of the encoder stack multi-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\n3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output where the query, keys, values, and output are all vectors.\nThe output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n( Given a query, output is computed by similarity between query and keys, then different keys have different weights, next combine the values based on the weights. )\n3.2.1 Scaled Dot-Product Attention We compute the dot products of the query with all keys, divide each by length of query ‚àö, and apply a softmax function to obtain the weights on the values.\nWhy divide each by length of query ‚àö ?\nIt prevents the input to the softmax from becoming excessively large, ensuring that the softmax outputs remain well-distributed and numerically stable. The softmax function normalize the scores into probabilities, but it doesn\u0026rsquo;t address the problem of large input magnitudes directly. If the inputs to softmax are extremely large, the e^x can become numerically unstable, leading to issues in computation.\nn refers to the length of a sequence, the number of the words.\ndk refers to the length of the one word vector.\nm refers to the number of the target words.\ndv refers to the length of the one target word vector.\nWhat there is a function of the Mask?\nWhen computing the output, I only use the key-value pairs up to the current time and do not use any later key-value pairs.\n3.2.2 Multi-Head Attention Linear projection, just like lots of channels.\nLinearly project the queries, keys and values h times to low dimensions with different, learned linear projections.\nFinally, these are concatenated (stack) and once again projected.\nWe emply h = 8 parallel attention layers, or heads.\nSo for each layer or head, we make their dimension to 64 to make the total computational cost is similar to single-head attention.\n3.2.3 Application of Attention in our Model The Transformer uses multi-head attention in three different ways:\nIn ‚Äúencoder-decoder attention‚Äù layers, queries ‚Üí previous decoder layer keys and values ‚Üí the output of the encoder In ‚Äúencoder self-attention‚Äù layers, keys, values and queries all come from the previous layer in the encoder In ‚Äúdecoder self-attention‚Äù layers, keys, values and queries all come from the previous layer in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position 3.3 Position-wise Feed-Forward Networks The fully connected feed-forward network is applied to each position separately and identically.\nThe Multi-Head Attention has already get the location information, now we need to add more expression ability by adding non-linear.\nThe output and MLP patterns are the same. The difference are the input source.\n3.4 Embedding and Softmax Embedding convert the input tokens to vectors of dimension ( 512 ), then multiply those weights by\n$\\sqrt {d_{models}}$\nWhy multiply those weights by $\\sqrt {d_{models}}$ ?\nTo boosts the magnitude, making it more aligned with other components of the model. Because the initial value is between 0~1 by using normal distribution.\nWe update the L2 norm of the embeddings to 1 finally, so we need to ensure the value of each dimension not too small. The L2 norm value of a vector to 1 is best to express a word, because it only express direction.\nSoftmax convert the decoder output to predicted next-token probabilities.\n3.5 Positional Encoding The order change, but the values don‚Äôt change. So we add the sequential information to the input.\nWe use sine and cosine functions of different frequencies [-1, 1]\n4. Why Self-Attention Length of a word ‚Üí d Number of words ‚Üí n Self-Attention ‚Üí n words ‚úñÔ∏è every words need to multiply with n words and for each two words do d multiplying. Recurrent ‚Üí d-dimension vector multiply d‚úñÔ∏èd matrix, n times Convolutional ‚Üí k kernel_size, n words, d^2 input_channels ‚úñÔ∏è output_channels (Draw picture clear) Self-Attention (restricted) ‚Üí r the number of neighbors It seems like Self-Attention architecture has lots of advantages, but it needs more data and bigger model to train to achieve the same effect.\n5. Training 5.1 Training Data and Batching Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. ‚Üí So we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation.\n5.2 Hardware and Schedule 5.3 Optimizer 5.4 Regularization Residual Dropout Label Smoothing 6. Results $N:$ number of blocks\n$d_{model}:$ the length of a token vector\n$d_{ff}:$ Feed-Forward Full-Connected Layer Intermediate layer output size\n$h:$ the number of heads\n$d_k:$ the dimension of keys in a head\n$d_v :$ the dimension of values in a head\n$P_{drop}:$ dropout rate\n$\\epsilon_{ls}:$ Label Smoothing value, the learned label value\n$train steps:$ the number of batchs\n","permalink":"/posts/attention_is_all_you_need/","summary":"\u003cp\u003e\u003ca href=\"https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.1387.top_right_bar_window_history.content.click\u0026amp;vd_source=83d7a54445de4810b52e58e4864b4605\"\u003eVideo Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1706.03762\"\u003ePage Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=kCc8FmEb1nY\"\u003ePratical Source\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"1-introduction\"\u003e1. Introduction\u003c/h1\u003e\n\u003caside\u003e\n\u003cp\u003eExtension of Abstract.\u003c/p\u003e\n\u003c/aside\u003e\n\u003ch2 id=\"sequence-modeling\"\u003eSequence Modeling:\u003c/h2\u003e\n\u003cp\u003eRecurrent language modelsExtension of Abstract.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOperate step-by-step, processing one token or time step at a time.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state ‚Üí one output + hidden state (update every time)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., RNNs, LSTMs\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEncoder-decoder architectures\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eComposed of two RNNs (or other models):\n\u003cul\u003e\n\u003cli\u003eone to encode input into a representation\u003c/li\u003e\n\u003cli\u003eanother to decode it into the output sequence\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state ‚Üí ‚Ä¶ ‚Üí hidden state ‚Üí one output + hidden state ‚Üí ‚Ä¶\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., seq2seq\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"disadvantages-about-these-two\"\u003eDisadvantages about these two:\u003c/h2\u003e\n\u003cp\u003eRecurrent models preclude parallelization, so it‚Äôs not good.\u003c/p\u003e","title":"Attention is All You Need"}]