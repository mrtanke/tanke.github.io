<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Archive | Home</title><meta name=keywords content><meta name=description content="archives"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/archives/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/archives/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/archives/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Archive"><meta property="og:description" content="archives"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/selfile.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/selfile.png"><meta name=twitter:title content="Archive"><meta name=twitter:description content="archives"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Archive","item":"https://my-blog-alpha-vert.vercel.app/archives/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Archive","name":"Archive","description":"archives","keywords":[],"articleBody":"","wordCount":"0","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/selfile.png","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/archives/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body class=list id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>Archive
<a href=/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><div class=archive-year><h2 class=archive-year-header id=2025><a class=archive-header-link href=#2025>2025</a>
<sup class=archive-count>&nbsp;48</sup></h2><div class=archive-month><h3 class=archive-month-header id=2025-December><a class=archive-header-link href=#2025-December>December</a>
<sup class=archive-count>&nbsp;15</sup></h3><div class=archive-posts><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</h3><div class=archive-meta><span title='2025-12-28 15:32:18 +0000 +0000'>December 28, 2025</span>&nbsp;·&nbsp;<span>693 words</span></div><a class=entry-link aria-label="post link to Diffusion Policy: Visuomotor Policy Learning via Action Diffusion" href=/notes/diffusion_policy_visuomotor_policy_learning_via_ac/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Synthesizer: Rethinking Self-Attention for Transformer Models</h3><div class=archive-meta><span title='2025-12-16 08:40:53 +0000 +0000'>December 16, 2025</span>&nbsp;·&nbsp;<span>244 words</span></div><a class=entry-link aria-label="post link to Synthesizer: Rethinking Self-Attention for Transformer Models" href=/notes/synthesizer_rethinking_self-attention_for_transfor/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Learning Transformer Programs</h3><div class=archive-meta><span title='2025-12-15 08:38:28 +0000 +0000'>December 15, 2025</span>&nbsp;·&nbsp;<span>339 words</span></div><a class=entry-link aria-label="post link to Learning Transformer Programs" href=/notes/learning_transformer_programs/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Reformer: The Efficient Transformer</h3><div class=archive-meta><span title='2025-12-14 08:39:11 +0000 +0000'>December 14, 2025</span>&nbsp;·&nbsp;<span>287 words</span></div><a class=entry-link aria-label="post link to Reformer: The Efficient Transformer" href=/notes/reformer_the_efficient_transformer/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">OpenVLA: An Open-Source Vision-Language-Action Model</h3><div class=archive-meta><span title='2025-12-12 08:37:15 +0000 +0000'>December 12, 2025</span>&nbsp;·&nbsp;<span>312 words</span></div><a class=entry-link aria-label="post link to OpenVLA: An Open-Source Vision-Language-Action Model" href=/notes/openvla_an_open-source_vision-language-action_mode/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning</h3><div class=archive-meta><span title='2025-12-10 08:36:10 +0000 +0000'>December 10, 2025</span>&nbsp;·&nbsp;<span>864 words</span></div><a class=entry-link aria-label="post link to Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning" href=/notes/bayesian_optimization_is_superior_to_random_search/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Random Search for Hyper-Parameter Optimization</h3><div class=archive-meta><span title='2025-12-10 08:35:15 +0000 +0000'>December 10, 2025</span>&nbsp;·&nbsp;<span>774 words</span></div><a class=entry-link aria-label="post link to Random Search for Hyper-Parameter Optimization" href=/notes/random_search_for_hyper-parameter_optimization/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">ALTA: Compiler-Based Analysis of Transformers</h3><div class=archive-meta><span title='2025-12-09 08:34:00 +0000 +0000'>December 9, 2025</span>&nbsp;·&nbsp;<span>720 words</span></div><a class=entry-link aria-label="post link to ALTA: Compiler-Based Analysis of Transformers" href=/notes/alta_compiler-based_analysis_of_transformers/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Tracr: Compiled Transformers as a Laboratory for Interpretability</h3><div class=archive-meta><span title='2025-12-08 08:32:26 +0000 +0000'>December 8, 2025</span>&nbsp;·&nbsp;<span>59 words</span></div><a class=entry-link aria-label="post link to Tracr: Compiled Transformers as a Laboratory for Interpretability" href=/notes/tracr_compiled_transformers_as_a_laboratory_for_in/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Thinking Like Transformers</h3><div class=archive-meta><span title='2025-12-07 15:14:48 +0000 +0000'>December 7, 2025</span>&nbsp;·&nbsp;<span>273 words</span></div><a class=entry-link aria-label="post link to Thinking Like Transformers" href=/notes/thinking_like_transformers/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">It’s All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization</h3><div class=archive-meta><span title='2025-12-06 15:13:02 +0000 +0000'>December 6, 2025</span>&nbsp;·&nbsp;<span>923 words</span></div><a class=entry-link aria-label="post link to It’s All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization" href=/notes/its_all_connected_a_journey_through_test-time_mem/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">FNet: Mixing Tokens with Fourier Transforms</h3><div class=archive-meta><span title='2025-12-05 15:11:32 +0000 +0000'>December 5, 2025</span>&nbsp;·&nbsp;<span>470 words</span></div><a class=entry-link aria-label="post link to FNet: Mixing Tokens with Fourier Transforms" href=/notes/fnet_mixing_tokens_with_fourier_transforms/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Linformer: Self-Attention with Linear Complexity</h3><div class=archive-meta><span title='2025-12-04 15:10:45 +0000 +0000'>December 4, 2025</span>&nbsp;·&nbsp;<span>236 words</span></div><a class=entry-link aria-label="post link to Linformer: Self-Attention with Linear Complexity" href=/notes/linformer_self-attention_with_linear_complexity/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Rethinking Attention with Performers</h3><div class=archive-meta><span title='2025-12-03 15:09:23 +0000 +0000'>December 3, 2025</span>&nbsp;·&nbsp;<span>499 words</span></div><a class=entry-link aria-label="post link to Rethinking Attention with Performers" href=/notes/rethinking_attention_with_performers/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning</h3><div class=archive-meta><span title='2025-12-01 08:49:03 +0000 +0000'>December 1, 2025</span>&nbsp;·&nbsp;<span>462 words</span></div><a class=entry-link aria-label="post link to On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning" href=/notes/on_the_representational_capacity_of_neural_languag/></a></div></div></div><div class=archive-month><h3 class=archive-month-header id=2025-November><a class=archive-header-link href=#2025-November>November</a>
<sup class=archive-count>&nbsp;22</sup></h3><div class=archive-posts><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">What Formal Languages Can Transformers Express? A Survey</h3><div class=archive-meta><span title='2025-11-30 08:47:55 +0000 +0000'>November 30, 2025</span>&nbsp;·&nbsp;<span>327 words</span></div><a class=entry-link aria-label="post link to What Formal Languages Can Transformers Express? A Survey" href=/notes/what_formal_languages_can_transformers_express_a_s/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">ATLAS: Learning to Optimally Memorize the Context at Test Time</h3><div class=archive-meta><span title='2025-11-29 08:46:44 +0000 +0000'>November 29, 2025</span>&nbsp;·&nbsp;<span>628 words</span></div><a class=entry-link aria-label="post link to ATLAS: Learning to Optimally Memorize the Context at Test Time" href=/notes/atlas_learning_to_optimally_memorize_the_context_a/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Solving olympiad geometry without human demonstrations</h3><div class=archive-meta><span title='2025-11-28 08:42:56 +0000 +0000'>November 28, 2025</span>&nbsp;·&nbsp;<span>522 words</span></div><a class=entry-link aria-label="post link to Solving olympiad geometry without human demonstrations" href=/notes/solving_olympiad_geometry_without_human_demonstrat/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Formal Mathematical Reasoning A New Frontier in AI</h3><div class=archive-meta><span title='2025-11-27 08:42:29 +0000 +0000'>November 27, 2025</span>&nbsp;·&nbsp;<span>347 words</span></div><a class=entry-link aria-label="post link to Formal Mathematical Reasoning A New Frontier in AI" href=/notes/formal_mathematical_reasoning_a_new_frontier_in_ai/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Titans: Learning to Memorize at Test Time</h3><div class=archive-meta><span title='2025-11-26 08:42:17 +0000 +0000'>November 26, 2025</span>&nbsp;·&nbsp;<span>916 words</span></div><a class=entry-link aria-label="post link to Titans: Learning to Memorize at Test Time" href=/notes/titans_learning_to_memorize_at_test_time/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Roformer: Enhanced Transformer With Rotary Position Embedding</h3><div class=archive-meta><span title='2025-11-25 08:40:15 +0000 +0000'>November 25, 2025</span>&nbsp;·&nbsp;<span>348 words</span></div><a class=entry-link aria-label="post link to Roformer: Enhanced Transformer With Rotary Position Embedding" href=/notes/roformer_enhanced_transformer_with_rotary_position/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</h3><div class=archive-meta><span title='2025-11-24 08:39:45 +0000 +0000'>November 24, 2025</span>&nbsp;·&nbsp;<span>360 words</span></div><a class=entry-link aria-label="post link to Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm" href=/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Mastering the game of Go without human knowledge</h3><div class=archive-meta><span title='2025-11-24 08:38:30 +0000 +0000'>November 24, 2025</span>&nbsp;·&nbsp;<span>342 words</span></div><a class=entry-link aria-label="post link to Mastering the game of Go without human knowledge" href=/notes/mastering_the_game_of_go_without_human_knowledge/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Disentangling Light Fields for Super-Resolution and Disparity Estimation</h3><div class=archive-meta><span title='2025-11-19 07:42:14 +0000 +0000'>November 19, 2025</span>&nbsp;·&nbsp;<span>1379 words</span></div><a class=entry-link aria-label="post link to Disentangling Light Fields for Super-Resolution and Disparity Estimation" href=/notes/disentangling_light_fields_for_super-resolution_an/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Hyena Hierarchy: Towards Larger Convolutional Language Models</h3><div class=archive-meta><span title='2025-11-18 07:39:29 +0000 +0000'>November 18, 2025</span>&nbsp;·&nbsp;<span>516 words</span></div><a class=entry-link aria-label="post link to Hyena Hierarchy: Towards Larger Convolutional Language Models" href=/notes/hyena_hierarchy_towards_larger_convolutional_langu/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</h3><div class=archive-meta><span title='2025-11-17 07:38:28 +0000 +0000'>November 17, 2025</span>&nbsp;·&nbsp;<span>397 words</span></div><a class=entry-link aria-label="post link to Mamba: Linear-Time Sequence Modeling with Selective State Spaces" href=/notes/mamba_linear-time_sequence_modeling_with_selective/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">A survey for light field super-resolution</h3><div class=archive-meta><span title='2025-11-14 07:41:11 +0000 +0000'>November 14, 2025</span>&nbsp;·&nbsp;<span>341 words</span></div><a class=entry-link aria-label="post link to A survey for light field super-resolution" href=/notes/a_survey_for_light_field_super-resolution/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Efficiently Modeling Long Sequences with Structured State Spaces</h3><div class=archive-meta><span title='2025-11-11 13:19:33 +0000 +0000'>November 11, 2025</span>&nbsp;·&nbsp;<span>930 words</span></div><a class=entry-link aria-label="post link to Efficiently Modeling Long Sequences with Structured State Spaces" href=/notes/efficiently_modeling_long_sequences_with_structure/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Retentive Network: A Successor to Transformer for Large Language Models</h3><div class=archive-meta><span title='2025-11-11 10:20:33 +0000 +0000'>November 11, 2025</span>&nbsp;·&nbsp;<span>472 words</span></div><a class=entry-link aria-label="post link to Retentive Network: A Successor to Transformer for Large Language Models" href=/notes/retentive_network_a_successor_to_transformer_for_l/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Exploiting Spatial and Angular Correlations With Deep Efficient Transformers for Light Field Image Super-Resolution</h3><div class=archive-meta><span title='2025-11-10 13:22:13 +0000 +0000'>November 10, 2025</span>&nbsp;·&nbsp;<span>1071 words</span></div><a class=entry-link aria-label="post link to Exploiting Spatial and Angular Correlations With Deep Efficient Transformers for Light Field Image Super-Resolution" href=/notes/exploiting_spatial_and_angular_correlations_with_d/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</h3><div class=archive-meta><span title='2025-11-09 15:01:10 +0000 +0000'>November 9, 2025</span>&nbsp;·&nbsp;<span>1312 words</span></div><a class=entry-link aria-label="post link to Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation" href=/notes/mixture-of-recursions_learning_dynamic_recursive_d/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Reference-Based Face Super-Resolution Using the Spatial Transformer</h3><div class=archive-meta><span title='2025-11-07 09:32:10 +0000 +0000'>November 7, 2025</span>&nbsp;·&nbsp;<span>428 words</span></div><a class=entry-link aria-label="post link to Reference-Based Face Super-Resolution Using the Spatial Transformer" href=/notes/reference-based_face_super-resolution_using_the_sp/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">LMR: A Large-Scale Multi-Reference Dataset for Reference-based Super-Resolution</h3><div class=archive-meta><span title='2025-11-07 08:38:54 +0000 +0000'>November 7, 2025</span>&nbsp;·&nbsp;<span>1157 words</span></div><a class=entry-link aria-label="post link to LMR: A Large-Scale Multi-Reference Dataset for Reference-based Super-Resolution" href=/notes/lmr_a_large-scale_multi-reference_dataset_for_refe/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Latent Diffusion Models</h3><div class=archive-meta><span title='2025-11-06 21:19:54 +0000 +0000'>November 6, 2025</span>&nbsp;·&nbsp;<span>964 words</span></div><a class=entry-link aria-label="post link to Latent Diffusion Models" href=/notes/high-resolution_image_synthesis_with_latent_diffus/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</h3><div class=archive-meta><span title='2025-11-04 12:06:46 +0000 +0000'>November 4, 2025</span>&nbsp;·&nbsp;<span>2299 words</span></div><a class=entry-link aria-label="post link to DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning" href=/notes/deepseek-r1_incentivizing_reasoning_capability_in/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</h3><div class=archive-meta><span title='2025-11-03 12:36:53 +0000 +0000'>November 3, 2025</span>&nbsp;·&nbsp;<span>1851 words</span></div><a class=entry-link aria-label="post link to An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" href=/notes/vit/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">A Tutorial on Bayesian Optimization</h3><div class=archive-meta><span title='2025-11-01 23:05:46 +0000 +0000'>November 1, 2025</span>&nbsp;·&nbsp;<span>3591 words</span></div><a class=entry-link aria-label="post link to A Tutorial on Bayesian Optimization" href=/notes/a_tutorial_on_bayesian_optimization/></a></div></div></div><div class=archive-month><h3 class=archive-month-header id=2025-October><a class=archive-header-link href=#2025-October>October</a>
<sup class=archive-count>&nbsp;11</sup></h3><div class=archive-posts><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution</h3><div class=archive-meta><span title='2025-10-29 08:23:31 +0000 +0000'>October 29, 2025</span>&nbsp;·&nbsp;<span>1433 words</span></div><a class=entry-link aria-label="post link to CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution" href=/notes/crossnet++_cross-scale_large-parallax_warping_for/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">xLSTM: Extended Long Short-Term Memory</h3><div class=archive-meta><span title='2025-10-28 13:18:30 +0000 +0000'>October 28, 2025</span>&nbsp;·&nbsp;<span>1394 words</span></div><a class=entry-link aria-label="post link to xLSTM: Extended Long Short-Term Memory" href=/notes/xlstm_extended_long_short-term_memory/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">RWKV: Reinventing RNNs for the Transformer Era</h3><div class=archive-meta><span title='2025-10-27 22:50:43 +0000 +0000'>October 27, 2025</span>&nbsp;·&nbsp;<span>1499 words</span></div><a class=entry-link aria-label="post link to RWKV: Reinventing RNNs for the Transformer Era" href=/notes/rwkv/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Mastering the game of Go with MCTS and Deep Neural Networks</h3><div class=archive-meta><span title='2025-10-24 10:00:00 +0000 +0000'>October 24, 2025</span>&nbsp;·&nbsp;<span>2246 words</span></div><a class=entry-link aria-label="post link to Mastering the game of Go with MCTS and Deep Neural Networks" href=/notes/mastering-go-mcts/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping</h3><div class=archive-meta><span title='2025-10-21 09:40:06 +0000 +0000'>October 21, 2025</span>&nbsp;·&nbsp;<span>1976 words</span></div><a class=entry-link aria-label="post link to CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping" href=/notes/crossnet_an_end-to-end_reference-based_super_resol/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</h3><div class=archive-meta><span title='2025-10-20 13:58:55 +0200 CEST'>October 20, 2025</span>&nbsp;·&nbsp;<span>314 words</span></div><a class=entry-link aria-label="post link to Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" href=/notes/chain-of-thought-prompting/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Learning‑based light field imaging</h3><div class=archive-meta><span title='2025-10-20 09:50:58 +0000 +0000'>October 20, 2025</span>&nbsp;·&nbsp;<span>6550 words</span></div><a class=entry-link aria-label="post link to Learning‑based light field imaging" href=/notes/learningbased_light_field_imaging/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">From Local to Global A GraphRAG Approach to Query-</h3><div class=archive-meta><span title='2025-10-16 19:42:01 +0000 +0000'>October 16, 2025</span>&nbsp;·&nbsp;<span>588 words</span></div><a class=entry-link aria-label="post link to From Local to Global A GraphRAG Approach to Query-" href=/notes/from_local_to_global_a_graphrag_approach_to_query-/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</h3><div class=archive-meta><span title='2025-10-15 09:43:53 +0000 +0000'>October 15, 2025</span>&nbsp;·&nbsp;<span>2177 words</span></div><a class=entry-link aria-label="post link to Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" href=/notes/retrieval-augmented_generation_for_knowledge-inten/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">A Bridging Model for Parallel Computation</h3><div class=archive-meta><span title='2025-10-10 12:30:04 +0000 +0000'>October 10, 2025</span>&nbsp;·&nbsp;<span>201 words</span></div><a class=entry-link aria-label="post link to A Bridging Model for Parallel Computation" href=/notes/a_bridging_model_for_parallel_computation/></a></div><div class=archive-entry><h3 class="archive-entry-title entry-hint-parent">Attention is All You Need</h3><div class=archive-meta><span title='2025-10-01 00:36:58 +0000 +0000'>October 1, 2025</span>&nbsp;·&nbsp;<span>1268 words</span></div><a class=entry-link aria-label="post link to Attention is All You Need" href=/notes/attention_is_all_you_need/></a></div></div></div></div></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>